{
  "abstract": "Due to the rapid development of deep learning, we can now\nsuccessfully separate singing voice from mono audio music.\nHowever, this separation can only extract human voices from\nother musical instruments, which is undesirable for karaoke\ncontent generation applications that only require the separa-\ntion of lead singers. For this karaoke application, we need to\nseparate the music containing male and female duets into two\nvocals, or extract a single lead vocal from the music contain-\ning vocal harmony. For this reason, we propose in this article\nto use a singer separation system, which generates karaoke\ncontent for one or two separated lead singers. In particular,\nwe introduced three models for the singer separation task and\ndesigned an automatic model selection scheme to distinguish\nhow many lead singers are in the song. We also collected\na large enough data set, MIR-SingerSeparation 1, which has\nbeen publicly released to advance the frontier of this research.\nOur singer separation is most suitable for sentimental ballads\nand can be directly applied to karaoke content generation.",
  "introduction": "An audio signal separation model was initially used to sep-\narate the target voice from background interference, such as\nthe TasNet model [1], which uses the encoder to extract the\ntwo-dimensional speech features, and then uses the separa-\ntion to estimate the speaker mask, and finally uses the de-\ncoder to convert the two-dimensional features into a speech\nwaveform to obtain the separated speech. The decoder can-\nnot be perfectly reconstructed, driving in-depth exploration\nand modification of TasNet, leading to the development of\nthe Multi-phase Gammatone filterbank, which can obtain a\nbetter frequency response distribution than random initializa-\ntion learning. Compared with a single-channel audio signal, a\n1https://gulaerchen.github.io/MIR-SingerSeparation/\nmulti-channel audio signal obtains more spatial information,\nthereby further assisting speech separation. Wave-U-Net [2]\nsplices multi-channel signals are input into U-Net and need to\nchange the number of input channels, but the input length of\nthe time domain is usually not fixed if the series is very long,\nand its resistance to optimization means the traditional RNN\nmodel cannot be effectively used. Dual-path recurrent neural\nnetworks (DPRNN) optimize RNN in the deep model to pro-\ncess extremely long speech sequences [3].",
  "related_work": "",
  "methodology": "of generation. Section 3 introduces the pro-\nposed system architecture and auto-selection method. Sec-\ntion 4 presents the content,",
  "results": ", and findings of each ex-\nperiment. Section 5 presents",
  "discussion": "",
  "conclusion": "s and directions for\nfuture work.\n2. DATASET\nMany datasets have been developed for music source separa-\ntion. For instance, MUSDB18 is composed of drums, bass,\nvocals, and remaining accompaniment [6]. The Mixing Se-\ncrets dataset is another multi-track dataset used for instru-\nment recognition in polyphonic music [7], but features se-\nrious leakage between tracks. The Choral Singing Dataset\nis a good multi-microphone dataset [8] that contains the per-\nsonal recordings of 16 singers from Spain, but has only three\n  [cs.SD]  18 Aug 2024\n\nFig.",
  "references": "[1] Yi Luo and Nima Mesgarani,\n“Tasnet: time-domain\naudio separation network for real-time, single-channel\nspeech separation,” in 2018 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 696–700.\n[2] Daniel Stoller, Sebastian Ewert, and Simon Dixon,\n“Wave-u-net: A multi-scale neural network for end-\nto-end audio source separation,”\narXiv preprint\n 2018.\n[3] Yi Luo, Zhuo Chen, and Takuya Yoshioka,\n“Dual-\npath rnn: efficient long sequence modeling for time-\ndomain single-channel speech separation,” in ICASSP\n2020-2020 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE,\n2020, pp. 46–50.\n[4] Jingjing Chen, Qirong Mao, and Dong Liu,\n“Dual-\npath transformer network: Direct context-aware model-\ning for end-to-end monaural speech separation,” arXiv\npreprint  2020.\n[5] Carolyn Drake and Caroline Palmer, “Accent structures\nin music performance,” Music perception, vol."
}