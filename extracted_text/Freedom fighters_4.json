{
  "abstract": "combat helps a red army fighter gain a dominant situation, which is the winning factor in later air combat. However,\ndue to the high speed and even hypersonic capabilities of advanced fighters, the diversity of tactical maneuvers, and\nthe instantaneous nature of situational transitions， it is difficult to meet the requirements of practical combat\napplications in terms of prediction accuracy. To improve prediction accuracy, this paper proposes a spatio-temporal\ngraph attention network (ST-GAT) using encoding and decoding structures to predict the flight trajectory. The\nencoder adopts a parallel structure of Transformer and GAT branches embedded with the multi-head self-attention\nmechanism in each front end. The Transformer branch network is used to extract the temporal characteristics of\nhistorical trajectories and capture the impact of the fighter's historical state on future trajectories, while the GAT\nbranch network is used to extract spatial features in historical trajectories and capture potential spatial correlations\nbetween fighters. Then we concatenate the outputs of the two branches into a new feature vector and input it into a\ndecoder composed of a fully connected network to predict the future position coordinates of the blue army fighter.",
  "results": "flight trajectories compared to the enhanced CNN-LSTM network (ECNN-LSTM), with improvements of 47% and\n34% in both ADE and FDE indicators, providing strong support for subsequent autonomous combat missions.\n**Keywords:** Trajectory prediction Spatio-temporal graph attention Transformer GAT\nfollow-up autonomous combat mission of\nunmanned aerial vehicles and the auxiliary\ncombat mission of manned aerial vehicles,\nwhich can help the red side to prevent risks in\nthe fight, take the advantage of the attack, and\nthen achieve the final victory.\nThe trajectory prediction problem refers to\nthe use of historical trajectory information and\nexisting knowledge to predict the future position\nof a target. Currently, there are more\napplications and research in intelligent traffic\nsuch as civil aviation and vehicle autopilot fields,\nwhere trajectory prediction is usually used for\nPreprint submitted to _**Name of Journal**_ May 13, 2024\nthe research of techniques such as traffic flow\nestimation and obstacle avoidance for long\nperiods in the future. Early trajectory prediction\nperformance in trajectory prediction tasks,\noutperforming RNNs in the time series domain\n[6]. In addition, variational self-encoder models\nand Generative Adversarial Networks (GANs)\ncan also be used for trajectory prediction. Such\nas Walker et al [7] proposed a framework for\nunsupervised learning in 2014. Gupta et al [8]\nproposed a Social-GAN network that uses a new\naggregation mechanism to aggregate\ninformation between people to predict\nreasonable pedestrian trajectories. Then Li et al\n[9] proposed a probabilistic trajectory prediction\nWith the emergence of new technologies\nsuch as deep learning, reinforcement learning,\nand the rapid application of trajectory prediction\nin the civilian field, air combat flight trajectory\ntarget maneuver segmentation points. In\naddition, multiple achievements have been made\nin jointly solving trajectory prediction problems\nwith maneuver decision-making, situational\nassessment, and other related issues: Z. Wei et al\n[19] designed a TSO-GRU-Ada prediction\nmodel, which divides the trajectory prediction\nmodel into tactical maneuver prediction as well\nas trajectory point prediction, and combines the\ntriangular search optimization of the Adaboost\nand the GRU network. Xie Lei et al [20]\n_3.1_ _Description of the dataset_\nIn this paper, thirty typical air battles are\nselected on the DCS World platform, including\neight \"4vs4\", twelve \"2vs2\" and ten \"1vs1\"\nbattles. The sampling frequency is 2 Hz and each\nair battle has 300 to 3,800 data items, each of\nwhich records the flight data such as the altitude,\nspeed, longitude, latitude, roll angle, yaw angle,\npitch angle, etc. After the obtained data were\nlow-pass filtered to filter out the noise\ninterference they are divided into a training set\nand a test set in the ratio of 4:1.\n_3.2_ _Development environment_\nThis paper uses Python language and\nPyTorch framework for model construction and\nsimulation experiments, and adpot a dual-card\ncrossfire 1080ti graphics card host as the\nexperimental hardware environment. The\nintegrated development environment is\nPyCharm, and the deep learning framework is\nPytorch 1.3.1.\n_3.3_ _Evaluation indicators_\n(1) ADE(Average Displacement Error): Used to\nmeasure the average error of the trajectory\nprediction model throughout the prediction\nprocess, calculated as follows:\n(2) FDE(Final Displacement Error): The\nEuclidean distance between the endpoint of the\npredicted trajectory and the endpoint of the true\ntrajectory. Its calculation formula is as follows:\n(6)\nwhere _xitobs_ + _t_ _pred_, _yitobs_ + _t_ _pred_ and _zitobs_ + _t_ _pred_\nrepresent the Cartesian coordinate values of\nfighter _i_ at the last moment (true values), and\n_x_ % _itobs_ + _t_ _pred_, _y_ % _itobs_ + _t_ _pred_ and _z_ % _itobs_ + _t_ _pred_ represent the\nCartesian coordinate values of fighter _i_\npredicted by the network at the last moment\n(predicted values), respectively.\n_yitobs_ + _t_ _pred_\n+ _t_ _pred_ and\n_zitobs_ + _t_ _pred_\nwhere\n_xitobs_ + _t_ _pred_\n+ _t_ _pred_,\n_x_ % _itobs_ + _t_ _pred_,\n_y_ % _itobs_ + _t_ _pred_ and\n_y_ % _itobs_ + _t_ _pred_\n_z_ % _itobs_ + _t_ _pred_\n_x_ % _itobs_ + _t_ _pred_\n(5)\n+ _t_\n_obs_ _pred_\n_3.4_ _Comparative experimental analysis_\n3.4.1. Comparison of spatio-temporal graph\nattention network and the enhanced CNN\nLSTM network in different scenarios\nTo make a more comprehensive comparison,\ntwo different engagement scenarios are selected\nto be analyzed in this paper, namely: \"2vs2\",\n\"4vs4\". The historical trajectories of different\nenvironments are used as inputs and fed into the\ntwo networks respectively to obtain the output\ntrajectories of the two sets of networks, to\nmeasure the advantages and disadvantages of\ndifferent networks in \"2vs2\" combat scenarios\ndifferent networks in \"4vs4\" combat scenarios\nAs shown in Fig. 2, in the \"2vs2\"\nengagement scenario, both the enhanced CNNLSTM network and the spatio-temporal graph\nattention network can correctly judge the future\nflight direction of the fighters from the\ninformation of the historical trajectories, and\npredict the flight trajectories of the fighters in a\ncertain moment. However, on the right side of\nFig. 2(a) and Fig. 2(b), the prediction error of the\nspatio-temporal graph attention network is\nsmaller than that of the enhanced CNN-LSTM\nnetwork when there exists a more intense\nconfrontation.\nAs shown in Fig. 3, in the \"4vs4\"\nengagement scenario, the enhanced CNNLSTM network has a slightly poorer ability to\ncapture the spatial relationship between the\nfighters, which leads to a lower accuracy of the\nair combat environments are recorded as shown\nin Table2, 3.\nTable 2 \"2vs2\" air combat environments\nModule ADE FDE\nECNN-LSTM 0.185 0.197\nST-GAT 0.098 0.124\nTable 3 \"4vs4\" air combat environments\nModule ADE FDE\nECNN-LSTM 0.465 0.563\nST-GAT 0.239 0.313\nAfter observing the predicted trajectory\ngraphs and analyzing the evaluation indicators,\nit can be analyzed that the spatio-temporal graph\nattention network in different air combat\nenvironments has higher accuracy compared to\nthe enhanced CNN-LSTM network using the\nattention mechanism, and the predicted\ntrajectories are closer to the real trajectories.\n3.4.2. Effect of sampling frequency on the\nthe sampling frequency increasing, the number\nof history points used at the same time increases\ndramatically, the computation volume increases,\nand the accuracy improvement is small.\n_3.5_ _Ablation experiment_\nTo better reflect the effectiveness of this\nfollows.\nFig. 4 Spatio-temporal graph attention network\nFig. 5 GAT branch network\nFig. 6 Transformer branch network\nFrom the figures, it can be seen that the\nTransformer branch network is well for\ntemporal information, but the prediction\naccuracy decreases when there is a sudden\nchange in the flight attitude of the fighter. The\nGAT branch network can predict the change in\nthe flight trajectory of the fighter in time due to\nFrom the perspective of quantitative\nanalysis, this paper records the ADE, FDE of\npredicting the trajectory points of one moment\nin the future from the trajectory points of eight\nmoments in history under different networks\nunder the ablation experiments. The values of\nADE and FDE are the same when only one point\nis predicted. The ADE is 0.009 km in the spatiotemporal graph attention network, 0.021 km in\nthe Transformer branch network, and 0.094 km",
  "introduction": "At present, the development of the air force\nof each military power gradually presents the\ntrend of integration of mechanization,\ninformatization, and intelligence. Intelligent air\ncombat based on informatization can directly\nempower and enhance the combat power of the\nair force, then promote the rapid evolution and\ndevelopment of the combat mode and technical\nform of air combat, and continuously derive new\ntactical systems[1]. Due to the continuous\nimprovement of high-speed breakthrough\ndefense capability and stealth performance of\nfighters, close-range combat is still one of the\ninevitable forms of current and future air combat.\nIn the close-range intelligent air combat system\ncomposed of core technologies such as\nsituational awareness, identification prediction,\nsituational assessment, and maneuver decisionmaking, the prediction of fighter flight trajectory\nplays a very crucial role, and its prediction",
  "methods": "engineering, both of which achieve the\nprediction of future trajectories by using the\nfeatures and statistical laws in a large amount of\nhistorical trajectory data. However, these\nability and expression ability of features,\nresulting in poor generalization performance\nand low prediction accuracy.\nWith the increasing maturity of deep\nlearning algorithms, significant progress has\nbeen made in trajectory prediction, leveraging\ntechniques such as CNN, LSTM, attention\nmechanisms, and graph networks. Recurrent\nneural networks represented by RNN and LSTM\ncan learn well for data with temporal\ncharacteristics. For example, to ensure the safe\nand orderly completion of civil aviation flight\ntasks, Z.Shi et al[2] proposed a constrained long\nshort-term memory network for flight trajectory\nprediction, which combines the dynamic\ncharacteristics of aircraft (climb, cruise, descent)\nwith the LSTM network. Alahi et al. [3]\nintroduced Social LSTM, a model based on\nsocial long short-term memory networks. Deo et\nal. [4] further enhanced this approach by\nincorporating a convolutional layer to optimize\nthe original social pooling layer, resulting in the\nnetwork outputting multiple possible future\ntrajectories. Due to the high spatio-temporal\ncorrelation of trajectories, Nikhil et al. [5]\nargued that such data could be more effectively\nhandled using CNNs. They applied a\nconvolutional layer to the sequence structure to\ncapture the spatial correlation in trajectories. As\nsystem, which firstly inputs historical\ntrajectories and environmental information as\nconditions into a neural network, and then uses\na generative model to generate future\ntrajectories. Subsequently, Kipf et al [10]\nintroduced the concept of CNN into graph\nneural networks for the first time and proposed\na Graph Convolutional Network (GCN), which\nallows for the efficient extraction of node and\nedge features. Huang et al [11] proposed a\nspatio-temporal graph attentional network based\non the sequence-to-sequence structure for\npredicting future pedestrian trajectories.\nChandra et al [12] used a two-layer GNNLSTM structure to solve the trajectory\nprediction problem. Huang Zijie [13] proposed\na Social-Spatial-Temporal Graph Convolutional\nNeural Network (Social-STGCNN) to complete\nthe trajectory prediction by focusing on the\ninteraction between pedestrians on the road for\nmodeling. Wang Tianbao [14] proposed the TPGCN algorithm to address the problem of the\ndifficulty of efficiently constructing pedestrian\ninteractions for the task of pedestrian trajectory\nAVGCN, for trajectory prediction based on\nhuman attention and improved using variational\ntrajectory prediction with full consideration of\nthe stochastic nature of pedestrian trajectories.\nperformance on several trajectory prediction\nindicators. H. Jeon [16] used an edge-enhanced\ngraph convolutional neural network to construct\nan interactive embedding network between\nvehicles and proposed the first fully scalable\n3\nvehicle trajectory prediction network SCALENet, which can guarantee good prediction\nperformance and consistent calculation load\nregardless of the number of surrounding\nvehicles. Y-Fang et al [17] take into account the\ninfluence of the surrounding pedestrian\nmovement and use a high-order GCN network\nto model the interaction between pedestrians,\nwhich takes into account both the neighbors of\nthe target pedestrians as well as the neighbors of\ntheir neighbors, which in turn can avoid the\nprogress. Recently, Zhi fei et al. [18] proposed a\nhybrid algorithm called AERTros-Volterra and\nintroduced an ensemble learning scheme into air\ncombat flight trajectory prediction. The\nensemble prediction model is adaptively\nupdated based on the model's performance on\ncombines the dynamic relational weighting\nalgorithm with the movement time strategy and\nadded the Ada-LSTM trajectory prediction\nalgorithm to the maneuver decision. Although\ndeep learning can effectively extract potential\nfeatures from Euclidean spatial data, it is still\npowerless in the face of extensive nonEuclidean data in practical application scenarios,\nbecause the data of these application scenarios\nare generated in non-Euclidean space. Such as\nnon-Euclidean spatial data that is common in air\ncombat environments: graph data, so it is\nfor such scenarios. Y. Sun et al. [21] proposed a\nprediction algorithm based on attention\nmechanism and graph convolutional networks,\nwhich pioneered the addition of graph\nconvolutional networks to reflect the positional\nrelationships between different fighter jets,\nachieving trajectory prediction during multifighter confrontation.\nThis paper draws on the successful\nexperience of Transformer networks in the field\nof time series prediction [22], and is inspired by\nJiansen Zhao's [23] use of GAT networks to\nextract spatial features of ship trajectories for\naccurate prediction. Combining the mission\ncharacteristics and requirements of close-range\nfighter combat, this paper proposes a spatiotemporal graph attention network to achieve\nflight trajectory prediction.\n**2.** **Modelling framework**\nThe overall network structure of this paper\nis shown in Fig. 1, the model as a whole is\ndivided into two parts: encoder and decoder. On\nthe encoder side, two fully connected layers FC1,\nFC2 are used to transform and adjust the raw\ndata in the trajectory respectively. The\nTransformer module is used to extract features\nfrom the historical flight trajectory\n_traj_ = { _traj traj traj_ 1, 2, 3, , _trajn_ } of the fighter in\nthe time dimension, in which the purely selfattention network is used to predict the mutated\ntrajectories in the complex and changing\ntrajectory scenarios. At the same time, the GAT\nmodule is used to extract features of the spatial\ndimension of the flight trajectory of the fighter,\nthe temporal and spatial features are spliced and\nsent to the fully connected network FC3 for\ndecoding to obtain the coordinate values of _x_,\n4\n_y_, _z_ and at the next moment. The sliding\nblack arrow in the figure. The new output points\nare used as known trajectory points and spliced\nwith the historical trajectory of the previous\nmoment as inputs for the next round of\nprediction, until the prediction of all target\npoints is completed.\n_2.2_ _GAT module_\ncannot effectively share the flight characteristics\nof fighters in many to many air combat\nenvironments. Therefore, this paper uses the\nGAT module to aggregate the spatial correlation\nof historical trajectories using the relationship\nbetween vertices and edges in the graph\nstructure.\nEach fighter is characterized by _hi_, and the\ndimension of _hi_ is 8 3  . It is fed into the graph\nnetwork to calculate the attention weight of the\ncurrent node concerning the other nodes with the\nfollowing formula:\n_eijt_ = ( _Wh Whi_, _j_ ) (1)\nIn Equation (1), the features of different\nnodes are dimensionally expanded using a\nweight matrix 8 24  to enhance their feature\nexpression. Subsequently, the dimensionexpanded features are spliced and transformed\ninto a constant for representing the attention\ncoefficient by  matrix operation. To better\ncharacterize the relationship between different\n## nodes, it can be normalized using the softmax\nfunction:\n= _ij_ _softmax ei_ ( _ij_ ) (2)\nEquation (2) is modified to obtain a new\nformula for calculating the attention weights:\nFig. 1 Spatio-temporal graph attention network\nmodel\n_2.1_ _Transformer module_\nReferring to the article [24], this paper uses\nthe encoder part of the Transformer network\nstructure for temporal feature extraction of\ntrajectory information. The Transformer module\noutputs a feature vector of dimension _l_  _n_ 24,\n## where l  denotes the length of the historical\ntrajectory, and _n_ denotes the number of\nfighters. The specific structural parameters in\nthe Transformer module are shown in the\nfollowing table, PE denotes position encoding,\nMHA denotes multi-head attention, FNN\ndenotes feed-forward network, LN denotes\nnormalization layer.\nTable 1 Transformer module structure\nparameters\nWhere **||** denotes the splicing operation and\n_N_ denotes the node adjacent to node _i_ .\nThe formula for the feature vector after\nmulti-attention fusion is:\nexp( _Leaky_ Re _LU a Wh_ ( _T_ [ || _Wh_ ]))\n=\n# ~~~~\n_Leaky_ Re _LU a Wh_ ( _T_ [ || _Wh_\n_T_\n_ij_ = _T_ _i_ _j_\n _ij_ = exp( _Leaky_ Re _LU a Wh_ ( _T_ [ _i_ || _Whj_\nexp( _Leaky_ Re _LU a Wh_ ( _T_ [ || _Wh_ ]))\n(3)\n_i_ _jk_\n_k N_ \n\n_K_\n'' '\nNetwork\nNetwork Input\nheads\nlayer size\nDrop\nout_r\nate\nOutput\nsize\nwhere _[h]_ _i_ [ and ] \" _[h]_ _i_ [ denote the features after ] '\nfusion of the multi-head attention mechanism\nand the features of the single attention\nmechanism, respectively, and  is the\nSigmoid activation function. The dimension of\nthe final GAT module output feature vector is\n8  _n_ 24, and _n_ is the number of fighters.\n# hij '' = (  hi '\n_K_ _i_ = 1\n# = (  hi ' ) (4)\n_K_ _i_ = 1\n( 1 _K_ _h_ ' )\n# \n1\nsize\nPE - _l_  _n_ 24 0 _l_  _n_ 24\nMHA 4 _l_  _n_ 24 0.1 _l_  _n_ 24\nFNN - _l_  _n_ 24 0.1 _l_  _n_ 24\nLN - _l_  _n_ 24 0 _l_  _n_ 24\n5\nimages, the red part represents the trajectory of\nthe past 8 moments, the blue part represents the\nreal future trajectory, and the green dotted line\nrepresents the future trajectory predicted by the\nmodel based on the historical trajectory. Figure\n(a) both show the predicted trajectory graphs of\nthe spatio-temporal graph attention network.\nFigure (b) both show the predicted trajectories\nof the enhanced CNN-LSTM network. When\nreading the graphs the x-axis needs to add 3590\nkm, the z-axis needs to add 4100 km, in Fig. 4\nthe y-axis needs to add 3220 km, and in Fig. 5\nthe y-axis needs to add 3200 km.\n_i_ = 1 _pred_ _t t_ = + 1\n_obs_\nwhere the dataset contains a total of _n_\nmoments; _tobs_ represents the trajectory data\ncontaining _obs_ historical moments during the\nprediction process; _xi_, _t_ _yi_ and _t_ _[z]_ [ represent ] _it_\nprediction process; _xi_, _t_ _yi_ and _t_ _[z]_ [ represent ] _it_\nthe Cartesian coordinate values (true values) of\nfighter _i_ at moment _t_, while _x_ % _it_, _[y]_ [%] _it_ [  and ]\n_z_ % _it_ represent the Cartesian coordinate values\n(predicted values) of fighter _i_ predicted by the\nnetwork at moment _t_, respectively.\n_xi_, _t_\n_yi_ and _t_\n_x_ % _it_,\n_t_\n_[y]_ [%] _i_ [  and ]\n6\nThe spatio-temporal graph attention network\nextracts spatio-temporal features through\nTransformer and GAT, and utilizes the\nrelationship between points and edges in the\ngraph data structure to simulate the spatial\npositional change relationship of fighters. This\nnetwork can better extract the interaction\nfeatures in complex historical trajectories and\nmake the predicted trajectories closer to the real\ntrajectories.\nFrom the perspective of quantitative\nanalysis, the ADE, FDE (in kilometers) of the\nThe sampling frequency used in this paper\nto obtain trajectories from air battles is 2Hz.\nThe distance difference between the interval\npoints is relatively large. To verify the feasibility\nof the algorithm in this paper, this section uses\nthe same air battle scenario with the sampling\nfrequency of 2Hz and 50Hz to conduct\ncomparative experiments. When the sampling\nfrequency is 2Hz, the historical 8 trajectory\npoints are used to predict the next trajectory\npoint. At a sampling frequency of 50Hz, a\n7\nhistory of 200 trajectory points is used to predict\nthe next trajectory point. Both use the first 4\nseconds of trajectory information.\nWhen the sampling frequency is 2Hz, the\nerror of predicting the next trajectory point is\n0.009 km by using 8 trajectory points in history;\nwhen the sampling frequency is 50Hz, the error\nof predicting the next trajectory point is 0.0078\nkm by using 200 trajectory points in history. It is\nverified that increasing the sampling frequency\nbranch network, GAT branch network, and\nspatio-temporal graph attention network to carry\nout experiments under the same experimental\nconditions. To more intuitively reflect the\nchanging trend of trajectory, the trajectory\npoints of 8 moments in history are taken to\npredict the trajectory points of 8 moments in the\nin this paper combines the advantages of the two,\nfuses the temporal and spatial features, and\ngation, Formal analysis, Writing-original draft.\nTengyu Jing and Jiapeng Wang: Conceptualiza\ntion. Wei Wang: Writing-review & editing.\n**Declaration of Competing Interest**\nThe authors declare that they have no\nknown competing financial interests or personal\nrelationships that could have appeared to influ\nence the work reported in this paper.\n**Acknowledgments**\nThis work was supported by FUND (Grant\nNo. XXX)\n**References**\n[1] Sun Zhi-Xiao, Yang Sheng-Qi et al. An overview\nof the development of future intelligent air warfare[J]. Journal of Aeronautics, 2021, 42(8): 3549.\n[2] Z. Shi, M. Xu and Q. Pan, \"4-D Flight Trajectory\nPrediction With Constrained LSTM Network,\" in\nIEEE Transactions on Intelligent Transportation\nSystems, vol. 22, no. 11, pp. 7242-7255, Nov.\n2021.\n[3] Alahi A, Goel K, Ramanathan V, et al. Social lstm:\nHuman trajectory prediction in crowded\nspaces[C]//Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 2016:\n961-971.\n[4] Deo N, Trivedi M M. Convolutional social pool\ning for vehicle trajectory prediction[C]//Proceed\nings of the IEEE conference on computer vision\nand pattern recognition workshops. 2018: 1468\n1476.\n[5] Nikhil N, Tran Morris B. Convolutional neural\nnetwork for trajectory prediction[C]//Proceedings\nof the European Conference on Computer Vision\n(ECCV) Workshops. 2018: 0-0\n[6] Bai S, Kolter J Z, Koltun V. An empirical evalua\ntion of generic convolutional and recurrent net\nworks for sequence modeling[J]. arXiv preprint\narXiv:1803.01271, 2018.\n[7] Walker J, Gupta A, Hebert M. Patch to the future:\nUnsupervised visual prediction[C]//Proceedings\nof the IEEE conference on Computer Vision and\nPattern Recognition. 2014: 3302-3309.\n9\n[8] Gupta A, Johnson J, Fei-Fei L, et al. Social gan:\nSocially acceptable trajectories with generative\nadversarial networks[C]//Proceedings of the IEEE\nconference on computer vision and pattern recog\nnition. 2018: 2255-2264.\n[9] Li J, Ma H, Tomizuka M. Conditional generative\nneural system for probabilistic trajectory predic\ntion[C]//2019 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS). IEEE,\n2019: 6150-6156.\n[10] Kipf T N, Welling M. Semi-supervised classifica\ntion with graph convolutional networks[J]. arXiv\npreprint arXiv:1609.02907, 2016.\n[11] Huang Y, Bi H, Li Z, et al. Stgat: Modeling spa\ntial-temporal interactions for human trajectory\nprediction[C]//Proceedings of the IEEE/CVF in\nternational conference on computer vision. 2019:\n6272-6281.\n[12] Chandra R, Guan T, Panuganti S, et al. Forecast\ning trajectory and behavior of road-agents using\nspectral clustering in graph-lstms[J]. IEEE Robot\nics and Automation Letters, 2020, 5(3): 4882\n4890.\n[13] Z.J. Huang. Pedestrian trajectory prediction based\non scene and pedestrian interaction force [D]. Wu\nhan Textile University, 2021.000241\n[14] T.B. Wang,Y. Liu,J.C. Guo et al. Pedestrian tra\njectory prediction algorithm for graph convolu\ntional neural networks[J]. Journal of Harbin Insti\ntute of Technology,2021,53(02):53-60.\n[15] C. Liu, Y. Chen, M. Liu and B. E. Shi, \"AVGCN:\nTrajectory Prediction using Graph Convolutional\nNetworks Guided by Human Attention,\" 2021\nIEEE International Conference on Robotics and\nAutomation (ICRA), Xi'an, China, 2021, pp.\n14234-14240, doi:\n10.1109/ICRA48506.2021.9560908.\n[16] H. Jeon, J. Choi and D. Kum, \"SCALE-Net: Scal\nable Vehicle Trajectory Prediction Network under\nRandom Number of Interacting Vehicles via\nEdge-enhanced Graph Convolutional Neural Net\nwork,\" 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), Las\nVegas, NV, USA, 2020, pp. 2095-2102, doi:\n10.1109/IROS45743.2020.9341288.\n[17] Fang, Y., Jin, Z., Cui, Z. et al. Modeling human–\nhuman interaction with attention-based high-order\nGCN for trajectory prediction. Vis Comput 38,\n2257–2269 (2022).\n[18] Xi Zhi-fei, Kou Ying-xin, Li Zhan-wu,et al.Air\ncombat target maneuver trajectory prediction\nbased on robust regularized Volterra series and\nadaptive ensemble online transfer learning, De\nfence Technology,Volume 20,2023,Pages 187\n206,ISSN 2214-9147.\n[19] Z. Wei et al., \"A tactical maneuver trajectory pre\ntriangle search optimization with AdaBoost,\"\n2021 33rd Chinese Control and Decision Confer\nence (CCDC), Kunming, China, 2021, pp. 2325\n2330, doi: 10.1109/CCDC52312.2021.9602340.\n[20] Xie Lei, Ding Dali, Wei Zhenglei, Xi Zhifei, Tang\nAndi, \"Moving Time UCAV Maneuver Decision\nBased on the Dynamic Relational Weight Algo\nrithm and Trajectory Prediction\", Mathematical\nProblems in Engineering, vol. 2021, Article ID\n6641567, 19 pages, 2021.\nhttps://doi.org/10.1155/2021/6641567\n10\n[21] Y. Sun, X. Zhou, Z. Yang, W. Wang and Q. Shi,\nAttentional Graph Convolutional Network,\" 2023\n3rd International Symposium on Computer Tech\nnology and Information Science (ISCTIS),\nChengdu, China, 2023, pp. 127-132, doi:\n10.1109/ISCTIS58954.2023.10213104.\n[22] LI, SHIYANG et al. “Enhancing the Locality and\nBreaking the Memory Bottleneck of Transformer\non Time Series Forecast\ning.” ArXiv abs/1907.00235 (2019): n. pag.\n[23] Jiansen Zhao, Zhongwei Yan, ZhenZhen Zhou,\nXinqiang Chen, Bing Wu, Shengzheng Wang,A\nand LSTM,Ocean Engineering,Volume 289, Part\n1,2023,116159,ISSN 0029-8018.2023.116159.\n[24] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017)\nAttention Is All You Need. 31st Conference on\nNeural Information Processing Systems (NIPS\n2017), Long Beach, 4-9 December 2017.\n[25] Hao Q, Zhang J, Jing T, et al. Flight Trajectory\nPrediction Using an Enhanced CNN-LSTM Net\nwork[J]. arXiv preprint arXiv:2404.19218, 2024.\n11",
  "conclusion": "algorithm in this paper performs best.\n8\nIn this paper, the graph structure is\nintroduced into the prediction of air combat\ntrajectory, and the GAT network is combined\nwith the Transformer network for constructing\nthe spatio-temporal graph attention network.\nThrough comparison experiments with\nextensive groups of networks, it is verified that\nthe spatio-temporal graph attention network has\na better prediction effect. The current spatiotemporal graph attention network can extract\nfeatures in both spatial and temporal dimensions,\nbut whether the network structure can be further\noptimized to analyze the pilot's flight intention\nby using multimodal fusion so that it can be\nbetter adapted to the characteristics of the air\ncombat flight trajectory prediction mission, is\nworth exploring in depth.\n**CRediT authorship contribution statement**"
}