{
  "abstract": "Learning curves are a concept from social sciences that has been adopted\nin the context of machine learning to assess the performance of a learn-\ning algorithm with respect to a certain resource, e.g., the number of\ntraining examples or the number of training iterations. Learning curves\nhave important applications in several machine learning contexts, most\nnotably in data acquisition, early stopping of model training, and model\nselection. For instance, learning curves can be used to model the per-\nformance of the combination of an algorithm and its hyperparameter\nconfiguration, providing insights into their potential suitability at an\nearly stage and often expediting the algorithm selection process. Var-\nious learning curve models have been proposed to use learning curves\nfor decision making. Some of these models answer the binary deci-\nsion question of whether a given algorithm at a certain budget will\noutperform a certain reference performance, whereas more complex mod-\nels predict the entire learning curve of an algorithm.",
  "introduction": "Learning curves describe a system’s performance on a task as a function of\nsome resource to solve that task. There can be a pre-defined budget of that\nresource, limiting the amount of resources that can be spent. In other cases, the\ngoal can be to obtain reasonable results while minimising the spent budget of\nthat resource. Typical types of budgets are the number of examples the learner\nhas observed before performing the task or the number of iterations or time\nthe learner spends in an environment. The performance measure expresses the\nquality of the obtained model, e.g., error rate or F1 measure.",
  "related_work": "s on learning curves:\nAnother prominent literature review that centres around learning curves is\nthe highly complementary work by Viering and Loog (2023), which has been\ndeveloped in parallel. While both works have identified the three types of\ndecision-making situations that are referred to in the literature, Viering and\nLoog (2023) survey more theoretical work that analyses the shape of learning\ncurves, whereas this work surveys work that is more oriented towards",
  "methodology": "presented\nin the literature along the various axes of this framework. Sec. 5 lists all\nthese methods (where each subsection represents a type of question being\nanswered), and Table 1 overviews all methods along the three axes of our\nframework.\n• Based on the framework, we identify unexplored routes for further research.\nMost notably, we note that there is a mismatch between the research ques-\ntions being answered and the learning curve modelling method being used;\nin many cases, a high-level modelling technique is used to answer a low-\nlevel question. We speculate that matching the level of the question being\nanswered with the appropriate level of the modelling technique can further\nimprove the obtained",
  "results": "while minimising the spent budget of\nthat resource. Typical types of budgets are the number of examples the learner\nhas observed before performing the task or the number of iterations or time\nthe learner spends in an environment. The performance measure expresses the\nquality of the obtained model, e.g., error rate or F1 measure. Learning curves\nare an important source of information for making decisions on the following\nmatters in machine learning:\n• Data Acquisition determines how many data points should reasonably be\nacquired to obtain a desired performance. The top right plot in Fig.",
  "discussion": "s and feedback on this survey.\n\nSpringer Nature 2021 LATEX template\nLearning Curves for Decision Making\nA Notation\nTable 2 contains an overview of the notation used throughout this paper.\n\nterm\ndescription\nD\nThe space of all possible datasets\nd\nAn instantiation of a dataset\ndtr\nThe instances from a given dataset based upon which a given hypothesis h\nis trained\nX\nThe space of all possible input values for a given dataset d\nY\nThe space of all possible labels of a given dataset d\nH\nThe space that a given model or hypothesis h can take\nh\nModel or hypothesis h, induced based on a given train set dtr\na\nAn algorithm that given a training set dtr induces a hypothesis h\nA\nSet of all possible learners under consideration\nRout\nThe theoretical performance of a hypothesis under the true distribution of\nthe data (note that this true distribution is typically unknown)\nRin\nThe empirical risk of a hypothesis under some sample d (i.e., a dataset) from\nthe true distribution\nC (a, n)\ntrue mean performance of learner a when trained on n samples (related to\nobservation learning curve)\nC (a,n,t)\ntrue mean performance of learner a when trained on n samples with t\niterations (related to iteration learning curve)\nf (a, b)\nThe performance of a given learner a trained on a dataset of sample size b\n(in case of observation curve) or iteration b (in case of iteration curve). In\ncontrast to C. f (a, b) is a random variable with C as mean value\nµa,b\nThe mean of f (a, b)\nσ2\na,b\nThe variance of f (a, b)\nO\nSet of performance observations from anchors of historic learning curves, pos-\nsibly for different learners, e.g., O = {(a1, b1), (a2, b2), (a3, b3), . . .}, where\nai are learners and bj are budgets.\nˆf (a, b)\nThe performance estimated by a learning curve model for learner a at budget\nb.",
  "conclusion": "or belief that there is a\nnecessary relationship to correctness. In particular, an epistemic uncertainty\nof 0 does not imply a correct prediction. Suppose the model ˆfa is chosen from a\nclass of which the mean curve µa,· is not a member. In that case, it is guaranteed\nthat there will be wrong predictions, regardless of the epistemic uncertainty\nexpressed by the model. But even if µa,· is among the models from which ˆfa\ncan be built, it can still (and usually will) happen that, based on insufficient\nobservations, a wrong ˆfa will be picked.",
  "references": "Adriaensen S, Rakotoarison H, M¨uller S, et al (2023) Efficient bayesian learning\ncurve extrapolation using prior-data fitted networks. In: Advances in Neural\nInformation Processing Systems 36, pp 19,858–19,886\nAlwosheel A, van Cranenburgh S, Chorus CG (2018) Is your dataset big\nenough? sample size requirements when using artificial neural networks for\ndiscrete choice analysis. Journal of choice modelling 28:167–182\nAmari S, Murata N (1993) Statistical theory of learning curves under entropic\nloss criterion. Neural Computation 5(1):140–153\nBaker B, Gupta O, Raskar R, et al (2018) Accelerating neural architecture\nsearch using performance prediction."
}