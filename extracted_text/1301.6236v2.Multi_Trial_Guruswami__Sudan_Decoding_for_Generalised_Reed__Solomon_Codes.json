{
  "full_text": "arXiv:1301.6236v2  [cs.IT]  31 Jan 2013\nNoname manuscript No.\n(will be inserted by the editor)\nMulti-Trial Guruswami–Sudan Decoding for Generalised\nReed–Solomon Codes\nJohan S. R. Nielsen · Alexander Zeh\nJune 19, 2018\nAbstract An iterated reﬁnement procedure for the Guruswami–Sudan list decoding\nalgorithm for Generalised Reed–Solomon codes based on Alekhnovich’s module min-\nimisation is proposed. The method is parametrisable and allows variants of the usual\nlist decoding approach. In particular, ﬁnding the list of closest codewords within an\nintermediate radius can be performed with improved average-case complexity while\nretaining the worst-case complexity.\nKeywords Guruswami–Sudan · List Decoding · Reed–Solomon Codes · Multi-Trial\n1 Introduction\nSince the discovery of a polynomial-time hard-decision list decoder for Generalised\nReed–Solomon (GRS) codes by Guruswami and Sudan (GS) [12,7] in the late 1990s,\nmuch work has been done to speed up the two main parts of the algorithm: inter-\npolation and root-ﬁnding. Notably, for interpolation Beelen and Brander [2] mixed\nthe module reduction approach by Lee and O’Sullivan [8] with the parametrisation of\nZeh et al. [13], and employed the fast module reduction algorithm by Alekhnovich [1].\nBernstein [4] pointed out that a slightly faster variant can be achieved by using the\nreduction algorithm by Giorgi et al. [6].\nFor the root-ﬁnding step, one can employ the method of Roth and Ruckenstein [11]\nin a divide-and-conquer fashion, as described by Alekhnovich [1]. This step then be-\ncomes an order of magnitude faster than interpolation, leaving the latter as the main\ntarget for further optimisations.\nFor a given code, the GS algorithm has two parameters, both positive integers: the\ninterpolation multiplicity s and the list size ℓ. Together with the code parameters they\ndetermine the decoding radius τ. To achieve a higher decoding radii for some given\nJ. S. R. Nielsen is with the Institute of Mathematics, Technical University of Denmark\nE-mail: j.s.r.nielsen@mat.dtu.dk\nA. Zeh is with the Institute of Communications Engineering, University of Ulm, Germany and\nResearch Center INRIA Saclay - ˆIle-de-France, ´Ecole Polytechnique, France\nE-mail: alex@codingtheory.eu\n\n2\nGRS code, one needs higher s and ℓ, and the value of these strongly inﬂuence the\nrunning time of the algorithm.\nIn this work, we present a novel iterative method: we ﬁrst solve the interpolation\nproblem for s = ℓ= 1 and then iteratively reﬁne this solution for increasing s and ℓ.\nIn each step of our algorithm, we obtain a valid solution to the interpolation problem\nfor these intermediate parameters. The method builds upon that of Beelen–Brander [2]\nand has the same asymptotic complexity.\nThe method therefore allows a fast multi-trial list decoder when our aim is just to\nﬁnd the list of codewords with minimal distance to the received word. At any time dur-\ning the reﬁnement process, we will have an interpolation polynomial for intermediate\nparameters ˆs ≤s, ˆℓ≤ℓyielding an intermediate decoding radius ˆτ ≤τ. If we perform\nthe root-ﬁnding step of the GS algorithm on this, all codewords with distance at most\nˆτ from the received are returned; if there are any such words, we break computation\nand return those; otherwise we continue the reﬁnement. We can choose any number\nof these trials, e.g. for each possible intermediate decoding radius between half the\nminimum distance and the target τ.\nSince the root-ﬁnding step of GS is cheaper than the interpolation step, this multi-\ntrial decoder will have the same asymptotic worst-case complexity as the usual GS using\nthe Beelen–Brander interpolation; however, the average-case complexity is better since\nfewer errors are more probable.\nThis contribution is structured as follows. In the next section we give necessary pre-\nliminaries and state the GS interpolation problem for decoding GRS codes. In Section 3\nwe give a deﬁnition and properties of minimal matrices. Alekhnovich’s algorithm can\nbring matrices to this form, and we give a more ﬁne-grained bound on its asymptotic\ncomplexity. Our new iterative procedure is explained in detail in Section 4.\n2 Preliminaries\n2.1 Notation\nLet Fq be the ﬁnite ﬁeld of order q and let Fq[X] be the polynomial ring over Fq with\nindeterminate X. Let Fq[X, Y ] denote the polynomial ring in the variables X and Y\nand let wdegu,v XiY j ≜ui + vj be the (u, v)-weighted degree of XiY j.\nA vector of length n is denoted by v = (v0, . . . , vn−1). If v is a vector over\nFq[X], let deg v ≜maxi{deg vi(X)}. We introduce the leading position as LP(v) =\nmaxi{i| deg vi(X) = deg v} and the leading term LT(v) = vLP(v) is the term at this\nposition. An m × n matrix is denoted by V = ∥vi,j∥m−1,n−1\ni=0,j=0 . The rows of such\na matrix will be denoted by lower-case letters, e.g. v0, . . . , vm−1. Furthermore, let\ndeg V = Pm−1\ni=0 deg vi. Modules are denoted by capital letters such as M.\n2.2 Interpolation-Based Decoding of GRS Codes\nLet α0, . . . , αn−1 be n nonzero distinct elements of Fq with n < q and let w0, . . . , wn−1\nbe n (not necessarily distinct) nonzero elements of Fq. A GRS code GRS(n, k) of length\nn and dimension k over Fq is given by\nGRS(n, k) ≜\b\n(w0f(α0), . . . , wn−1f(αn−1)) : f(X) ∈Fq[X], deg f(X) < k\t\n.\n(1)\n\n3\nGRS codes are Maximum Distance Separable (MDS) codes, i.e., their minimum Ham-\nming distance is d = n −k + 1. We shortly explain the interpolation problem of GS [7,\n12] for decoding GRS codes in the following.\nTheorem 1 (Guruswami–Sudan for GRS Codes [7,12]) Let c ∈GRS(n, k) be\na codeword and f(X) the corresponding information polynomial as deﬁned in (1). Let\nr = (r0, . . . , rn−1) = c+e be a received word where weight(e) ≤τ. Let r′\ni denote ri/wi.\nLet Q(X, Y ) ∈Fq[X, Y ] be a nonzero polynomial that passes through the n points\n(α0, r′\n0), . . . , (αn−1, r′\nn−1) with multiplicity s ≥1, has Y -degree at most ℓ, and\nwdeg1,k−1 Q(X, Y ) < s(n −τ). Then (Y −f(X)) | Q(X, Y ).\nOne can easily show that a polynomial Q(X, Y ) that fulﬁls the above conditions can\nbe constructed whenever E(s, ℓ, τ) > 0, where\nE(s, ℓ, τ) ≜(ℓ+ 1)s(n −τ) −\u0000ℓ+1\n2\n\u0001\n(k −1) −\u0000s+1\n2\n\u0001\nn\n(2)\nis the diﬀerence between the maximal number of coeﬃcients of Q(X, Y ), and the num-\nber of homogeneous linear equations on Q(X, Y ) speciﬁed by the interpolation con-\nstraint. This determines the maximal number of correctable errors, and one can show\nthat satisfactory s and ℓcan always be chosen whenever τ < n−\np\nn(k −1) (for n →∞\nsee e.g. [7]).\nDeﬁnition 2 (Permissible Triples) An integer triple (s, ℓ, τ) ∈(Z+)3 is permissi-\nble if E(s, ℓ, τ) > 0.\nWe deﬁne also the decoding radius-function τ(s, ℓ) as the greatest integer such that\n(s, ℓ, τ(s, ℓ)) is permissible.\nIt is easy to show that E(s, ℓ, τ) > 0 for s > ℓimplies τ < ⌊n−k\n2 ⌋, which is half\nthe minimum distance. Therefore, it never makes sense to consider s > ℓ, and in the\nremainder we will always assume s ≤ℓ. Furthermore, we will also assume s, ℓ∈O(n2)\nsince this e.g. holds for any τ for the closed-form expressions in [7].\n2.3 Module Reformulation of Guruswami–Sudan\nLet Ms,ℓ⊂Fq[X, Y ] denote the space of all bivariate polynomials passing through the\npoints (α0, r′\n0), . . . , (αn−1, r′\nn−1) with multiplicity s and with Y -degree at most ℓ. We\nare searching for an element of Ms,ℓwith low (1, k −1)-weighted degree.\nFollowing the ideas of Lee and O’Sullivan [8], we can ﬁrst remark that Ms,ℓis\nan Fq[X] module. Second, we can give an explicit basis for Ms,ℓ. Deﬁne ﬁrst two\npolynomials G(X) = Qn−1\ni=0 (X −αi) as well as R(X) as the Lagrange polynomial going\nthrough the points (αi, r′\ni) for i = 0, . . . , n −1. Denote by Q[t](X) the Y t-coeﬃcient of\nQ(X, Y ) when Q is regarded over Fq[X][Y ].\nLemma 3 Let Q(X, Y ) ∈Ms,ℓ. Then G(X)s−t | Q[t](X) for t < s.\nProof Q(X, Y ) interpolates the n points (αi, r′\ni) with multiplicity s, so for any i, Q(X+\nαi, Y + r′\ni) = Pt\nj=0 Q[j](X + αj)(Y + r′\nj)j has no monomials of total degree less than\ns. Multiplying out the (Y + r′\nj)j-terms, Q[t](X + αj)Y t will be the only term with\nY -degree t. Therefore Q[t](X + αj) can have no monomials of degree less than s −t,\nwhich implies (X −αi) | Q[t](X). As this holds for any i, we proved the lemma.\n⊓⊔\n\n4\nTheorem 4 The module Ms,ℓis generated as an Fq[X]-module by the ℓ+1 polynomials\nP (i)(X, Y ) ∈Fq[X, Y ] given by\nP (t)(X, Y ) = G(X)s−t(Y −R(X))t,\nfor 0 ≤t < s,\nP (t)(X, Y ) = Y t−s(Y −R(X))s,\nfor s ≤t ≤ℓ.\nProof It is easy to see that each P (t)(X, Y ) ∈Ms,ℓsince both G(X) and (Y −R(X))\ngo through the n points (αi, r′\ni) with multiplicity one, and that G(X) and (Y −R(X))\ndivide P (t)(X, Y ) with total power s for each t.\nTo see that any element of Ms,ℓcan be written as an Fq[X]-combination of the\nP (t)(X, Y ), let Q(X, Y ) be some element of Ms,ℓ. Then the polynomial Q(ℓ−1)(X, Y ) =\nQ(X, Y ) −Q[ℓ]P (ℓ)(X, Y ) has Y -degree at most ℓ−1. Since both Q(X, Y ) and\nP (ℓ)(X, Y ) are in Ms,ℓ, so must Q(ℓ−1)(X, Y ) be in Ms,ℓ. Since P (t)(X, Y ) has Y -\ndegree t and P (t)\n[t] (X) = 1 for t = ℓ, ℓ−1, . . . , s, we can continue reducing this way until\nwe reach a Q(s−1)(X, Y ) ∈Ms,ℓwith Y -degree at most s −1. From then on, we have\nP (t)\n[t] (X) = G(X)s−t, but by Lemma 3, we must also have G(X) | Q(s−1)\n[s−1] (X), so we can\nalso reduce by P (s−1)(X, Y ). This can be continued with the remaining P (t)(X, Y ),\neventually reducing the remainder to 0.\n⊓⊔\nWe can represent the basis of Ms,ℓby the (ℓ+ 1) × (ℓ+ 1) matrix As,ℓ\n=\n∥P (i)\n[j] (X, Y )∥ℓ,ℓ\ni=0,j=0 over Fq[X]. Any Fq[X]-linear combination of rows of As,ℓthus\ncorresponds to an element in Ms,ℓby its tth term being the Fq[X]-coeﬃcient to Y t.\nAll other bases of Ms,ℓcan be similarly represented by matrices, and these will be\nunimodular equivalent to As,ℓ, i.e., they can be obtained by multiplying As,ℓon the\nleft with an invertible matrix over Fq[X].\nExtending the work of Lee and O’Sullivan [8], Beelen and Brander [2] gave a fast\nalgorithm for computing a satisfactory Q(X, Y ): start with As,ℓas a basis of Ms,ℓand\ncompute a diﬀerent, “minimal” basis of Ms,ℓwhere an element of minimal (1, k −1)-\nweighted degree appears directly.1\nIn the following section, we give further details on how to compute such a basis,\nbut our ultimate aims in Section 4 are diﬀerent: we will use a minimal basis of Ms,ℓ\nto eﬃciently compute one for Mˆs,ˆℓfor ˆs ≥s and ˆℓ> ℓ. This will allow an iterative\nreﬁnement for increasing s and ℓ, where after each step we have such a minimal basis\nfor Ms,ℓ. We then exploit this added ﬂexibility in our multi-trial algorithm.\n3 Module Minimisation\nGiven a basis of Ms,ℓ, e.g. As,ℓ, the module minimisation here refers to the process\nof obtaining a new basis, which is the smallest among all bases of Ms,ℓin a precise\nsense. We will deﬁne this and connect various known properties of such matrices, and\nuse this to more precisely bound the asymptotic complexity with which they can be\ncomputed by Alekhnovich’s algorithm.\nDeﬁnition 5 (Weak Popov Form [10]) A matrix V over Fq[X] is in weak Popov\nform if an only if the leading position of each row is diﬀerent.\n1 Actually, in both [8,2], a slight variant of As,ℓis used, but the diﬀerence is non-essential.\n\n5\nWe are essentially interested in short vectors in a module, and the following lemma\nshows that the simple concept of weak Popov form will provide this. It is a paraphrasing\nof [1, Proposition 2.3] and we omit the proof.\nLemma 6 (Minimal Degree) If a square matrix V over Fq[X] is in weak Popov\nform, then one of its rows has minimal degree of all vectors in the row space of V.\nDenote now by Wℓthe diagonal (ℓ+ 1) × (ℓ+ 1) matrix over Fq[X]:\nWℓ≜diag\n\u0010\n1, Xk−1, . . . , Xℓ(k−1)\u0011\n.\n(3)\nSince we seek an element of minimal (1, k −1)-weighted degree, we also need the\nfollowing corollary.\nCorollary 7 (Minimal Weighted Degree) Let B ∈Fq[X](ℓ+1)×(ℓ+1) be the ma-\ntrix representation of a basis of Ms,ℓ. If BWℓis in weak Popov form, then one of the\nrows of B corresponds to a polynomial in Ms,ℓwith minimal (1, k −1)-weighted degree.\nProof Let eB = BWℓ. Now, eB will correspond to the basis of an Fq[X]-module f\nM\nisomorphic to Ms,ℓ, where an element Q(X, Y ) ∈Ms,ℓis mapped to Q(X, Xk−1Y ) ∈\nf\nM. By Lemma 6, the row of minimal degree in eB will correspond to an element of f\nM\nwith minimal X-degree. Therefore, the same row of B corresponds to an element of\nMs,ℓwith minimal (1, k −1)-weighted degree.\n⊓⊔\nWe introduce what will turn out to be a measure of how far a matrix is from being\nin weak Popov form.\nDeﬁnition 8 (Orthogonality Defect [9]) Let the orthogonality defect of a square\nmatrix V over Fq[X] be deﬁned as D(V) ≜deg V −deg det V.\nLemma 9 If a square matrix V over Fq[X] is in weak Popov form then D(V) = 0.\nProof Let v0, . . . , vm−1 be the rows of V ∈Fq[X]m×m and vi,0, . . . , vi,m−1 the el-\nements of vi. In the alternating sum-expression for det V, the term Qm−1\ni=0 LT(vi)\nwill occur since the leading positions of vi are all diﬀerent. Thus deg det V\n=\nPm−1\ni=0 deg LT(vi) = deg V unless leading term cancellation occurs in the determi-\nnant expression. However, no other term in the determinant has this degree: regard\nsome (unsigned) term in det V, say t = Qm−1\ni=0 vi,σ(i) for some permutation σ ∈Sm.\nIf not σ(i) = LP(vi) for all i, then there must be an i such that σ(i) > LP(vi) since\nP\nj σ(j) is the same for all σ ∈Sm. Thus, deg vi,σ(i) < deg vi,LP(vi). As none of the\nother terms in t can have greater degree than their corresponding row’s leading term,\nwe get deg t < Pm−1\ni=0 deg LT(vi). Thus, D(V) = 0. However, the above also proves\nthat the orthogonality defect is at least 0 for any matrix. Since any matrix unimodular\nequivalent to V has the same determinant, V must therefore have minimal row-degree\namong these matrices.\n⊓⊔\nAlekhnovich [1] gave a fast algorithm for transforming a matrix over Fq[X] to weak\nPopov form. For the special case of square matrices, a ﬁner description of its asymptotic\ncomplexity can be reached in terms of the orthogonality defect, and this is essential\nfor our decoder.\n\n6\nLemma 10 (Alekhnovich’s Row-Reducing Algorithm) Alekhnovich’s\nalgo-\nrithm inputs a matrix V ∈Fq[X]m×m and outputs a unimodular equivalent matrix\nwhich is in weak Popov form. Let N be the greatest degree of a term in V. If\nN ∈O(D(V)) then the algorithm has asymptotic complexity:\nO\u0000m3 D(V) log2 D(V) log log D(V)\u0001\noperations over Fq.\nProof The description of the algorithm as well as proof of its correctness can be found\nin [1]. We only prove the claim on the complexity. The method R(V, t) of [1] computes\na unimodular matrix U such that deg(UV) ≤deg V −t or UV is in weak Popov form.\nAccording to [1, Lemma 2.10], the asymptotic complexity of this computation is in\nO(m3t log2 t log log t). Due to Lemma 9, we can set t = D(V) to be sure that UV is in\nweak Popov form. What remains is just to compute the product UV. Due to [1, Lemma\n2.8], each entry in U can be represented as p(X)Xd for some d ∈N0 and p(X) ∈Fq[X]\nof degree at most 2t. If therefore N ∈O(D(V)), the complexity of performing the\nmatrix multiplication us"
}