{
  "full_text": "End-to-end optimization of nonlinear\ntransform codes for perceptual quality\nJohannes Ballé, Valero Laparra, Eero P. Simoncelli\nCenter for Neural Science and Courant Institute of Mathematical Sciences\nNew York University, New York, NY, USA\n{johannes.balle,valero,eero.simoncelli}@nyu.edu\nAbstract—We introduce a general framework for end-to-end\noptimization of the rate–distortion performance of nonlinear\ntransform codes assuming scalar quantization. The framework\ncan be used to optimize any differentiable pair of analysis\nand synthesis transforms in combination with any differentiable\nperceptual metric. As an example, we consider a code built\nfrom a linear transform followed by a form of multi-dimensional\nlocal gain control. Distortion is measured with a state-of-the-\nart perceptual metric. When optimized over a large database of\nimages, this representation offers substantial improvements in\nbitrate and perceptual appearance over ﬁxed (DCT) codes, and\nover linear transform codes optimized for mean squared error.\nI. INTRODUCTION\nAccepted as a conference contribution to Picture Coding Symposium 2016 c⃝IEEE\nTransform coding [1] is one of the most successful areas\nof signal processing. Virtually all modern image and video\ncompression standards operate by applying an invertible trans-\nformation to the signal, quantizing the transformed data to\nachieve a compact representation, and inverting the transform\nto recover an approximation of the original signal.\nGenerally,\nthese\ntransforms\nhave\nbeen\nlinear.\nNon-\nGaussian/nonlinear aspects of signal statistics are typically\nhandled by augmenting the linear system with carefully se-\nlected nonlinearities (for example, companding nonlinearities\nto enable non-uniform quantization, prediction for hybrid\ncompression, etc.). Deciding which combination of these op-\nerations, also known as “coding tools,” are ultimately useful is\na cumbersome process. The operations are generally studied\nand optimized individually, with different objectives, and any\nproposed combination of coding tools must then be empirically\nvalidated in terms of average code rate and distortion.\nThis is reminiscent of the state of affairs in the ﬁeld of\nobject and pattern recognition about a decade ago. As in\nthe compression community, most solutions were built by\nmanually combining a sequence of individually designed and\noptimized processing stages. In recent years, that ﬁeld has seen\nremarkable performance gains [2], which have arisen primar-\nily because of end-to-end system optimization. Speciﬁcally,\nresearchers have chosen architectures that consist of a cascade\nof transformations that are differentiable with respect to their\nparameters, and then used modern optimization tools to jointly\noptimize the full system over large databases of images.\nHere, we take a step toward using such end-to-end opti-\nmization in the context of compression. We develop an op-\ntimization framework for nonlinear transform coding (ﬁg. 1),\nwhich generalizes the traditional transform coding paradigm.\ncode!\ndomain\nperceptual!\ndomain\nsignal!\ndomain\nrate\ndistortion\nFig. 1.\nNonlinear transform coding optimization framework. See text.\nAn image vector x is transformed to a code domain vector\nusing a differentiable function y = ga(x; φ) (the analysis\ntransform), parameterized by a vector φ (containing linear\nﬁlter coefﬁcients, for example). The transformed y is subjected\nto scalar quantization, yielding a vector of integer indices q\nand a reconstructed vector ˆy. The latter is then nonlinearly\ntransformed back to the signal domain to obtain the recon-\nstructed image ˆx = gs(ˆy; θ), where this synthesis transform\ngs is parameterized by vector θ.\nThe code rate is assessed by measuring the entropy, H,\nof the discrete probability distribution Pq of the quantization\nindices over an ensemble of images. Traditionally, the dis-\ntortion is assessed directly in the image domain by taking\nthe squared Euclidean norm of the difference between x and\nˆx (or equivalently, the peak signal-to-noise ratio, PSNR).\nHowever, it is well known that PSNR is not well-aligned\nwith human perception [3]. To alleviate this problem, we\nallow an additional “perceptual” transform of both vectors\nz = h(x) and ˆz = h(ˆx), on which we then compute\ndistortion using a suitable norm. A well-chosen transform h\ncan provide a signiﬁcantly better approximation of subjective\nvisual distortion than PSNR (e.g., [4]).\nII. OPTIMIZATION FRAMEWORK\nIn the transform coding framework given above, we seek to\nadjust the analysis and synthesis transforms ga and gs so as\nto minimize the rate–distortion functional:\nL[ga, gs] = H[Pq] + λ E ∥z −ˆz∥.\n(1)\nThe ﬁrst term denotes the discrete entropy of the vector\nof quantization indices q. The second term measures the\ndistortion between the reference image z and its reconstruction\nˆz in a perceptual representation. Note that both terms are\nexpectations taken over an ensemble of images.\narXiv:1607.05006v2  [cs.IT]  17 Oct 2016\n\n2\n1\n0\n1\n2\npyi\np˜yi\npˆyi\nFig. 2.\nRelationship between densities of yi, ˜yi, and ˆyi. p˜yi is a continuous\nrelaxation of the the probability masses in each of the quantization bins.\nWe wish to minimize this objective over the continuous\nparameters {θ, φ}. Most optimization methods rely on dif-\nferentiability, but both terms in the objective depend on the\nquantized values in q, and the derivative of the quantizer is dis-\ncontinuous (speciﬁcally, it is zero or inﬁnite everywhere). To\nresolve this, we propose to approximate the objective function\nwith one that is continuously differentiable, by replacing the\ndeterministic quantizer with an additive uniform noise source.\nA uniform scalar quantizer is a piecewise constant function\napplied to each of the elements of y: ˆyi = round(yi).1 The\nmarginal density of the quantized values is given by:\npˆyi(t) =\n∞\nX\nn=−∞\nPqi(n) δ(t −n),\n(2)\nwhere\nPqi(n) = (pyi ∗rect)(n), for all n ∈Z,\n(3)\nis the probability mass in the nth quantization bin. Here,\n‘∗’ represents continuous convolution, and rect is a uniform\ndistribution on (−1\n2, 1\n2). If we add independent uniform noise\nto yi, i.e., form the signal ˜yi = yi+∆yi, with ∆yi ∼rect, then\nthe density of that signal is p˜yi = pyi ∗rect. p˜yi is identical\nto Pqi at all integer locations, and provides a continuous\nrelaxation for intermediate values (ﬁg. 2). We propose to\noptimize the differential entropy h[p˜yi] as a proxy for the\ndiscrete entropy H[Pqi]. To optimize it, we need a running\nestimate of p˜yi. This estimate need not be arbitrarily precise,\nsince p˜yi is band-limited by convolution with rect. Here, we\nsimply use a non-parametric, piecewise linear function (a ﬁrst-\norder spline approximation). We also use ˜yi rather than ˆyi to\nobtain gradients of the distortion term. The overall objective\ncan be written as:\nL(θ, φ) = Ex,∆y\n\u0010\n−log2 p˜y(ga(x; φ) + ∆y)\n+ λ\n\r\rh\n\u0000gs(ga(x; φ) + ∆y; θ)\n\u0001\n−h(x)\n\r\r\n\u0011\n,\n(4)\nwhere p˜y(˜y) = Q\ni p˜yi(˜yi). This is differentiable with respect\nto θ and φ and thus suited for stochastic gradient descent.\nAlthough ﬁnding a global optimum is not guaranteed, this op-\ntimization problem is similar to others which are encountered\n1Without loss of generality, we assume that the quantization bin size is 1,\nsince we can always modify the analysis/synthesis transforms to include\na rescaling. Further, we can implement non-uniform quantization by using\nnonlinear transforms (as in companding).\nwhen optimizing deep neural networks, and which have been\nfound to behave well in practice.\nIII. CHOICE OF PARAMETRIC TRANSFORMS\nIn a traditional transform code, both analysis and synthesis\ntransforms are linear, and exact inverses of each other. In\ngeneral, this need not be the case, so long as the overall system\nminimizes the rate–distortion functional. We have previously\nshown that a linear transform followed by a particular form\nof joint local gain control (generalized divisive normalization,\nGDN) is well-matched to the local probability structure of\nphotographic images [5]. This suggests that jointly normalized\nrepresentations might also prove useful for compression. To\ndemonstrate the use of our optimization framework, we exam-\nine GDN as a candidate analysis transform, and introduce an\napproximate inverse as the corresponding synthesis transform.\nFor the perceptual transform, we use the normalized Laplacian\npyramid [4] (NLP), which mimics the local luminance and\ncontrast behaviors of the human visual system.\nA. Generalized divisive normalization (GDN)\nThe GDN transform consists of a linear decomposition H\nfollowed by a joint nonlinearity, which divides each linear\nﬁlter output by a measure of overall ﬁlter activity:\ny = ga(x; φ)\ns.t.\nyi =\nvi\n\u0000βi + P\nj γij|vj|αij\u0001εi\nand\nv = Hx,\n(5)\nwith parameter vector φ = {α, β, γ, ε, H}.\nB. Approximate inverse of GDN\nThe approximate inverse we introduce here is based on the\nﬁxed point iteration for inversion of GDN introduced in [5]. It\nis similar in spirit to the LISTA algorithm [6], in that it uses\nthe parametric form of the inversion iteration, but unties its\nparameters from their original values for faster convergence.\nWe ﬁnd that for purposes of image compression, one iteration\nis sufﬁcient:\nˆx = gs(ˆy; θ)\ns.t.\nˆx = H′w\nand\nwi = ˆyi ·\n\u0000β′\ni +\nX\nj\nγ′\nij|ˆyj|α′\nij\u0001ε′\ni,\n(6)\nwhere the parameter vector consists of a distinct set of\nparameters: θ = {α′, β′, γ′, ε′, H′}.\nC. Normalized Laplacian pyramid (NLP)\nThe NLP imitates the transformations associated with the\nearly visual system: local luminance subtraction and local\ngain control [4]. Images are decomposed using a Laplacian\npyramid [7], which subtracts a local estimate of the mean\nluminance at multiple scales. Each pyramid coefﬁcient is then\ndivided by a local estimate of amplitude (a constant plus the\nweighted sum of absolute values of neighbors). Perceptual\nquality is assessed by evaluating a norm of the difference be-\ntween reference and reconstruction in this normalized domain.\nThe parameters (constant and weights used for amplitudes)\n\n22\n24\n26\n28\n30\n32\n34\n36\n38\nPSNR [dB]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nentropy [bit/px]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nD-NLP\nDCT\nDCT (dead zone)\nlinear (MSE-optimized)\nGDN (MSE-optimized)\nlinear (NLP-optimized)\nGDN (NLP-optimized)\nFig. 3.\nRate–distortion results averaged over the entire Kodak image set\n(24 images, 752 × 496 pixels each). Reported rates are average discrete\nentropy estimates. Reported distortion is average PSNR (top) and distance\nin the normalized Laplacian pyramid domain (bottom – see text).\nare optimized to best ﬁt perceptual data in the TID2008\ndatabase [8], which includes images corrupted by artifacts aris-\ning from compression with block transforms. This simple dis-\ntortion measure provides a near-linear ﬁt to the human percep-\ntual judgments in the database, outperforming the widely-used\nSSIM [9] and MS-SSIM [10] quality metrics [4]. Examples\nand code are available at http://www.cns.nyu.edu/~lcv/NLPyr.\nIV. EXPERIMENTAL RESULTS\nThe proposed framework can be used to optimize any\ndifferentiable pair of analysis and synthesis transforms in\ncombination with any differentiable perceptual metric. Here,\nwe consider two types of transform: a linear analysis and\nsynthesis transform operating on 16 × 16 pixel blocks (in this\ncase, θ and φ each only consist of 256×256 ﬁlter coefﬁcients),\nand a 16 × 16 block GDN transform with the approximate\ninverse deﬁned above (with θ and φ consisting of ﬁlter\ncoefﬁcients and normalization parameters, as deﬁned above).\nWe optimized each of these for two distortion metrics: mean\nsquared error (MSE) and distance in the NLP domain [4].\nEach combination of transform and distortion metric was\noptimized for different values of λ. We also include a ﬁxed\nlinear transform, the 16×16 discrete cosine transform (DCT),\nwith or without dead-zone quantization, serving as a baseline.\nAll other codes (i.e., those optimized in our framework) are\nconstrained to use uniform quantization.\nWe used the Adam algorithm [11], a variant of stochastic\ngradient descent, to optimize all codes over a large collection\nof images from the ImageNet database [2], initializing the\nparameters randomly. For each optimization step, we used a\nrandomly selected mini-batch of 4 images of 128×128 pixels.\nTo prevent overﬁtting to the training database, we performed\nall evaluations on a separate set of test images.2\nWe measured rate–distortion performance for all four trans-\nform/distortion metric combinations, along with the DCT\ntransform. Note that the additive noise approximation was used\nonly for optimization, not for evaluation: We evaluated rates\nby estimating discrete entropy of the quantized code vector q.\nFor each choice of λ, we optimized a separate set of transform\nparameters, which could be stored in the encoder and decoder.\nThe only side information a real-world codec would need\nto transmit is the choice of λ and the image size (although\nit would be desirable to reduce the storage requirements by\nstoring parameters jointly for different λ).\nFor evaluation of the distortion, we ﬁrst computed the mean\nsquared error (MSE) over the entire image set for each λ, and\nthen converted these values into PSNRs (ﬁg. 3, top panel).\nIn terms of PSNR, the optimized linear transform is slightly\nworse than the DCT, because the statistics of the ImageNet\ndatabase are slightly different from the Kodak set.3 The DCT\nwith dead-zone quantization is better, but doesn’t outperform\nthe MSE-optimized GDN transform, which uses only uniform\nquantization. The NLP-optimized transforms don’t perform\nwell in terms of PSNR.\nThe situation is reversed, however, when we examine per-\nformance in terms of perceptual distortion (ﬁg. 3, bottom).\nHere, we evaluated the norm in the NLP domain (D-NLP)\nfor each image in the set, and then averaged across images.\nNote that this norm is almost (inversely) proportional to human\nperceptual mean opinion scores (MOS) across several image\ndatabases [4]. Overall, the combination of NLP and GDN\nachieves an impressive rate savings at similar quality when\ncompared with MSE-optimized methods, and with the DCT\n(both uniform and dead-zone quantizers). It is also interesting\nto note that in terms of NLP distance, the optimized linear\ntransform with uniform quantization outperforms both versions\nof the DCT. This may be because the optimized ﬁlters tend to\nbe spatially localized (and oriented/bandpass), which possibly\nleads to visually less disturbing artifacts (not shown).\nFor visual evaluation, we show results on two example\nimages (ﬁgs. 4 and 5). Results for the entire test set are\navailable at http://www.cns.nyu.edu/~balle/nlpgdn. The ﬁgures\nserve to illustrate the main effect of using a perceptual metric\nthat is aware of local relative contrast. Traditional, linear\nsystems optimized for MSE give too much preference to\nhigh-contrast regions (e.g., the snow-covered mountains in the\nbackground, or the pebbles/debris in the foreground; ﬁg. 4,\ncenter image). By performing joint normalization before quan-\ntization, the "
}