{
  "full_text": "DOME: Recommendations for supervised machine \nlearning validation in biology \nIan Walsh​1,*​, Dmytro Fishman​2,*​, Dario Garcia-Gasulla​3​, Tiina Titma​4​, Gianluca Pollastri​5​, The ELIXIR \nMachine Learning focus group​#​, Jennifer Harrow​6,+​, Fotis E. Psomopoulos​7,+​ & Silvio C.E. Tosatto​8,+ \n \n1​Bioprocessing Technology Institute, Agency for Science, Technology and Research, Singapore, ​2​Institute of \nComputer Science, University of Tartu, Estonia,  ​3​Barcelona Supercomputing Center (BSC), Barcelona, Spain, \n4​School of Information Technologies, Tallinn University of Technology, Estonia, ​5​School of Computer Science, \nUniversity College Dublin, Ireland, ​6​ELIXIR HUB, South building, Wellcome Genome Campus, Hinxton, Cambridge, \nUK, ​7​Institute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece, ​8​Dept. of \nBiomedical Sciences, University of Padua, Padua, Italy. \n*​contributed equally \n#​see list of co-authors at the end of the manuscript \n+​corresponding authors \n \n \n \nAbstract \nModern biology frequently relies on machine learning to provide predictions and improve decision\n \n \n \n \n \n \n \n \n \n \n \n \n \nprocesses. There have been recent calls for more scrutiny on machine learning performance and possible\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlimitations. Here we present a set of community-wide recommendations aiming to help establish\n \n \n \n  \n \n \n \n \n \n \n \n \nstandards of supervised machine learning validation in biology. Adopting a structured methods\n \n \n \n \n \n \n \n \n \n \n \n \ndescription for machine learning based on data, optimization, model, evaluation (DOME) will aim to help\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nboth reviewers and readers to better understand and assess the performance and limitations of a method\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nor outcome. The recommendations are formulated as questions to anyone wishing to pursue\n \n \n \n \n \n \n \n \n \n \n \n \n \nimplementation of a machine learning algorithm. Answers to these questions can be easily included in the\n \n  \n \n \n \n  \n \n \n \n \n \n  \n \nsupplementary material of published papers.  \n \nIntroduction  \nWith the steep decline in the cost of high-throughput technologies, large amounts of biological data are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbeing generated and made accessible to researchers. Machine learning (ML) has been brought into the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nspotlight as a very useful approach to understand cellular​1​, genomic​2​, proteomic​3​, post-translational​4​,\n \n  \n \n \n \n \n \n \n \n \n \nmetabolic​5 and drug discovery data​6 with the potential to result in ground-breaking medical\n \n \n \n \n \n \n \n \n \n \n \n \n \napplications​7,8​. This is clearly reflected in the corresponding growth of ML publications (Figure 1),\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreporting a wide range of modelling techniques in biology. While every novel ML method should be\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nvalidated experimentally, this happens only in a fraction of the publications​9​. This sharp increase in\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \npublications inherently requires a corresponding increase in the number and depth of peer-reviews to\n \n \n  \n \n \n \n \n \n \n \n \n \n \noffer critical assessment​10​ and improve reproducibility​11,12​.  \nGuidelines or recommendations on how to appropriately construct ML algorithms can help to ensure\n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncorrect results and predictions​13,14​. In the biomedical research field, communities have defined standard\n \n \n \n \n \n \n \n \n \n \n \n \n \nguidelines and best practices for scientific data management​15 and reproducibility of computational\n \n \n \n \n \n \n \n \n \n \n \n \ntools​16,17​. On the other hand, a demand exists in the ML community for a cohesive and combined set of\n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \n \n \n \n1 \n\nrecommendations with respect to data, the optimization techniques, the final model, and evaluation\n \n \n \n \n \n \n \n \n \n \n \n \n \nprotocols as a whole.  \n \n \nFigure 1. ​Exponential increase of ML publications in biology. The number of ML publications per year is based\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \non Web of Science from 1996 onwards using the “topic” category for “machine learning” in combination with each\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nof the following terms: “biolog*”, “medicine”, “genom*”, “prote*”, “cell*”, “post translational”, “metabolic” and\n \n \n \n \n \n \n \n \n \n \n \n \n \n“clinical”. \n \nRecently, a comment highlighted the need for standards in ML​18​, arguing for the adoption of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \non-submission checklists​10 as a first step towards improving publication standards. Through a\n \n \n \n \n \n \n \n \n \n \n \n \ncommunity-driven consensus, we propose a list of minimal requirements asked as questions to ML\n \n \n \n  \n \n \n \n \n \n \n \n \n \nimplementers (Box 1) that, if followed, will help to assess the quality and reliability of the proposed\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nmethods more faithfully. We have focused on Data, Optimization, Model and Evaluation (DOME) as\n \n \n \n \n \n \n \n \n \n \n \n \n \n \neach component of an ML implementation usually falls within one of these four topics. Importantly, no\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nspecific solutions are discussed, only recommendations (Table 1) and a checklist are provided (Box 1).\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nOur recommendations are made primarily for the case of supervised learning in biology in the absence of\n \n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \ndirect experimental validation, as this is the most common type of ML approach used. We do not discuss\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nhow ML can be used in clinical applications​19,20​. It also remains to be investigated if the DOME\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrecommendations can be extended to other fields of ML, like unsupervised, semi-supervised and\n \n \n \n \n \n \n \n \n \n \n \n \n \nreinforcement learning. \n \nDevelopment of the recommendations \n \nThe recommendations outlined below were initially formulated through the ELIXIR ML focus group\n \n \n \n \n \n \n \n \n \n \n \n \n \nafter the publication of a comment calling for the establishment of standards for ML in biology​18​.\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nELIXIR, initially established in 2014, is now a mature intergovernmental European infrastructure for\n \n \n \n \n \n \n  \n \n \n \n \n \nbiological data and represents over 220 research organizations in 22 countries across many aspects of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbioinformatics​21​. Over 700 national experts participate in the development and operation of national\n \n \n \n \n \n \n \n \n \n \n \n \n \nservices that contribute to data access, integration, training and analysis for the research community. Over\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n50 of these experts involved in the field of ML have established an ELIXIR ML focus group\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n2 \n\n(​https://elixir-europe.org/focus-groups/machine-learning​) which held a number of meetings to develop\n \n \n  \n \n \n \n \n \nand refine the recommendations based on a broad consensus among them.  \n3 \nBroad topic \nBe on the \nlookout for \nConsequences \nRecommendation(s) \nData  \n- Data size & \nquality \n \n- Appropriate \npartitioning, \ndependence \nbetween train \nand test data.  \n \n- Class imbalance \n \n- No access to \ndata \n●Data not \nrepresentative of \ndomain application. \n \n●Unreliable or biased \nperformance \nevaluation.  \n \n●Cannot check data \ncredibility. \nIndependence of optimization (training) and evaluation \n(testing) sets. (​requirement)  \nThis is especially important for meta algorithms, where \nindependence of multiple training sets must be shown to be \nindependent of the evaluation (testing) sets.  \n \nRelease data preferably using appropriate long-term \nrepositories, including exact splits (​requirement​) \n \nSufficient evidence of data size & distribution being \nrepresentative of the domain​. (​recommendation​) \n \nOptimization - Overfitting, \nunderfitting and \nillegal parameter \ntuning \n \n- Imprecise \nparameters and \nprotocols given. \n●Reported \nperformance too \noptimistic or too \npessimistic. \n \n●Models noise or miss \nrelevant \nrelationships. \n \n●Results are not \nreproducible. \nClear statement that evaluation sets were not used for feature \nselection, pre-processing steps or parameter tuning. \n(​requirement​) \n \nReporting indicators on training and testing data that can aid \nin assessing the possibility of under/overfitting e.g. train vs. \ntest error. (​requirement) \n \nRelease definitions of all algorithmic hyper-parameters, \nregularization protocols, parameters and optimization \nprotocol. (​requirement​) \n \nFor neural networks, release definitions of train and learning \ncurves. (​recommendation​) \n \nInclude explicit model validation techniques, such as N-fold \nCross validation. (​recommendation​)  \nModel \n- Unclear if black \nbox or \ninterpretable \nmodel \n \n- No access to: \nresulting source \ncode, trained \nmodels & data \n \n- Execution time \nis impractical \n●An interpretable \nmodel shows no \nexplainable \nbehaviour \n \n●Cannot cross \ncompare methods, \nreproducibility,  & \ncheck data \ncredibility.  \n \n●Model takes too \nmuch time to \nproduce results \nDescribe the choice of black box / interpretable model. If \ninterpretable show examples of it doing so. (​requirement​).  \n \nRelease of: documented source code + models + executable \n+ UI/webserver + software containers. (​recommendation​) \n \nReport execution time averaged across many repeats. If \ncomputationally tough compare to similar methods \n(​recommendation​) \nEvaluation \n- Performance \nmeasures \ninadequate \n \n- No comparisons \nto baselines or \nother methods \n \n●Biased performance \nmeasures reported.  \n \n●The method is falsely \nclaimed as \nstate-of-the-art.  \n \nCompare with public methods & simple models (baselines). \n(​requirement​)  \n \nAdoption of community validated measures and benchmark \ndatasets for evaluation. (​requirement​) \n \nComparison of related methods and alternatives on the same \ndataset. (​recommendation​) \n \n\nTable 1. Supervised ML in Biology: concerns, the consequences they impart and recommendations/requirements\n \n \n \n \n \n \n \n \n \n \n \n \n \n(recommendations in ​italics​ and requirements in ​bold​). Key terms underlined.  \n \n \nBox 1: Structuring a Materials and Methods Section for Supervised Machine Learning \n \nHere we suggest a list of questions that must be asked about each DOME section to ensure high quality \nof ML analysis. \n●\nData​: ​(this section is to be repeated separately for each dataset) \n○\nProvenance​: What is the source of the data (database, publication, direct experiment)? If\n \n  \n \n \n \n \n \n \n \n \n  \ndata is in classes, how many data points are available in each class e.g., total for the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npositive (N​pos​) and negative (N​neg​) cases? If regression, how many real value points are\n \n \n \n \n \n  \n \n \n \n \n \n \n \nthere? Has the dataset been previously used by other papers and/or is it recognized by the\n \n \n \n \n \n \n \n \n \n \n   \n \n \n \ncommunity? \n○\nData splits​: How many data points are in the training and test sets? Was a separate\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nvalidation set used, and if yes, how large was it? Is the distribution of data types in the\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \ntraining and test sets different? Is the distribution of data types in both training and test\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsets plotted?  \n○\nRedundancy between data splits​: How were the sets split? Are the training and test sets\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nindependent? How was this enforced (e.g. redundancy reduction to less than X% pairwise\n \n \n \n \n \n \n \n  \n \n \n \n \nidentity)? How does the distribution compare to previously published ML datasets?  \n○\nAvailability of data​: Is the data, including the data splits used, released in a public forum?\n \n \n  \n \n \n \n \n \n \n \n   \n \n \nIf yes, where (e.g. supporting material, URL) and how (license)?  \n \n●\nOptimization​: ​(this section is to be repeated separately for each trained model) \n○\nAlgorithm​: What is the ML algorithm class used? Is the ML algorithm new? If yes, why is\n \n  \n \n \n \n \n \n \n \n \n \n  \n \n  \nit not published in a ML journal, and why was it chosen over better known alternatives?  \n○\nMeta-predictions​: Does the model use data from other ML algorithms as input? If yes,\n \n \n \n \n \n \n \n \n \n \n \n  \n \nwhich ones? Is it completely clear that training data of initial predictors and meta-predictor\n \n   \n \n \n \n \n \n \n \n \n \n \nis independent of test data for the meta-predictor?  \n○\nData encoding​: How was the data encoded and pre-processed for the ML algorithm? \n○\nParameters​: How many parameters (​p)​ are used in the model? How was ​p​ selected? \n○\nFeatures​: How many features (​f) are used as input? Was feature selection performed? If\n \n \n \n \n \n \n \n \n \n \n \n \n  \nyes, was it performed using the training set only?  \n○\nFitting​: Is the number of parameters (​p) much larger than the number of training points\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nand/or is the number of features (f) ​large (e.g. in classification is p>>(N​pos​+N​neg​) and/or\n  \n \n \n \n \n \n \n \n \n  \n \n \nf>100)? If yes, how was over-fitting ruled out? Conversely, if the number of training\n \n \n \n \n \n \n \n \n  \n \n \n \n \npoints seem very much larger than p and/or ​f is small (e.g. N​pos​+N​neg​>>p and/or f<5) how\n \n \n \n \n \n  \n   \n \n \n \n \n \n \nwas under-fitting ruled out?  \n○\nRegularization​: were any over-fitting prevention techniques performed (e.g. early stopping\n \n \n \n \n \n \n \n \n \n \nusing a validation set)? If yes, which ones? \n4 \n- Highly variable \nperformance.  \n●Unpredictable \nperformance in \nproduction. \nEvaluate performance on a final independent hold-out set. \n(​recommendation​) \n \nConfidence intervals/error intervals and statistical tests to \ngauge prediction robustness. (​requirement​)  \n\n○\nAvailability of configuration​: Are the hyper-parameter configurations, optimization\n \n \n \n \n \n \n \n \nschedule, model files and optimization parameters reported available? If yes, where (e.g.\n \n \n \n \n \n \n \n  \n \n \n \nURL) and how (license)?  \n \n●\nModel​: ​(this section is to be repeated separately for each trained model) \n○\nInterpretability​: Is the model black box or transparent? If the model is transparent, can you\n  \n \n \n \n \n \n  \n \n  \n \n \n \ngive clear examples for this?  \n○\nOutput: ​Is the model classification or regression? \n○\nExecution time​: How much real-time does a single representative prediction require on a\n \n \n \n \n \n  \n \n \n \n \n  \nstandard machine? (e.g. seconds on a desktop PC or high-performance computing cluster)  \n○\nAvailability of software​: Is the source code released? Is a method to run the algorithm such\n \n \n  \n \n \n \n   \n  \n \n \n \n \nas executable, web server, virtual machine or container instance released? If yes, where\n \n \n \n \n \n \n \n \n \n  \n \n \n(e.g. URL) and how (license)?  \n \n●\nEvaluation​:  \n○\nEvaluation method​: How was the method evaluated? (E.g. cross-validation, independent\n \n \n \n \n \n \n \n \n \n \ndataset, novel experiments) \n○\nPerformance measures​: Which performance metrics are reported? Is this set representative\n \n \n \n \n \n \n  \n \n \n \n(e.g. compared to the literature)?  \n○\nComparison​: Was a comparison to publicly ava"
}