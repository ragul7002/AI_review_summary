{
  "full_text": "CHANGING DATA SOURCES IN THE AGE OF\nMACHINE LEARNING\nFOR OFFICIAL STATISTICS\nPRESENTED AT UNECE MACHINE LEARNING FOR OFFICIAL STATISTICS WORKSHOP 2023\nCedric De Boom\nStatistics Flanders\nBelgium\ncedric.deboom@vlaanderen.be\nMichael Reusens\nStatistics Flanders\nBelgium\nmichael.reusens@vlaanderen.be\nJune 6, 2023\nABSTRACT\nData science has become increasingly essential for the production of official statistics, as it enables\nthe automated collection, processing, and analysis of large amounts of data. With such data science\npractices in place, it enables more timely, more insightful and more flexible reporting. However, the\nquality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data\nsources and the machine learning techniques that support them. In particular, changes in data sources\nare inevitable to occur and pose significant risks that are crucial to address in the context of machine\nlearning for official statistics.\nThis paper gives an overview of the main risks, liabilities, and uncertainties associated with changing\ndata sources in the context of machine learning for official statistics. We provide a checklist of the\nmost prevalent origins and causes of changing data sources; not only on a technical level but also\nregarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions\nof changing data sources on statistical reporting. These include technical effects such as concept\ndrift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential\ndiscontinuation of the statistical offering. We offer a few important precautionary measures, such as\nenhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In\ndoing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and\nrelevance in policy-making, decision-making, and public discourse.\n1\nIntroduction\nThe field of statistics has long played a critical role in informing policy decisions, driving innovation, and advancing\nscientific knowledge. Traditional statistical methods such as surveys and censuses have provided valuable insights into\na wide range of topics, from population demographics to economic trends and public opinion. However, in recent years,\nthe increasing availability of open and large data sources has opened up new opportunities for statistical analysis. In\nparticular, the rise of machine learning has transformed the field of statistics, enabling the analysis of massive datasets,\nthe identification of complex patterns and relationships, non-linear forecasting, etc. [1, 2]. Machine learning algorithms\ncan be used to analyze data from a wide range of sources, providing insights that traditional survey methods may not\ncapture.\nThe use of machine learning for official statistics has the potential to provide more timely, accurate and comprehensive\ninsights into a wide range of societal topics [3]. By leveraging the vast amounts of data that are generated by individuals\nand entities on a daily basis, statistical agencies can gain a more nuanced understanding of trends and patterns, and\nrespond more quickly to emerging issues.\nHowever, this shift towards machine learning also presents a number of challenges. In particular, there are concerns\nabout data quality, privacy, and security, as well as the need for appropriate technical skills and infrastructure [4, 5], as\narXiv:2306.04338v1  [stat.ML]  7 Jun 2023\n\nwell as challenges related to explainability, accuracy, reproducibility, timeliness, and cost effectiveness [6]. As statistical\nagencies grapple with these challenges, it is essential to ensure that the benefits of machine learning are balanced\nagainst the risks and that the resulting insights are both accurate and representative. In this paper, we explore the\nchanging data sources in the age of machine learning for official statistics, as we believe that this pervasive issue largely\nremains underexposed, as we will explain in Section 2.2. In that respect, we highlight some of the key considerations\nfor statistical agencies looking to incorporate machine learning into their workflows in Section 3, by zooming in on the\ncauses and risks associated with using external data sources, the consequences on using such sources for statistical\nproduction, and, finally, a set of mitigations that should ideally be incorporated in any genuine deployment of machine\nlearning for official statistics.\n2\nMachine learning for official statistics\nThe data abundance in governmental, corporate, social and personal contexts, both online and offline, becomes a\ntantalizing source and opportunity for the improvement and expansion of official statistics. For example, to inquire\nabout the overall satisfaction with life of its citizens, a nation could organize periodic surveys. But when this nation has\naccess to its citizens’ social media posts, likes, reader’s letters, media consumption, ticket sales, (online) shopping carts,\netc. it could use all of these data as a proxy to extract novel and innovative statistical insights [7]. Typically, the end\nresult is either a new statistic, a statistic that complements an existing one, or an ersatz statistic that aims to replace\none or more existing statistics. We will briefly zoom in on the different components of which such a novel statistic is\ngenerally comprised.\n2.1\nMachine learning\nTo derive novel insights and innovative statistics from data, data scientists and statistical researchers often use a wide\nvariety of powerful tools that are in the realm of machine learning1. In this paper we will not go into too much detail\nabout machine learning. However, the use of data sources together with machine learning models can cause unwanted\neffects regarding statistical production, see further in Section 3. So, we deem it useful to provide a high-level overview\nof the typical process that involves machine learning for official statistics.\nMachine learning is a subdiscipline of artificial intelligence that enables machines to learn from data and improve\ntheir performance over time without being explicitly programmed. This approach involves building algorithms that\nautomatically learn from data to identify patterns, relationships, and structures that may be difficult or impossible for\nhumans to discern. The typical process of designing a machine learning algorithm consists of two main phases: training\nand inference. During the training phase, the parameters of a machine learning model are tuned to solve a specific task.\nFor this, a wide variety of data sources can be used, or even outputs from existing or pre-trained machine learning\nmodels. After the model is trained, its parameters are kept fixed so that the model can be used to predict outcomes or\nidentify patterns in new or previously unseen data. This process is called inference. It is important to keep in mind the\ndistinction between training and inference. After training, the model remains unchanged, and it remains unchanged\nuntil it is retrained again. Later, in Section 3, we will focus on the disparities that this can cause w.r.t. the inference\nphase and, in consequence, official statistics production.\nSupervised learning is one of the most common types of machine learning used for official statistics. This method\ninvolves training a model on a labeled dataset, where each data point has a known outcome or target variable. The\nmodel learns to associate features in the data with the target variable, enabling it to make predictions on new data with\nsimilar features. In the context of official statistics, supervised learning can for example be used to predict the happiness\nof an individual based on their Twitter profile [8]. Unsupervised learning, on the other hand, is used when the target\nvariable is unknown or the goal is to identify patterns or relationships within the data. In this approach, the machine\nlearning model learns to recognize similarities and differences among input data without explicit guidance from labeled\ndata. In the context of official statistics, unsupervised learning can for example be used to identify citizens, companies\nor events that are similar to each other on one or more aspects that could be hidden from plain sight [9].\nMachine learning can be used to complement or even replace official statistics, and its ability to nowcast and forecast is\nan extremely valuable addition. Modern machine learning models, tools and hardware can analyze vast amounts of data\nin real-time or near-real-time, providing more up-to-date and precise estimates of e.g. economic and social trends. By\n1Although originally (and technically) they imply different methods and techniques, the terms machine learning, data science,\nartificial intelligence, deep learning... are nowadays considered interchangeable. In this paper we consistently use the term machine\nlearning to denote the scientific discipline concerned with learning the most optimal model parameters based on data. It is a\nsubdiscipline of artificial intelligence, while deep learning is a subdiscipline of machine learning. Data science encompasses both\nmachine learning as well as data preparation, analytics and visualization.\n2\n\nincorporating machine learning into official statistical production, one can benefit from the strengths of both approaches\nand make more informed decisions based on the most current and accurate data [10].\n2.2\nExternal data sources\nLet’s focus on the data sources that will power such machine learning models.\nTheir nature, size, structure,\nfrequency... can be vastly different, they must typically be gathered ‘in the wild’ and should often be combined with\neach other to extract meaningful insights. Compared to more traditional data sources for official statistics, they may\npresent unique and appealing characteristics such as:\nBroad-spectrum – Covers a wide variety of topics.\nDiversity – A large variety of sources to cover different perspectives.\nAvailability – Lots of data is freely and easily accessible.\nSize – Some datasets can be enormous, sometimes even complete.\nStructure – Not only tabular data, but also images, video, text, audio, etc.\nTimeliness – (Near) up-to-date and real-time information.\nFrequency – Raw data on various, even very fine-grained time scales.\nGranularity – Raw data on various, even fine-grained levels of detail.\nCoverage – Various locations and regions can be filtered and covered.\nOn the other hand, before all this data is ready to be exerted for machine learning and official statistical production, a\nfew challenges need to be overcome, such as:\nData quality – Data may contain errors, biases, or missing values that need to be addressed to ensure accuracy and\nreliability.\nData interpretation – Understanding the context and meaning of data can be difficult, especially when dealing with\nunstructured data such as text or images.\nData integration – Combining data from different sources with varying structures and formats can be challenging and\ntime-consuming.\nSelection bias – Proper randomization or compiling representative population samples can be challenging, and it\ngreatly depends on the underlying data origins.\nOperationalization bias – Reproducibility can be difficult as it depends on many implicit, hidden, and/or production-\nspecific design choices [11, 12].\nComputational resources – Processing and analyzing large amounts of data may require significant computational\nresources.\nPrivacy and security – Sensitive data may need to be protected and anonymized to ensure privacy and security.\nData ethics – Data collection and use should adhere to ethical principles.\nFairness and justness – The end solution should ideally be as neutral as possible and should not discriminate [13].\nCost – All of the above requires resources, budgets and a talented workforce. In addition, the data source itself might\nneed to be purchased. In 2016, McKinsey reported that many companies have started to specialize in acquiring and\nselling data [14].\n3\n\nWith the right tools, workforce, technological advances, mindset, and legislative support, these challenges can and\nshould be manageable. The most challenging piece of the puzzle, however – and one that is more than often ignored –\nis the lack of control you can exert over the data sources that are externally gathered. As a national statistics agency,\ntraditionally, survey data and administrative records that power official statistics are completely under your own control.\nBut once you start exploiting external data sources to power novel, innovative, complimentary or ersatz statistics, this\nlack of control of your data should never be ignored, and if possible, should be front and center on your agenda early on\nin the process.\nAs the popular saying goes: “With great power comes great responsibility” (from Spider-Man, 2002). Having control\nand power over your data is essential to fulfilling your responsibilities as a statistics agency. However, in the world of\ndata, the opposite is often true: with great amounts of external data comes great powerlessness. Therefore, it is crucial\nto prioritize the issue of data control when incorporating external sources into official statistics. Taking the time to\nestablish proper protocols and procedures for external data management can prevent a multitude of issues down the line\nand ensure that the data you rely on remain accurate and trustworthy.\nThis paper delves into the pervasive problem of powerlessness and lack of control, unraveling the multifaceted aspects,\nrisks, and pitfalls that arise from utilizing external data sources for machine learning in official statistics. We will\nexplore the concepts of ‘change’ and ‘consequence’ in their most expansive interpretations to comprehensively tackle\nthis question.\n3\nThe challenge of changing data sources\nRelying heavily on external data sources for machine learning in official statistics comes with significant risks. Such a\ndependence can leave statistical agencies vulnerable since they have limited control over these sources. This situation\nis similar to how our global economy, mobility, and prosperity were once highly dependent on the availability of oil.\nSince the prices and availability of these precious resources are often beyond our control, countries can do nothing but\nendure price fluctuations and shortages. Clive Humby proclaimed in 2006 that “data is the new oil”, given its powerful\nintrinsic value. However, his statement keeps holding true in terms of vulnerability, powerlessness, and lack of control\nover external providers.\nIn the following paragraphs, we will delve into the various types and causes of data changes. We will then discuss the\nramifications of changing data sources for machine learning in official statistics. Finally, we will provide a list of best\npractices and tips, although it is important to remember that there is no free lunch: whenever we incorporate external\ndata, w"
}