{
  "abstract": "Fairness is a critical trait in decision making. As machine-learning\nmodels are increasingly being used in sensitive application domains\n(e.g. education and employment) for decision making, it is crucial\nthat the decisions computed by such models are free of unintended\nbias. But how can we automatically validate the fairness of arbitrary\nmachine-learning models? For a given machine-learning model and\na set of sensitive input parameters, our Aeqitas approach auto-\nmatically discovers discriminatory inputs that highlight fairness\nviolation.",
  "introduction": "Nondiscrimination is one of the most critical factors for social\nprotection and equal human rights. The basic idea behind non-\ndiscrimination is to eliminate any societal bias based on sensi-\ntive attributes, such as race, gender or religion. For example, it\nis not uncommon to discover the declaration of following non-\ndiscrimination policy in universities [12]:\n“The University is committed to a policy of equal oppor-\ntunity for all persons and does not discriminate on\nthe basis of race, color, national origin, age, marital\nstatus, sex, sexual orientation, gender identity, gen-\nder expression, disability, religion, height, weight,\nor veteran status in employment, educational pro-\ngrams and activities, and admissions\"\nDue to the massive progress in machine learning in the last few\ndecades, its application has now escalated over a variety of sensitive\ndomains, including education and employment. The key insight is to\nprimarily automate decision making via machine-learning models.\nOn the flip side, such models may introduce unintended societal\nbias due to the presence of bias in their training dataset. This, in\nturn, violates the non-discrimination policy that the respective\norganization or the nation is intended to fight for.",
  "related_work": "(Section 6), we outline different\nthreats to validity (Section 7) before conclusion and consequences\n(Section 8).\nBACKGROUND\nIn this section, we will discuss the critical importance of fairness\ntesting and outline the key insight behind our approach.\nImportance of fairness The usage of machine learning is increas-\ningly being observed in areas that are under the purview of anti-\ndiscrimination laws. In particular, application domains such as law\nenforcement, credit, education and employment can all benefit from\nmachine learning. Hence, it is crucial that decisions influenced by\nany machine-learning model are free of any unnecessary bias.\nAs an example, consider a machine-learning model that predicts\nthe income levels of a person. It is possible that such a model was\ntrained on a dataset, which, in turn was unfairly biased to a certain\ngender or a certain race. As a result, for all equivalent characteristics,\nbarring the gender or race, the credit worthiness of a person will\nbe predicted differently by this model.",
  "methodology": "to realize our hypothesis. Our methods differ on how we\nsystematically explore the neighbourhood of a discriminatory input\nI(d). I(d), in turn, was discovered in the first step of Aeqitas.\n(1) First a parameter p ∈Ðn\ni=1 Pi \\ Pdisc is randomly chosen.\nThen a small perturbation (i.e. change) δ is added to I(d)\np . Typ-\nically δ ∈{−1, +1} as we consider integer and real-valued\ninput parameters in our evaluation.\n(2) In the second method, we assign probabilities on how to per-\nturb a chosen parameter.",
  "results": "in δ (i.e. pertur-\nbation value) andp (i.e. the parameter to perturb) both being chosen\nrandomly. Intuitively, Aeqitas random explores inputs around\nthe neighbourhood of disc_inputs (i.e. set of discriminatory inputs\ndiscovered via global search) uniformly at random.",
  "discussion": "",
  "conclusion": "and consequences\n(Section 8).\nBACKGROUND\nIn this section, we will discuss the critical importance of fairness\ntesting and outline the key insight behind our approach.\nImportance of fairness The usage of machine learning is increas-\ningly being observed in areas that are under the purview of anti-\ndiscrimination laws. In particular, application domains such as law\nenforcement, credit, education and employment can all benefit from\nmachine learning. Hence, it is crucial that decisions influenced by\nany machine-learning model are free of any unnecessary bias.\nAs an example, consider a machine-learning model that predicts\nthe income levels of a person. It is possible that such a model was\ntrained on a dataset, which, in turn was unfairly biased to a certain\ngender or a certain race. As a result, for all equivalent characteristics,\nbarring the gender or race, the credit worthiness of a person will\nbe predicted differently by this model.",
  "references": "[1] Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017.\nURL: http://archive.ics.uci.edu/ml.\n[2] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S.\nZemel. Fairness through awareness. In Innovations in Theoretical Computer\nScience 2012, Cambridge, MA, USA, January 8-10, 2012, pages 214–226, 2012.\n[3] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers’ ro-\nbustness to adversarial perturbations."
}