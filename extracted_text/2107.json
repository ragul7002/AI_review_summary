{
  "abstract": "Forecasting windmill time series is often the basis of other processes such as anomaly detection, health monitoring, or maintenance\nscheduling. The amount of data generated by windmill farms makes online learning the most viable strategy to follow. Such\nsettings require retraining the model each time a new batch of data is available. However, updating the model with new information\nis often very expensive when using traditional Recurrent Neural Networks (RNNs). In this paper, we use Long Short-term Cognitive\nNetworks (LSTCNs) to forecast windmill time series in online settings. These recently introduced neural systems consist of chained\nShort-term Cognitive Network blocks, each processing a temporal data chunk. The learning algorithm of these blocks is based on\na very fast, deterministic learning rule that makes LSTCNs suitable for online learning tasks. The numerical simulations using a\ncase study involving four windmills showed that our approach reported the lowest forecasting errors with respect to a simple RNN,\na Long Short-term Memory, a Gated Recurrent Unit, and a Hidden Markov Model. What is perhaps more important is that the\nLSTCN approach is significantly faster than these state-of-the-art models.\n_Keywords:_ long short-term cognitive network, recurrent neural network, multivariate time series, forecasting",
  "introduction": "Humanity‚Äôs sustainable development requires the adoption\nof less environmentally aggressive energy sources. Over the\nlast years, renewable energy sources (RES) have increased their\npresence in the energy matrix of several countries. These clean\nenergies are less polluting, renewable, and abundant in nature.\nHowever, limitations such as volatility and intermittency reduce\ntheir reliability and stability for power systems. This hinders\nthe integration of renewable sources into the main grid and increases their generation costs (Sinsel et al., 2020).\nPower generation forecasting (Foley et al., 2012) is one of the\napproaches adopted to facilitate the optimal integration of RES\nin power systems. Overall, the goal of power generation forecasting is to know in advance the possible disparity between\ngeneration and demand due to fluctuations in energy sources\nunits helped prevent (to some extent) the gradient problems associated with training RNN models (Sherstinsky, 2020).\nExisting neural network approaches to wind turbine data\nforecasting do not pay enough attention to the issue of model\ncomplexity and efficiency. In most studies, authors reduce the\navailable set of input variables rather than optimizing the neural architecture used. For example, Feng et al. (2019) used the\nLSTM model with hand-picked three SCADA input variables,\nwhile Riganti-Fulginei et al. (2018) used eleven SCADA variables. Qian et al. (2019) also used LSTM to predict wind turbine data. In their study, the initial set of input variables consisted of 121 series, but this was later reduced to only three variables and then to two variables using the Mahalanobis distance",
  "methods": "learning (ML) models. Although ML models often achieve the\nhighest performance compared to other models, their deployment in real applications is limited (Jorgensen & Shaker, 2020).\nOn the one hand, most ML models require feature engineering before building the model and lack interpretability. On the\nare unable to incorporate new information into the previously\nconstructed models (Wang et al., 2019).\n‚àóCorresponding author\n_Email address:_ `g.r.napoles@uvt.nl` (Gonzalo N¬¥apoles)\nWithin clean energy approaches, wind energy has shown sustained growth in installed capacity and exploitation in recent\nyears (Ahmed et al., 2020). However, wind energy involves\nsome peculiarities to be considered when designing new forecasting solutions. Firstly, wind-based power generation can\nheavily be affected by weather variability, which means that\nthe power generation fluctuates with extreme weather phenomena (i.e., frontal systems or rapidly evolving low-pressure systems). Weather events are unavoidable, but their impact can\nbe minimized if anticipated in advance. Secondly, wind generators are dynamic systems that behave differently over time\n(i.e., due to wear of turbine components, maintenance, etc). Finally, the data generated by windmills is not static since they\nwill continue to operate, thus producing new pieces of data.\nto model the dynamics of these systems properly. This means\nthat new approaches are needed to improve the prediction of\nwind generation. The development of algorithms capable of\nlearning beyond the production phase will also allow them to\nbe kept up-to-date at all times (Losing et al., 2018).\nRecently, N¬¥apoles et al. (2021) introduced a recurrent neural\nsystem termed _Long Short-term Cognitive Network_ (LSTCN)\nthat seems suitable for online learning setting where data might\nbe volatile. Moreover, the cognitive component of such a recurrent neural network allows for interpretability and it is given\nby two facts. Firstly, neural concepts and weights have a welldefined meaning for the problem domain being modeled. This\nmeans that the resulting model can easily be interpreted with\nlittle effort. For example, in (N¬¥apoles et al., 2021) the authors\ndiscussed a measure to compute the relevance of each variable\n_Preprint submitted to Expert Systems with Applications_ _September 20, 2021_\nin multivariate time series without the need for any post-hoc\nthe network by modifying the prior knowledge matrix, which is\nnot altered during the learning process. For example, we can\nmodify some connections in the weight matrix to manually encode patterns that have not yet been observed in the data or that\nwere observed under exceptional circumstances.\nDespite the advantages of the LSTCN model when it comes\nto its forecasting capabilities, intrinsic interpretability and short\ntraining time, it has not yet been applied to a real-world problem, as far as we know. In addition, we have little knowledge\nof the performance of this brand new model on online learning\nsettings operating with volatile data that might be shortly available. Such a lack of knowledge and the challenges related to\nthe wind prediction described above have motivated us to study\nthe LSTCNs‚Äô performance on a real-world problem concerning\nthe power forecasting of four windmills.\nMore explicitly, this paper elaborates on the task of forecasting power generation in windmills using the LSTCN model.\nBy doing that, we propose an LSTCN-based pipeline to tackle\nthe related online learning problem where each data chuck is\nprocessed only once. In this pipeline, each iteration processes\na data chunk using a Short-term Cognitive Network (STCN)\nblock (N¬¥apoles et al., 2019) that operates with the knowledge\ntransferred from the previous block. This means that the model\ncan be retrained without compromising what the network has\nlearned from previous data chunks. The numerical simulations\nshow that our solution (i) outperforms state-of-the-art recurrent\nneural networks when it comes to the forecasting error and (ii)\nreports significantly shorter training and test times.\nThe remainder of the paper is organized as follows. Section\n2 revises the literature about recurrent neural networks used to\nforecast windmill time series. Section 3 presents the proposed\nLSTCN-based power forecasting model for an online learning\nsetting. Section 4 describes the case study, the state-of-the-art\nalso raised by Wang et al. (2018), suggesting Principal Component Analysis to reduce the dimensionality of the data.\nLSTM has been found to perform well even when the time\nseries variables are of incompatible types. It is worth citing the\nstudy of Lei et al. (2019), who used LSTM to predict two qualitatively different types of time series simultaneously: (i) vibration measurements that have a high sampling rate and (ii) slow\nvarying measurements (e.g., bearing temperature). It should be\nnoted that existing studies bring additional techniques that enhance the capabilities of the standard LSTM model. For example, Cao et al. (2019b) propose segmenting the data and using\nsegment-related features instead of raw signals. Xiang et al.\n(2021) also do not use raw signals. Instead, they use Convolutional Neural Networks (CNNs) to extract the dynamic features of the data, which is then fed to LSTM. A similar approach, combining CNN with LSTM, was presented by Xue\net al. (2021). Another interesting technique was introduced by\nChen et al. (2021), who combined LSTM with an auto-encoder\n(AE) neural network so that their model can detect and re\n2\nforecasting accuracy of different models and compare their empirical computational complexity.\n**3. Long Short-term Cognitive Network**\nThis section elaborates on the LSTCN model used for online\nlearning of multivariate time series. The first sub-section will\nexplain how to prepare the data to simulate an online learning\nproblem, while the remaining ones will introduce the network\narchitecture and the learning algorithm.\n_3.1. Data preparation for online learning simulations_\nLet _x_ ‚àà R be a variable observed over a discrete time scale\nwithin a period _t_ ‚àà{1, 2, . . ., _T_ } where _T_ ‚àà N is the number of\nobservations. Hence, a univariate time series can be defined as\na sequence of observations { _x_ [(] _[t]_ [)] } _[T]_ _t_ =1 [=][ {] _[x]_ [(1)][,] _[ x]_ [(2)][, . . .,] _[ x]_ [(] _[T]_ [)][}][. Sim-]\nilarly, we can define a multivariate time series as a sequence\n{ _X_ [(] _[t]_ [)] } _[T]_ _t_ =1 [=][ {] _[X]_ [(1)][,] _[ X]_ [(2)][, . . .,] _[ X]_ [(] _[T]_ [)][}][ of vectors of] _[ M]_ [ variables, such]\nthat _X_ [(] _[t]_ [)] = [ _x_ 1 [(] _[t]_ [)][,] _[ x]_ 2 [(] _[t]_ [)][, . . .,] _[ x]_ [(] _M_ _[t]_ [)][]. A model] _[ F]_ [ is used to forecast the]\nnext _L_ < _T_ steps ahead. In this paper, we assume that the model\n_F_ is built as a sequence of neural blocks with local learning capabilities, each able to capture the trends in the current time\npatch (i.e., a chunk of the time series) being processed. Both\nthe network architecture and the parameter learning algorithm\nwill be detailed in the following sub-sections.\nLet us assume that _X_ ‚àà R _[M]_ [√ó] _[T]_ is a dataset comprising a multivariate time series (Figure 1a). Firstly, we need to transform\n_X_ into a set of _Q_ tuples with the form ( _X_ [(] _[t]_ [‚àí] _[R]_ [)], _X_ [(] _[t]_ [+] _[L]_ [)] ), _t_ - _R_ 0, _t_ + _L_ ‚â§ _T_ where _R_ represents how many past steps we will\nuse to forecast the following _L_ steps ahead (see Figure 1b). In\nthis paper, we assume that _R_ = _L_ for the sake of simplicity.\nSecondly, each component in the tuple is flattened such that we\nobtain a _Q_ √ó ( _M_ ( _R_ + _L_ )) matrix. Finally, we create a partition\n_P_ = { _P_ [(1)], . . ., _P_ [(] _[k]_ [)], . . ., _P_ [(] _[K]_ [)] } from the set of flattened tuples\nsuch that _P_ [(] _[k]_ [)] = ( _P_ [(] 1 _[k]_ [)][,] _[ P]_ 2 [(] _[k]_ [)][) is the] _[ k]_ [-th time patch involving two]\ndata pieces _P_ [(] 1 _[k]_ [)][,] _[ P]_ 2 [(] _[k]_ [)] [‚àà] [R] _[C]_ [√ó] _[N]_ [, where] _[ N]_ [ =] _[ MR]_ [ and] _[ C]_ [ denotes the]\nnumber of instances in that time patch.\nFirst, the times series is split into chunks of equal length as\ndefined by the _L_ and _R_ parameters. Second, we use the resulting\nchunks to create a set of input-output pairs. Finally, we flatten\nthese pairs to obtain the tuples with the inputs to the network\nand the corresponding expected outputs.\n|24|25|41|43|39|29|40|48|19|25|45|38|47|30|34|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|22|18|14|50|26|38|23|39|45|19|44|31|36|30|12|\n|2|14|40|36|26|46|28|19|19|10|39|18|10|25|56|\n|28|4|21|3|26|14|49|26|17|48|35|12|49|21|34|\nsplit 1 split 2 split 3 split 4 split 5\n(a) Original multivariate time series\n|24|25|41|43|39|29|\n|---|---|---|---|---|---|\n|22|18|14|50|26|38|\n|2|14|40|36|26|46|\n|28|4|21|3|26|14|\n|43|39|29|40|48|19|\n|---|---|---|---|---|---|\n|50|26|38|23|39|45|\n|36|26|46|28|19|19|\n|3|26|14|49|26|17|\n|40|48|19|25|45|38|\n|---|---|---|---|---|---|\n|23|39|45|19|44|31|\n|28|19|19|10|39|18|\n|49|26|17|48|35|12|\n|25|45|38|47|30|34|\n|---|---|---|---|---|---|\n|19|44|31|36|30|12|\n|10|39|18|10|25|56|\n|48|35|12|49|21|34|\ninput output input output input output input output\n(b) Rolling mean\n|ùëÉ(1)|24|25|41|22|18|14|2|14|40|28|4|21|43|39|29|50|26|38|36|26|46|3|26|14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|ùëÉ(1)|43|39|29|50|26|38|36|26|46|3|26|14|40|48|19|23|39|45|28|19|19|49|26|17|\n|ùëÉ(2)|40|48|19|23|39|45|28|19|19|49|26|17|25|45|38|19|44|31|10|39|18|48|35|12|\n|ùëÉ(2)|25|45|38|19|44|31|10|39|18|48|35|12|47|30|34|36|30|12|10|25|56|49|21|34|\ninput output\n(c) Flattening\nFigure 1: Data pre-processing using _R_ = _L_ = 3. (a) The original multivariate\ntime series _X_ ‚àà R _[M]_ [√ó] _[T]_, with rows as variables and columns as timestamps. (b)\nSelection of sub-sequences of the time series according to parameters _R_ and _L_ .\n(c) Each sub-sequence is flattened to obtain the temporal instances. In this\nexample, the flattened dataset is divided into two time parches.\n3\nIt should be highlighted that the forecasting model will have\naccess to a time patch in each iteration, as it usually happens in\nan online scenario. If the neural model is fed with several time\nsteps, then it will be able to forecast multiple-step ahead of all\nvariables describing the time series.\n_3.2. Network architecture and neural reasoning_\nIn the online learning setting, we consider a time series (regardless of the number of observed variables) as a sequence of\ntime patches of a certain length. Such a sequence refers to the\npartition _P_ = { _P_ [(1)], . . ., _P_ [(] _[k]_ [)], . . ., _P_ [(] _[K]_ [)] } obtained with the data\npreparation steps discussed in the previous subsection. Hence,\nthe proposed network architecture consists of an LSTCN model\nable to process the sequence of time patches.\nAn LSTCN model can be defined as a collection of STCN\nblocks, each processing a specific time patch and transferring\nknowledge to the following STCN block in the form of weight\nmatrices. Figure 2 shows the recurrent pipeline of an LSTCN\ninvolving three STCN blocks to model a multivariate time series decomposed into three time patches. It should be highlighted that learning happens inside each STCN block to prevent the information flow from vanishing as the network processes more time patches. Moreover, weights estimated in the\ncurrent STCN block are transferred to the following STCN\nblock to perform the next reasoning process (see Figure 3).\nThese weights will no longer be modified in subsequent learning processes, which allow preserving the knowledge we have\nlearned up to the current time patch. That makes our approach\nsuitable for the online learning setting.\nùëä1(ùëò)\nùëÉ1(ùëò)\n|Col1|Col2|Input gate|\n|---|---|---|\n||||\n||||\n||||\nùêµ2(ùëò)\n_Transfer function_\n_Matrix multiplication_\n_Matrix-vector addition_\n‡∑†ùëÉ2(ùëò)\n‡∑†ùëÉ2\nFigure 3: Reasoning within an STCN block. Firstly, the current time patch is\nmixed with the prior knowledge matrices _W_ 1 [(] _[k]_ [)] and _B_ [(] 1 _[k]_ [)][. This operation]\nproduces a temporal state matrix _H_ [(] _[k]_ [)] . Secondly, we operate the _H_ [(] _[k]_ [)] matrix\nFigure 4 portrays the workflow of the iterative learning process of an LSTCN model. An incoming chunk of data triggers\na new training process on the last STCN block using the stored\nknowledge that the network has learned in previous iterations.\nAfter that, the prior knowledge matrices are recomputed using\nan aggregation operator and stored to be used as prior knowledge when performing reasoning.\n**4. Numerical simulations**\nIn this section, we will explore the performance (forecasting\nerror and training time) of the proposed LSTCN-based online\nforecasting model for windmill time series.\n_4.1. Description of windmill datasets_\nTo conduct our experiments, we adopted four public datasets\nfrom the ENGIE web page [1] . Each dataset corresponds to a\nwindmill where measurements were recorded every 10 minutes from 2013 to 2017. The time series of each windmill contains 264,671 timestamps. Eight variables concerning the windmill and environmental conditions were selected: _generated_\n_power, rotor temperature, rotor bearing temperature, gearbox_\n_inlet temperature, generator stator temperature, wind speed,_\n_outdoor temperature,_ and _nacelle temperature_ .\nAs of the pre-processing steps, we removed duplicated timestamps, imputed missing timestamps and values, and applied a\n1https://opendata-renewables.engie.com/explore/index\n5\nFigure 4: The LSTCN model can be seen as a sequential collection of STCN blocks that perform iterative learning. When a new chunk of data is available, a new\nSTCN block is trained and the prior knowledge is updated using an aggregation procedure.\nfashion, thus making them quite sensitive to the initial conditions. Notice that HMM also requires several iterations to build\nthe probability transition matrix.\n_Processing (EMNLP)_ (pp. 1724‚Äì1734). doi: `[https://doi.org/10.3115/](http://dx.doi.org/https://doi.org/10.3115/v1/D14-1179)`\n`[v1/D14-1179](http://dx.doi.org/https://doi.org/10.3115/v1/D14-1179)` .\nCui, Y., Bangalore, P., & Bertling Tjernberg, L. (). A fault detection framework\nusing recurrent neural networks for condition monitoring of wind turbines.\n_Wind Energy_, _n_ / _a_ . doi: `[https://doi.org/10.1002/we.2628](http://dx.doi.org/https://doi.org/10.1002/we.2628)` .\nDelgado, I., & Fahim, M. (2021). Wind turbine data analysis and LSTM-based\nprediction in SCADA system. _Energies_, _14_ . doi: `[https://doi.org/10.](http://dx.doi.org/https://doi.org/10.3390/en14010125)`\n`[3390/en14010125](http://dx.doi.org/https://doi.org/10.3390/en14010125)` .\nDu, M., Yi, J., Mazidi, P., Cheng, L., & Guo, J. (2017). A parameter selection\n_gies_, _10_ . doi: `[https://doi.org/10.3390/en10020253](http://dx.doi.org/https://doi.org/10.3390/en10020253)` .\n8\n|Col1|Original data<br>Test predictions|\n|---|---|\n|||\n|||\n|||\n|||\n(a) _L_ = 6\n|Col1|Original data<br>Test predictions|\n|---|---|\n|||\n|||\n|||\n|||\n(a) _L_ = 6\n|Col1|Original data<br>Test predictions|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n(b) _L_ = 48\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n|||\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n(c) _L_ = 72\nFigure 7: Moving average power predictions ( _w_ = 24) for the first windmill\nwith (a) _L_ = 6, (b) _L_ = 48 and (c) _L_ = 72.\n(b) _L_ = 48\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n(c) _L_ = 72\nFigure 8: Moving average power predictions ( _w_ = 24) for the second windmill\nwith (a) _L_ = 6, (b) _L_ = 48 and (c) _L_ = 72.\n9\n|Col1|Original data<br>Test predictions|\n|---|---|\n|||\n|||\n|||\n|||\n(a) _L_ = 6\n|Col1|Original data<br>Test predictions|\n|---|---|\n|||\n|||\n|||\n|||\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n|||\n(b) _L_ = 48\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n|||\n(c) _L_ = 72\nFigure 9: Moving average power predictions ( _w_ = 24) for the third windmill\nwith (a) _L_ = 6, (b) _L_ = 48 and (c) _L_ = 72.\n(a) _L_ = 6\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n|||\n(b) _L_ = 48\n|Col1|Col2|\n|---|---|\n||~~Original data~~<br>Test predictions|\n|||\n|||\n|||\n|||\n|||\n(c) _L_ = 72\nFigure 10: Moving average power predictions ( _w_ = 24) for the fourth windmill\nwith (a) _L_ = 6, (b) _L_ = 48 and (c) _L_ = 72.\n10\n_Energy_, _37_, 1‚Äì8. doi: `[https://doi.org/10.1016/j.renene.2011.05.](http://dx.doi.org/https://doi.org/10.1016/j.renene.2011.05.033)`\n`[033](http://dx.doi.org/https://doi.org/10.1016/j.renene.2011.05.033)` .\nGers, F. A., Schraudolph, N. N., & Schmidhuber, J. (2002). Learning precise\ntiming with lstm recurrent networks. _Journal of machine learning research_,\n_3_, 115‚Äì143.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. _Neu-_\n_ral Computation_, _9_, 1735‚Äì1780. doi: `[https://doi.org/10.1162/neco.](http://dx.doi.org/https://doi.org/10.1162/neco.1997.9.8.1735)`\n`[1997.9.8.1735](http://dx.doi.org/https://doi.org/10.1162/neco.1997.9.8.1735)` .\nJanssens, O., Noppe, N., Devriendt, C., de Walle, R. V., & Hoecke, S. V.\n(2016). Data-driven multivariate power curve modeling of offshore wind\nturbines. _Engineering Applications of Artificial Intelligence_, _55_, 331‚Äì338.\ndoi: `[https://doi.org/10.1016/j.engappai.2016.08.003](http://dx.doi.org/https://doi.org/10.1016/j.engappai.2016.08.003)` .\nJorgensen, K. L., & Shaker, H. R. (2020). Wind power forecasting using machine learning: State of the art, trends and challenges. _2020 8th International_\n_Conference on Smart Energy Grid Engineering, SEGE 2020_, (pp. 44‚Äì50).\ndoi: `[10.1109/SEGE49949.2020.9181870](http://dx.doi.org/10.1109/SEGE49949.2020.9181870)` .\nKong, W., Dong, Z. Y., Jia, Y., Hill, D. J., Xu, Y., & Zhang, Y. (2019). Shortterm residential load forecasting based on LSTM recurrent neural network.\n_IEEE Transactions on Smart Grid_, _10_, 841‚Äì851. doi: `[https://doi.org/](http://dx.doi.org/https://doi.org/10.1109/TSG.2017.2753802)`\n`[10.1109/TSG.2017.2753802](http://dx.doi.org/https://doi.org/10.1109/TSG.2017.2753802)` .\nKong, Z., Tang, B., Deng, L., Liu, W., & Han, Y. (2020). Condition monitoring\nof wind turbines based on spatio-temporal fusion of SCADA data by convolutional neural networks and gated recurrent units. _Renewable Energy_, _146_,\n760‚Äì768. doi: `[https://doi.org/10.1016/j.renene.2019.07.033](http://dx.doi.org/https://doi.org/10.1016/j.renene.2019.07.033)` .\nKramti, S. E., Ben Ali, J., Saidi, L., Sayadi, M., & Bechhoefer, E. (2018). Direct\nwind turbine drivetrain prognosis approach using elman neural network. In\n_2018 5th International Conference on Control, Decision and Information_\n_Technologies (CoDIT)_ (pp. 859‚Äì864). doi: `[https://doi.org/10.1109/](http://dx.doi.org/https://doi.org/10.1109/CoDIT.2018.8394926)`\n`[CoDIT.2018.8394926](http://dx.doi.org/https://doi.org/10.1109/CoDIT.2018.8394926)` .\nLei, J., Liu, C., & Jiang, D. (2019). Fault diagnosis of wind turbine based\non long short-term memory networks. _Renewable Energy_, _133_, 422‚Äì432.\ndoi: `[https://doi.org/10.1016/j.renene.2018.10.031](http://dx.doi.org/https://doi.org/10.1016/j.renene.2018.10.031)` .\nLi, T., Tang, J., Jiang, F., Xu, X., Li, C., Bai, J., & Ding, T. (2019). Fill missing\ndata for wind farms using long short-term memory based recurrent neural\nnetwork. In _2019 IEEE 3rd International Electrical and Energy Conference_\n_(CIEEC)_ (pp. 705‚Äì709). doi: `[https://doi.org/10.1109/CIEEC47146.](http://dx.doi.org/https://doi.org/10.1109/CIEEC47146.2019.CIEEC-2019284)`\n`[2019.CIEEC-2019284](http://dx.doi.org/https://doi.org/10.1109/CIEEC47146.2019.CIEEC-2019284)` .\nLin, C.-H. (2013). Recurrent modified elman neural network control of PM synchronous generator system using wind turbine emulator of pm synchronous\nservo motor drive. _International Journal of Electrical Power_ & _Energy Sys-_\n_tems_, _52_, 143‚Äì160. doi: `[https://doi.org/10.1016/j.ijepes.2013.](http://dx.doi.org/https://doi.org/10.1016/j.ijepes.2013.03.021)`\n`[03.021](http://dx.doi.org/https://doi.org/10.1016/j.ijepes.2013.03.021)` .\nLin, C.-H. (2016). Wind turbine driving a PM synchronous generator using\nnovel recurrent Chebyshev neural network control with the ideal learning\nrate. _Energies_, _9_ . doi: `[10.3390/en9060441](http://dx.doi.org/10.3390/en9060441)` .\nLiu, B., Zhao, S., Yu, X., Zhang, L., & Wang, Q. (2020). A novel deep learning\napproach for wind power forecasting based on WD-LSTM model. _Energies_,\n_13_ . doi: `[https://doi.org/10.3390/en13184964](http://dx.doi.org/https://doi.org/10.3390/en13184964)` .\nL¬¥opez, E., Valle, C., Allende-Cid, H., & Allende, H. (2020). Comparison of recurrent neural networks for wind power forecasting. In _Pattern Recognition_\n(pp. 25‚Äì34). doi: `[https://doi.org/10.1007/978-3-030-49076-8_3](http://dx.doi.org/https://doi.org/10.1007/978-3-030-49076-8_3)` .\nLosing, V., Hammer, B., & Wersing, H. (2018). Incremental on-line learning: A\nreview and comparison of state of the art algorithms. _Neurocomputing_, _275_,\n1261‚Äì1274. doi: `[https://doi.org/10.1016/j.neucom.2017.06.084](http://dx.doi.org/https://doi.org/10.1016/j.neucom.2017.06.084)` .\nManero, J., Bejar, J., & Cortes, U. (2018). Wind energy forecasting with neural\nnetworks: A literature review. _Computing and Systems_, _22_, 1085‚Äì1098.\ndoi: `[https://doi.org/10.13053/cys-22-4-3081](http://dx.doi.org/https://doi.org/10.13053/cys-22-4-3081)` .\nMishra, S., Bordin, C., Taharaguchi, K., & Palu, I. (2020). Comparison of\ndeep learning models for multivariate prediction of time series wind power\ngeneration and temperature. _Energy Reports_, _6_, 273‚Äì286. doi: `[https://](http://dx.doi.org/https://doi.org/10.1016/j.egyr.2019.11.009)`\n`[doi.org/10.1016/j.egyr.2019.11.009](http://dx.doi.org/https://doi.org/10.1016/j.egyr.2019.11.009)` .\nN¬¥apoles, G., Grau, I., Jastrzebska, A., & Salgueiro, Y. (2021). Long short-term\ncognitive networks. _[arXiv preprint arXiv:2106.16233](http://arxiv.org/abs/2106.16233)_, .\nN¬¥apoles, G., Vanhoenshoven, F., & Vanhoof, K. (2019). Short-term cognitive\nnetworks, flexible reasoning and nonsynaptic learning. _Neural Networks_,\n_115_, 72‚Äì81. doi: `[https://doi.org/10.1016/j.neunet.2019.03.012](http://dx.doi.org/https://doi.org/10.1016/j.neunet.2019.03.012)` .\nNiu, Z., Yu, Z., Tang, W., Wu, Q., & Reformat, M. (2020). Wind power forecasting using attention-based gated recurrent unit network. _Energy_, _196_,\n117081. doi: `[https://doi.org/10.1016/j.energy.2020.117081](http://dx.doi.org/https://doi.org/10.1016/j.energy.2020.117081)` .\nQian, P., Tian, X., Kanfoud, J., Lee, J. L. Y., & Gan, T.-H. (2019). A novel\nmemory neural network. _Energies_, _12_ . doi: `[https://doi.org/10.3390/](http://dx.doi.org/https://doi.org/10.3390/en12183411)`\n`[en12183411](http://dx.doi.org/https://doi.org/10.3390/en12183411)` .\nQu, K., Si, G., Sun, X., Lian, W., Huang, Y., & Li, P. (2021). Time series\nsimulation for multiple wind farms based on hmms and regular vine copulas.\n_Journal of Renewable and Sustainable Energy_, _13_, 023311. doi: `[https://](http://dx.doi.org/https://doi.org/10.1063/5.0033313)`\n`[doi.org/10.1063/5.0033313](http://dx.doi.org/https://doi.org/10.1063/5.0033313)` .\n11\nRabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. _Proceedings of the IEEE_, _77_, 257‚Äì286.\ndoi: `[10.1109/5.18626](http://dx.doi.org/10.1109/5.18626)` .\nRiganti-Fulginei, F., Sun, Z., & Sun, H. (2018). Health status assessment\nfor wind turbine with recurrent neural networks. _Mathematical Problems_\n_in Engineering_, _2018_, 6972481. doi: `[https://doi.org/10.1155/2018/](http://dx.doi.org/https://doi.org/10.1155/2018/6972481)`\n`[6972481](http://dx.doi.org/https://doi.org/10.1155/2018/6972481)` .\nSherstinsky, A. (2020). Fundamentals of recurrent neural network (RNN) and\nlong short-term memory (LSTM) network. _Physica D: Nonlinear Phe-_\n_nomena_, _404_, 132306. doi: `[https://doi.org/10.1016/j.physd.2019.](http://dx.doi.org/https://doi.org/10.1016/j.physd.2019.132306)`\n`[132306](http://dx.doi.org/https://doi.org/10.1016/j.physd.2019.132306)` .\nSinsel, S. R., Riemke, R. L., & Hoffmann, V. H. (2020). Challenges\nand solution technologies for the integration of variable renewable energy sources‚Äîa review. _Renewable Energy_, _145_, 2271‚Äì2285. doi: `[https:](http://dx.doi.org/https://doi.org/10.1016/j.renene.2019.06.147)`\n`[//doi.org/10.1016/j.renene.2019.06.147](http://dx.doi.org/https://doi.org/10.1016/j.renene.2019.06.147)` .\nStrobelt, H., Gehrmann, S., Pfister, H., & Rush, A. M. (2018). Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recurrent neural networks.\n_IEEE Transactions on Visualization and Computer Graphics_, _24_, 667‚Äì676.\ndoi: `[10.1109/TVCG.2017.2744158](http://dx.doi.org/10.1109/TVCG.2017.2744158)` .\nWang, H., Lei, Z., Zhang, X., Zhou, B., & Peng, J. (2019). A review of deep\nlearning for renewable energy forecasting. _Energy Conversion and Man-_\n_agement_, _198_, 111799. doi: `[https://doi.org/10.1016/j.enconman.](http://dx.doi.org/https://doi.org/10.1016/j.enconman.2019.111799)`\n`[2019.111799](http://dx.doi.org/https://doi.org/10.1016/j.enconman.2019.111799)` .\nWang, Y., Xie, D., Wang, X., & Zhang, Y. (2018). Prediction of wind\nturbine-grid interaction based on a principal component analysis-long short\nterm memory model. _Energies_, _11_ . doi: `[https://doi.org/10.3390/](http://dx.doi.org/https://doi.org/10.3390/en11113221)`\n`[en11113221](http://dx.doi.org/https://doi.org/10.3390/en11113221)` .\nWeerakody, P. B., Wong, K. W., Wang, G., & Ela, W. (2021). A review of irregular time series data handling with gated recurrent neural networks. _Neuro-_\n_computing_, _441_, 161‚Äì178. doi: `[https://doi.org/10.1016/j.neucom.](http://dx.doi.org/https://doi.org/10.1016/j.neucom.2021.02.046)`\n`[2021.02.046](http://dx.doi.org/https://doi.org/10.1016/j.neucom.2021.02.046)` .\nXiang, L., Wang, P., Yang, X., Hu, A., & Su, H. (2021). Fault detection of\nwind turbine based on SCADA data analysis using CNN and LSTM with\nattention mechanism. _Measurement_, _175_, 109094. doi: `[https://doi.org/](http://dx.doi.org/https://doi.org/10.1016/j.measurement.2021.109094)`\n`[10.1016/j.measurement.2021.109094](http://dx.doi.org/https://doi.org/10.1016/j.measurement.2021.109094)` .\nXue, X., Xie, Y., Zhao, J., Qiang, B., Mi, L., Tang, C., & Li, L. (2021). Attention mechanism-based CNN-LSTM model for wind turbine fault prediction using SSN ontology annotation. _Wireless Communications and Mo-_\n_bile Computing_, _2021_, 6627588. doi: `[https://doi.org/10.1155/2021/](http://dx.doi.org/https://doi.org/10.1155/2021/6627588)`\n`[6627588](http://dx.doi.org/https://doi.org/10.1155/2021/6627588)` .\nYin, A., Yan, Y., Zhang, Z., Li, C., & Sanchez, R.-V. (2020). Fault diagnosis\nof wind turbine gearbox based on the optimized lstm neural network with\ncosine loss. _Sensors_, _20_ . doi: `[https://doi.org/10.3390/s20082339](http://dx.doi.org/https://doi.org/10.3390/s20082339)` .\nZhang, F., Wen, Z., Liu, D., Jiao, J., Wan, H., & Zeng, B. (2020). Calculation and analysis of wind turbine health monitoring indicators based\non the relationships with scada data. _Applied Sciences_, _10_ . doi: `[https:](http://dx.doi.org/https://doi.org/10.3390/app10010410)`\n`[//doi.org/10.3390/app10010410](http://dx.doi.org/https://doi.org/10.3390/app10010410)` .\nZhang, X.-M., Han, Q.-L., Ge, X., & Ding, D. (2018). An overview of recent\ndevelopments in lyapunov-krasovskii functionals and stability criteria for\nrecurrent neural networks with time-varying delays. _Neurocomputing_, _313_,\n392‚Äì401. doi: `[https://doi.org/10.1016/j.neucom.2018.06.038](http://dx.doi.org/https://doi.org/10.1016/j.neucom.2018.06.038)` .\nZhen, H., Niu, D., Yu, M., Wang, K., Liang, Y., & Xu, X. (2020). A hybrid\ndeep learning model and comparison for wind power forecasting considering\ntemporal-spatial feature extraction. _Sustainability_, _12_ . doi: `[https://doi.](http://dx.doi.org/https://doi.org/10.3390/su12229490)`\n`[org/10.3390/su12229490](http://dx.doi.org/https://doi.org/10.3390/su12229490)` .\n12",
  "results": "further research directions to be explored.\n**2. Forecasting models with recurrent neural networks**\nNeural networks are a family of biology-inspired computational models that have found applications in many fields. An\nexample of engineering applications of neural models is the\nsupport of wind turbine operation and maintenance. In this area,\nneural models dedicated to the analysis of temporal data have\nproven to be quite useful. This is motivated by the fact that typical data describing the operation of a wind turbine are collected\nby sensors forming a supervisory control and data acquisition\n(SCADA) system (Du et al., 2017; Weerakody et al., 2021).\nSuch data come in the form of long sequences of numerical values, thus making Recurrent Neural Networks (RNNs) the right\nchoice for processing such data. This section briefly revises the\nliterature on the applications of RNNs on data analysis in the\narea of wind turbine operation and maintenance support.\nRNNs differ from other neural networks in the way the input data is propagated. In standard neural networks, the input\ndata is processed in a feed-forward manner, meaning the signal is transmitted unidirectionally. In RNN models, the signal\ngoes through neurons that can have backward connections from\nfurther layers to earlier layers (Che et al., 2018). Depending\non a particular neural model architecture, we can restrict the\nlayers with feedback connections to only selected ones. The\noverall idea is to allow the network to ‚Äúrevisit‚Äù nodes, which\nmimics the natural phenomenon of memory (Kong et al., 2019).\nRNNs turned out to be useful for accurate time series prediction\ntasks (Strobelt et al., 2018), including wind turbine time series\nprediction (Cui et al.).\nAs reported by Zhang et al. (2020), the task of analyzing\nwind turbine data often involves building a regression model\noperating on multi-attribute data from SCADA sensors. Such\nmodels can help us understand the data (Delgado & Fahim,\n2021; Janssens et al., 2016).\nCurrently, the most popular variant of RNN in the field of\nwind turbine data processing is the Long Short-Term Memory (LSTM) model (Hochreiter & Schmidhuber, 1997; Mishra\net al., 2020). In this model, the inner operations are defined by\nneural gates called _cell_, _input gate_, _output gate_, and _forget gate_ .\ndata. Liu et al. (2020) used wavelet decomposition together\non LSTM and wind power prediction have focused on tuning\nthe LSTM architecture, for example, by testing different transformation functions (Yin et al., 2020) or by adding a specialized\nimputation module for missing data (Li et al., 2019).\nIn addition, the bidirectional LSTM model (Gers et al., 2002)\nhas also been applied to forecast wind turbine data. The application of this model was found in the study of Zhen et al. (2020)\nand, in a deeper architecture, in the study of Cao et al. (2019a).\nWhile most of the recently published studies using neural\nmodels to predict multivariate wind turbine time series employ\nLSTM, there are also several alternative approaches focusing\non other RNN variants. For example, there are several papers\non the use of Elman neural networks in forecasting multivariate\nwind turbine data (Lin, 2013, 2016). Kramti et al. (2018) also\napplied Elman neural networks, but using a slightly modified\narchitecture. Likewise, we should mention the work of L¬¥opez\net al. (2020), which involved Echo State Network and LSTM.\nFinally, it is worth mentioning the work of Kong et al. (2020),\nin which the task of processing data from wind turbines is implemented using CNNs and Gated Recurrent Unit (GRU) (Cho\net al., 2014). The latter neural architecture is a variant of RNN,\nwhich can be seen as a simplification of the LSTM architecture.\nGRU was also used in the study of Niu et al. (2020), which employs attention mechanisms to reduce the forecasting error.\nThere are other models equipped with reasoning mechanisms\nsimilar to the one used by neural networks. In particular, the\nconcept of ‚Äúneuron‚Äù can also be found in Hidden Markov Models (HMMs) (Rabiner, 1989). Such neurons are implemented\nas _states_, and the set of states essentially plays a role analogous\nto that of hidden neurons in a standard neural network. HMMs\nhave also found applications in wind power forecasting. The\nstudies of Bhaumik et al. (2019) and Qu et al. (2021) should be\nmentioned in this context. Both research teams highlight decent\npredictions and robustness to noise in the data.\nAs pointed out by Manero et al. (2018), the task of comparing\nwind energy forecasting approaches described in the literature\nis challenging due to several factors such as the differences in\ntime series datasets, the alternative forecast horizons, etc. In\nthis paper, we will conduct experiments for key state-of-theart models for our data alongside the LSTCN formalism. The\napproximation of the expected output _P_ [(] 2 _[k]_ [)][.]\n_P_ ÀÜ2( _k_ ) = _f_       - _H_ [(] _[k]_ [)] _W_ 2 [(] _[k]_ [)] [‚äï] _[B]_ [(] 2 _[k]_ [)]       - (1)\nand\n_H_ [(] _[k]_ [)] = _f_      - _P_ [(] 1 _[k]_ [)] _[W]_ 1 [(] _[k]_ [)] [‚äï] _[B]_ [(] 1 _[k]_ [)]      - (2)\nwhere _f_ ( _x_ ) = 1+1 _e_ [‚àí] _[x]_ [, whereas ÀÜ] _[P]_ [2] ( _k_ ) is an approximation of the\nexpected block‚Äôs output. In these equations, the ‚äï operator performs a matrix-vector addition by operating each row of a given\nmatrix with a vector, provided that both the matrix and the vector have the same number of columns. Notice that we assumed\nthat values to be forecast are in the [0, 1] interval.\nAs mentioned, the LSTCN model consists of a sequential\ncollection of STCN blocks. In this neural system, the knowledge from one block is passed to the next one using an aggregation procedure (see Figure 2). This aggregation operates on\nthe knowledge learned in the previous block (that is to say, the\n_W_ 2 [(] _[k]_ [‚àí][1)] matrix). In this paper, we use the following non-linear\noperator in all our simulations:\n_W_ 1 [(] _[k]_ [)] = Œ®( _W_ 2 [(] _[k]_ [‚àí][1)] ), _k_     - 1 ‚â• 0 (3)\nand\n_B_ [(] 1 _[k]_ [)] [= Œ®][(] _[B]_ 2 [(] _[k]_ [‚àí][1)] ), _k_       - 1 ‚â• 0 (4)\nsuch that Œ®( _x_ ) = _tanh_ ( _x_ ). However, we can design operators\ncombining the knowledge in both _W_ 1 [(] _[k]_ [‚àí][1)] and _W_ 2 [(] _[k]_ [‚àí][1)] .\nThere is an important detail to be discussed. Once we have\nprocessed the available sequence (i.e., performed _K_ short-term\nreasoning steps with their corresponding learning processes),\nthe whole LSTCN model will narrow down to the last STCN\nblock. Therefore, that network will be used to forecast new data\nchunks as they arrive and a new learning process will follow, as\nneeded in online learning settings.\n_3.3. Parameter learning_\nTraining the LSTCN in Figure 2 means training each STCN\nblock with its corresponding time patch. The learning process\nwithin a block is partially independent of other blocks as it only\nuses the prior weights matrices that are transferred from the\nprevious block. As mentioned, these prior knowledge matrices\nare used to compute the temporal state and are not modified\nduring the block‚Äôs learning process.\n(1)\n(2)\n‡∑†ùëÉ2\n(0) ‡∑†ùëÉ2\n|Col1|ùëä(0), ùêµ(0)<br>1 1<br>STCN<br>ùëä(0), ùêµ(0)<br>2 2|\n|---|---|\n|||\nùëÉ1\nFigure 2: LSTCN architecture of three STCN blocks. The weights learned in\nthe current block are transferred to the following STCN block as a prior\nknowledge matrix.\nThe reasoning within an STCN block involves two gates: the\n_input gate_ and the _output gate_ . The input gate operates the prior\nknowledge matrix _W_ 1 [(] _[k]_ [)] ‚àà R _[N]_ [√ó] _[N]_ with the input data _P_ [(] 1 _[k]_ [)] [‚àà] [R] _[C]_ [√ó] _[N]_\nand the prior bias matrix _B_ [(] 1 _[k]_ [)] [‚àà] [R][1][√ó] _[N]_ [ denoting the bias weights.]\nBoth matrices _W_ 1 [(] _[k]_ [)] and _B_ [(] 1 _[k]_ [)] are transferred from the previous\ntemporal state _H_ [(] _[k]_ [)] ‚àà R _[C]_ [√ó] _[N]_ that represents the outcome that the\nblock would have produced given _P_ [(] 1 _[k]_ [)] if the block would not\nhave been adjusted to the block‚Äôs expected output _P_ [(] 2 _[k]_ [)][. Such an]\nadaptation is done in the output gate where the temporal state is\noperated with the matrices _W_ 2 [(] _[k]_ [)] ‚àà R _[N]_ [√ó] _[N]_ and _B_ [(] 2 _[k]_ [)] [‚àà] [R][1][√ó] _[N]_ [, which]\ncontain learnable weights. Figure 3 depicts the reasoning process within the _k_ -th block.\nEquations (1) and (2) show the short-term reasoning process\nof this model in the _k_ -th iteration,\n4\nThe learning task within an STCN block can be summarized\nas follows. Given a temporal state _H_ [(] _[k]_ [)] resulting from the input\ngate and the block‚Äôs expected output _P_ [(] 2 _[k]_ [)][, we need to compute]\nthe matrices _W_ 2 [(] _[k]_ [)] ‚àà R _[N]_ [√ó] _[N]_ and _B_ [(] 2 _[k]_ [)] [‚àà] [R][1][√ó] _[N]_ [.]\nMathematically speaking, the learning is performed by solving a system of linear equations that adapt the temporal state\nto the expected output. Equation (5) displays the deterministic\nlearning rule solving this regression problem,\n- _W_ 2( _k_ )\n_B_ 2 [(] _[k]_ [)]\nÔøΩÔøΩ   = Œ¶ [(] _[k]_ [)][ÔøΩ][‚ä§] Œ¶ [(] _[k]_ [)] + Œª‚Ñ¶ [(] _[k]_ [)][ÔøΩ][‚àí][1][ ÔøΩ] Œ¶ [(] _[k]_ [)][ÔøΩ][‚ä§] _f_ [‚àí] [ÔøΩ] _P_ [(] 2 _[k]_ [)] (5)\nmin-max normalization. Moreover, the data preparation procedure described in Figure 1 was applied to each dataset. Table 1\nFigure 5 shows an analysis of the influence of _w_ and _L_ on\nthe model‚Äôs behavior. The parameters were varied in the discrete set _w_ = {1, 6, 10, 20, 48, 72, 144} and _L_ = {6, 48, 72, 144},\nchanging _w_ while keeping _L_ fixed. However, the reduction in\nmodel performance was more evident when _L_ increases, which\nis usual in time series forecasting models.\nAs mentioned, the knowledge used by the first STCN is extracted from a smoothed representation of the time series data\nwe have. Nevertheless, we can start with a zero-filled matrix\nif such knowledge is not available. Figure 6 shows the MAE\nof the predictions in the training set of the four windmills in\nboth settings. Starting from scratch (no knowledge about the\ndata), the LSTCN starts predicting with a large MAE in the\nfirst time patch. As new data is received, the network updates\nits knowledge and reduces the prediction error. In this simulation, we used five time patches such that each STCN block\nis fitted on the newly received data. The LSTCN model using\ngeneral knowledge of the time series (assumed as a warm-up)\ngenerates small errors from the first time patch.\n72, respectively. More explicitly, we report the training and test\nerrors, and the training and test times (in seconds). The LSTCN\nmodel obtained the lowest MAE values in all cases (the lowest\ntest error for each windmill is highlighted in boldface). Those\nrecurrent neural networks. It should be noted, however, that\noutperforms the other models in both in accuracy and training time.\n**Training** **Test** **Training** **Test**\n**Model**\n**error** **error** **time** **time**\nWT1\nWT2\nWT3\nWT4\nLSTCN 0.0270 **0.0441** 0.33 0.03\nRNN 0.1087 0.1043 17.66 0.86\nLSTM 0.1036 0.0911 37.43 1.54\nGRU 0.1193 0.0986 45.21 1.58\nHMM 0.0588 0.1219 260.27 94.88\nLSTCN 0.0129 **0.0236** 0.30 0.03\nRNN 0.1086 0.0730 20.95 1.14\nLSTM 0.1035 0.0617 40.10 1.92\nGRU 0.1243 0.0821 37.57 1.40\nHMM 0.0280 0.0942 333.61 117.57\nLSTCN 0.0253 **0.0627** 0.35 0.03\nRNN 0.1221 0.1228 17.06 0.87\nLSTM 0.1107 0.1285 35.88 1.59\nGRU 0.1219 0.1002 37.43 1.44\nHMM 0.0586 0.1878 331.04 108.29\nLSTCN 0.0238 **0.0544** 0.34 0.03\nRNN 0.1069 0.1195 17.42 1.30\nLSTM 0.0979 0.0972 36.80 1.61\nGRU 0.1162 0.1115 35.94 1.42\nHMM 0.0543 0.1665 305.35 106.25\n6\n|0.058 0.049 0.044 0.046 0.051 0.047 0.055 0.05<br>6<br>0.077 0.081 0.075 0.079 0.075 0.078 0.078 0.078<br>48<br>0.099 0.095 0.092 0.095 0.093 0.095 0.096 0.1<br>72<br>0.25 0.28 0.26 0.28 0.28 0.3 0.49 0.49<br>144<br>1 6 10 20 48 72 144 200|0.058|0.049|0.044|0.046|0.051|0.047|0.055|0.05|\n|---|---|---|---|---|---|---|---|---|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.058<br>0.049<br>0.044<br>0.046<br>0.051<br>0.047<br>0.055<br>0.05<br>0.077<br>0.081<br>0.075<br>0.079<br>0.075<br>0.078<br>0.078<br>0.078<br>0.099<br>0.095<br>0.092<br>0.095<br>0.093<br>0.095<br>0.096<br>0.1<br>0.25<br>0.28<br>0.26<br>0.28<br>0.28<br>0.3<br>0.49<br>0.49|0.077|0.081|0.075|0.079|0.075|0.078|0.078|0.078|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.058<br>0.049<br>0.044<br>0.046<br>0.051<br>0.047<br>0.055<br>0.05<br>0.077<br>0.081<br>0.075<br>0.079<br>0.075<br>0.078<br>0.078<br>0.078<br>0.099<br>0.095<br>0.092<br>0.095<br>0.093<br>0.095<br>0.096<br>0.1<br>0.25<br>0.28<br>0.26<br>0.28<br>0.28<br>0.3<br>0.49<br>0.49|0.099<br>|0.095<br>|0.092<br>|0.095<br>|0.093<br>|0.095<br>|0.096<br>|0.1<br>|\n(a) WT1\n|0.024 0.024 0.024 0.023 0.024 0.043<br>6<br>0.045 0.045 0.041 0.042 0.041 0.047<br>48<br>0.051 0.049 0.048 0.049 0.051 0.049<br>72<br>0.43 0.099 0.1 0.087 0.13 0.1<br>144<br>1 6 10 20 48 72|0.024|0.024|0.024|0.023|0.024|0.043|0.026|0.025|\n|---|---|---|---|---|---|---|---|---|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.024<br>0.024<br>0.024<br>0.023<br>0.024<br>0.043<br>0.045<br>0.045<br>0.041<br>0.042<br>0.041<br>0.047<br>0.051<br>0.049<br>0.048<br>0.049<br>0.051<br>0.049<br>0.43<br>0.099<br>0.1<br>0.087<br>0.13<br>0.1|0.045|0.045|0.041|0.042|0.041|0.047|0.044|0.046|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.024<br>0.024<br>0.024<br>0.023<br>0.024<br>0.043<br>0.045<br>0.045<br>0.041<br>0.042<br>0.041<br>0.047<br>0.051<br>0.049<br>0.048<br>0.049<br>0.051<br>0.049<br>0.43<br>0.099<br>0.1<br>0.087<br>0.13<br>0.1|0.051<br>|0.049<br>|0.048<br>|0.049<br>|0.051<br>|0.049<br>|0.049<br>|0.052<br>|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.024<br>0.024<br>0.024<br>0.023<br>0.024<br>0.043<br>0.045<br>0.045<br>0.041<br>0.042<br>0.041<br>0.047<br>0.051<br>0.049<br>0.048<br>0.049<br>0.051<br>0.049<br>0.43<br>0.099<br>0.1<br>0.087<br>0.13<br>0.1|0.051<br>|0.049<br>|0.048<br>|0.049<br>|0.051<br>|0.049<br>|144<br>200<br>0.082<br>0.088|144<br>200<br>0.082<br>0.088|\n(b) WT2\n|0.05 0.054 0.063 0.053 0.051 0.051 0.055 0.059<br>6<br>0.074 0.073 0.075 0.075 0.073 0.075 0.074 0.072<br>48<br>0.098 0.09 0.092 0.092 0.092 0.092 0.1 0.095<br>72<br>0.24 0.27 0.28 0.28 0.29 0.33 0.49 0.29<br>144<br>1 6 10 20 48 72 144 200|0.05|Col3|0.054|0.063|Col6|0.053|0.051|0.051|Col10|0.055|Col12|0.059|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.05<br>0.054<br>0.063<br>0.053<br>0.051<br>0.051<br>0.055<br>0.059<br>0.074<br>0.073<br>0.075<br>0.075<br>0.073<br>0.075<br>0.074<br>0.072<br>0.098<br>0.09<br>0.092<br>0.092<br>0.092<br>0.092<br>0.1<br>0.095<br>0.24<br>0.27<br>0.28<br>0.28<br>0.29<br>0.33<br>0.49<br>0.29|0.074|0.074|0.073|0.075|0.075|0.075|0.073|0.075|0.075|0.074|0.074|0.072|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.05<br>0.054<br>0.063<br>0.053<br>0.051<br>0.051<br>0.055<br>0.059<br>0.074<br>0.073<br>0.075<br>0.075<br>0.073<br>0.075<br>0.074<br>0.072<br>0.098<br>0.09<br>0.092<br>0.092<br>0.092<br>0.092<br>0.1<br>0.095<br>0.24<br>0.27<br>0.28<br>0.28<br>0.29<br>0.33<br>0.49<br>0.29|0.098|0.098|0.09|0.092|0.092|0.092|0.092|0.092|0.092|0.1|0.1|0.095|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.05<br>0.054<br>0.063<br>0.053<br>0.051<br>0.051<br>0.055<br>0.059<br>0.074<br>0.073<br>0.075<br>0.075<br>0.073<br>0.075<br>0.074<br>0.072<br>0.098<br>0.09<br>0.092<br>0.092<br>0.092<br>0.092<br>0.1<br>0.095<br>0.24<br>0.27<br>0.28<br>0.28<br>0.29<br>0.33<br>0.49<br>0.29|0.24|0.24|0.27|0.28|0.28|0.28|0.29|0.33|0.33|0.49|0.49|0.29|\n|1<br>6<br>10<br>20<br>48<br>72<br>144<br>200<br>6<br>48<br>72<br>144<br>0.05<br>0.054<br>0.063<br>0.053<br>0.051<br>0.051<br>0.055<br>0.059<br>0.074<br>0.073<br>0.075<br>0.075<br>0.073<br>0.075<br>0.074<br>0.072<br>0.098<br>0.09<br>0.092<br>0.092<br>0.092<br>0.092<br>0.1<br>0.095<br>0.24<br>0.27<br>0.28<br>0.28<br>0.29<br>0.33<br>0.49<br>0.29|0.24|6<br>1|6<br>1|6<br>1|20<br>48<br>7|20<br>48<br>7|20<br>48<br>7|20<br>48<br>7|||44<br>200|44<br>200|\n(c) WT3\n|0.054 0.053 0.054 0.063 0.047 0.05<br>6<br>0.07 0.071 0.074 0.073 0.071 0.07<br>48<br>0.099 0.089 0.092 0.093 0.09 0.09<br>72<br>0.24 0.26 0.27 0.28 0.3 0.3<br>144<br>1 6 10 20 48 72|0.054|Col3|0.053|0.054|Col6|0.063|0.047|0.05|1 0.048|Col11|0.05|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.054<br>0.053<br>0.054<br>0.063<br>0.047<br>0.05<br>0.07<br>0.071<br>0.074<br>0.073<br>0.071<br>0.07<br>0.099<br>0.089<br>0.092<br>0.093<br>0.09<br>0.09<br>0.24<br>0.26<br>0.27<br>0.28<br>0.3<br>0.3|0.07|0.07|0.071|0.074|0.074|0.073|0.071|0.07|4<br>0.071|4<br>0.071|0.074|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.054<br>0.053<br>0.054<br>0.063<br>0.047<br>0.05<br>0.07<br>0.071<br>0.074<br>0.073<br>0.071<br>0.07<br>0.099<br>0.089<br>0.092<br>0.093<br>0.09<br>0.09<br>0.24<br>0.26<br>0.27<br>0.28<br>0.3<br>0.3|0.099|0.099|0.089|0.092|0.092|0.093|0.09|0.09|2<br>0.097|2<br>0.097|0.096|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.054<br>0.053<br>0.054<br>0.063<br>0.047<br>0.05<br>0.07<br>0.071<br>0.074<br>0.073<br>0.071<br>0.07<br>0.099<br>0.089<br>0.092<br>0.093<br>0.09<br>0.09<br>0.24<br>0.26<br>0.27<br>0.28<br>0.3<br>0.3|0.24|0.24|0.26|0.27|0.27|0.28|0.3|0.3|0.43|0.43|0.51|\n|1<br>6<br>10<br>20<br>48<br>72<br>6<br>48<br>72<br>144<br><br>0.054<br>0.053<br>0.054<br>0.063<br>0.047<br>0.05<br>0.07<br>0.071<br>0.074<br>0.073<br>0.071<br>0.07<br>0.099<br>0.089<br>0.092<br>0.093<br>0.09<br>0.09<br>0.24<br>0.26<br>0.27<br>0.28<br>0.3<br>0.3|0.24|6<br>1|6<br>1|6<br>1|20<br>48<br>72|20<br>48<br>72|20<br>48<br>72|20<br>48<br>72|1|44<br>200|44<br>200|\n(d) WT4\nFigure 5: MAE values obtained by the LSTCN-based model when changing the _w_ and _L_ parameters. As expected, expanding the prediction horizon (that is to say,\nincreasing the number of steps ahead to be predicted) leads to performance degradation of predictions. However, the model does not seem to be especially sensitive\nto the _w_ parameter, except for larger _L_ values where the error increases as the _w_ gets larger.\nTo alleviate the problem caused by larger prediction horizons, we could increase the batch size in our model (or decrease\nthe batch size of other recurrent models used for comparison\npurposes). In that way, we will have more data in each training\nprocess, which will likely lead to models with improved predictive power. Alternatively, we could adopt an incremental learning approach to reuse data concerning previous time patches as\ndefined by a given window parameter. However, we should be\naware that many online learning problems operate on volatile\ndata that is just available for a short period.\n**5. Concluding remarks**\nIn this paper, we investigated the performance of Long Shortterm Cognitive Networks to forecast windmill time series in online setting scenarios. This brand-new recurrent model system\nconsists of a sequence of Short-term Cognitive Network blocks.\nEach of these blocks is trained with the available data at that\nmoment in time such that the learned knowledge is propagated\nto the next blocks. Therefore, the network is able to adjust its\nknowledge to new information, which makes this model suitable for online settings since we retain the knowledge learned\nfrom previous learning processes.\nThe experiments conducted using four windmill datasets reported that our approach outperforms other state-of-the-art recurrent neural networks in terms of MAE. In addition, the pro\nposed LSTCN-based model is significantly faster than these recurrent models when it comes to both training and test times.\nSuch a feature is of paramount relevance when designing forecasting models operating in online learning modes. Regrettably,\nthe overall performance of all forecasting models deteriorated\nwhen increasing the number of steps ahead to be predicted.\nto build forecasting models with better scalability properties as\ndefined by the prediction horizon.\nBefore concluding our paper, it is worth mentioning that the\nproposed architecture for online time series forecasting is not\nrestricted to windmill data. Instead, the architecture can be applied to any univariate or multivariate time series provided that\nthe proper pre-processing steps are conducted.\n**Acknowledgement**\nAlejandro Morales and Koen Vanhoof from Hasselt University would like to thank the support received by the Flanders\nAI Research Program, as well as other partners involved in\nthis project. Agnieszka Jastrzebska‚Äôs contribution was founded\nby the POB Research Center for Artificial Intelligence and\nRobotics of Warsaw University of Technology within the Excellence Initiative Program - Research University (ID-UB). The\nauthors would like to thank Isel Grau from the Eindhoven University of Technology for revising the paper.\n7\n|Col1|Col2|Col3|Col4|without initia<br>with initial p|l prior knowledge<br>rior knowledge|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n(b) WT2\n|Col1|Col2|Col3|Col4|without initi<br>with initial p|al prior knowledge<br>rior knowledge|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n(d) WT4\n|Col1|Col2|Col3|Col4|Col5|without initia<br>with initial p|l prior knowledge<br>rior knowledge|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n(a) WT1\n|Col1|Col2|Col3|Col4|Col5|without initi<br>with initial p|al prior knowledge<br>rior knowledge|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n(c) WT3\nFigure 6: MAE values obtained by the LSTCN-based model on the four windmill datasets with and without using initial prior knowledge. It can be noticed that the\nmodel needs to process more time patches to reduce the error when the model is initialized with a random weight matrix. If this knowledge is not available, the\n**References**\nAhmed, A., & Khalid, M. (2019). A review on the selected applications\nof forecasting models in renewable power systems. _Renewable and Sus-_\n_tainable Energy Reviews_, _100_, 9‚Äì21. doi: `[https://doi.org/10.1016/j.](http://dx.doi.org/https://doi.org/10.1016/j.rser.2018.09.046)`\n`[rser.2018.09.046](http://dx.doi.org/https://doi.org/10.1016/j.rser.2018.09.046)` .\nAhmed, S. D., Al-Ismail, F. S., Shafiullah, M., Al-Sulaiman, F. A., & El-Amin,\nI. M. (2020). Grid integration challenges of wind energy: A review. _IEEE_\n_Access_, _8_, 10857‚Äì10878. doi: `[10.1109/ACCESS.2020.2964896](http://dx.doi.org/10.1109/ACCESS.2020.2964896)` .\nBhaumik, D., Crommelin, D., Kapodistria, S., & Zwart, B. (2019). Hidden\nmarkov models for wind farm power output. _IEEE Transactions on Sustain-_\n_able Energy_, _10_, 533‚Äì539. doi: `[https://doi.org/10.1109/TSTE.2018.](http://dx.doi.org/https://doi.org/10.1109/TSTE.2018.2834475)`\n`[2834475](http://dx.doi.org/https://doi.org/10.1109/TSTE.2018.2834475)` .\nCao, L., Qian, Z., Zareipour, H., Huang, Z., & Zhang, F. (2019a). Fault diagnosis of wind turbine gearbox based on deep bi-directional long short-term\nmemory under time-varying non-stationary operating conditions. _IEEE Ac-_\n_cess_, _7_, 155219‚Äì155228. doi: `[https://doi.org/10.1109/ISIE.2019.](http://dx.doi.org/https://doi.org/10.1109/ISIE.2019.8781108)`\n`[8781108](http://dx.doi.org/https://doi.org/10.1109/ISIE.2019.8781108)` .\nCao, L., Zhang, J., Wang, J., & Qian, Z. (2019b). Intelligent fault diagnosis of\nwind turbine gearbox based on long short-term memory networks. In _2019_\n_IEEE 28th International Symposium on Industrial Electronics (ISIE)_ (pp.\n890‚Äì895). doi: `[https://doi.org/10.1109/ACCESS.2019.2947501](http://dx.doi.org/https://doi.org/10.1109/ACCESS.2019.2947501)` .\nChe, Z., Purushotham, S., Cho, K., Sontag, D., & Liu, Y. (2018). Recurrent\nneural networks for multivariate time series with missing values. _Scientific_\n_Reports_, _8_, 6085. doi: `[10.1038/s41598-018-24271-9](http://dx.doi.org/10.1038/s41598-018-24271-9)` .\nChen, H., Liu, H., Chu, X., Liu, Q., & Xue, D. (2021). Anomaly detection\nand critical SCADA parameters identification for wind turbines based on\nLSTM-AE neural network. _Renewable Energy_, _172_, 829‚Äì840. doi: `[https:](http://dx.doi.org/https://doi.org/10.1016/j.renene.2021.03.078)`\n`[//doi.org/10.1016/j.renene.2021.03.078](http://dx.doi.org/https://doi.org/10.1016/j.renene.2021.03.078)` .\nCho, K., van Merri¬®enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,\nSchwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder‚Äìdecoder for statistical machine translation. In _Proceed-_\noutperforms the other models in both in accuracy and training time.\n**Training** **Test** **Training** **Test**\n**Model**\n**error** **error** **time** **time**\nWT1\nWT2\nWT3\nWT4\nLSTCN 0.0383 **0.0751** 0.79 0.04\nRNN 0.5963 0.6489 76.38 3.35\nLSTM 0.1248 0.1048 869.13 20.26\nGRU 0.1641 0.1086 762.64 13.42\nHMM 0.0794 0.1378 1585.80 445.12\nLSTCN 0.0190 **0.0407** 0.77 0.04\nRNN 0.5340 0.4560 134.81 3.80\nLSTM 0.1424 0.0777 906.91 20.49\nGRU 0.2317 0.0821 625.43 11.74\nHMM 0.0388 0.0821 1586.91 441.19\nLSTCN 0.0371 **0.0750** 0.87 0.04\nRNN 0.4991 0.5569 100.70 3.46\nLSTM 0.1332 0.1168 915.26 20.15\nGRU 0.1887 0.1100 645.27 11.18\nHMM 0.0791 0.1582 1462.68 413.93\nLSTCN 0.0351 **0.0744** 1.08 0.05\nRNN 0.3447 0.2579 169.40 3.43\nLSTM 0.1254 0.1060 888.81 20.02\nGRU 0.1634 0.1107 662.83 11.45\nHMM 0.0744 0.1327 1791.60 513.98\noutperforms the other models in both in accuracy and training time.\n**Training** **Test** **Training** **Test**\n**Model**\n**error** **error** **time** **time**\nWT1\nWT2\nWT3\nWT4\nLSTCN 0.0370 **0.0916** 0.77 0.05\nRNN 0.5187 0.5350 107.74 5.29\nLSTM 0.1376 0.1237 1516.66 39.05\nGRU 0.1891 0.1250 1852.98 29.47\nHMM 0.0832 0.1434 2070.78 594.75\nLSTCN 0.0184 **0.0479** 1.02 0.05\nRNN 0.6490 0.6317 149.05 6.79\nLSTM 0.1320 0.1021 1483.30 38.94\nGRU 0.2304 0.1641 1981.13 31.73\nHMM 0.0409 0.0817 2243.35 651.34\nLSTCN 0.0363 **0.0925** 0.82 0.04\nRNN 0.5524 0.6116 127.70 6.70\nLSTM 0.1441 0.1360 1471.54 36.88\nGRU 0.1846 0.1468 1820.24 30.58\nHMM 0.0827 0.1590 2096.07 601.62\nLSTCN 0.0355 **0.0916** 1.10 0.05\nRNN 0.5413 0.5911 115.70 5.55\nLSTM 0.1358 0.1202 1453.89 38.38\nGRU 0.1813 0.1178 1737.80 33.77\nHMM 0.0784 0.1330 2270.64 634.80\n_ing (ICAC)_ (pp. 1‚Äì4). doi: `[https://doi.org/10.23919/IConAC.2019.](http://dx.doi.org/https://doi.org/10.23919/IConAC.2019.8895037)`\n`[8895037](http://dx.doi.org/https://doi.org/10.23919/IConAC.2019.8895037)` .\nFoley, A. M., Leahy, P. G., Marvuglia, A., & McKeogh, E. J. (2012). Current",
  "conclusion": "Pearson‚Äôs correlation values among the variables are denoted as\n_min_, _med_, _max_, respectively.\nTable 1: Descriptive statistics for the windmill datasets\n|Dataset|min med max|\n|---|---|\n|1|0.0708<br>0.2799<br>0.9456|\n|2|0.0888<br>0.3032<br>0.8848|\n|3|0.0687<br>0.3014<br>0.9497|\n|4|0.0835<br>0.3148<br>0.9441|\nWe split each dataset using a hold-out approach (80% for\ntraining and 20% for testing purposes). As for the performance\nmetric, we use the mean absolute error (MAE) in all simulations reported in this section. In addition, we report the training\nand test times of each forecasting model. The training time (in\nseconds) of each algorithm was computed by adding the time\nneeded to train the algorithm in each time patch. Finally, we\narbitrarily fix the patch size to 1024.\n_4.2. Recurrent online learning models_\nWe contrast the LSTCNs‚Äô performance against four recurrent\nlearning networks used to handle online learning settings. The\nmodels adopted for comparison are GRU, LSTM, HMM, and\na fully connected Recurrent Neural Network (RNN) where the\noutput is to be fed back to the input.\nThe RNN, LSTM and GRU networks were implemented\nusing Keras v2.4.3, while HMM was implemented using the\n_hmmlearn_ library [2] . The training of these models was adapted\nto online learning scenarios. In practice, this means that RNN,\nGRU, and LSTM were retrained on each time patch using the\nprior knowledge structures learned in previous learning steps.\nIn the HMM-based forecasting model, the transition probability matrix is passed from one patch to another, and it is updated\nbased on the new information.\nIn the LSTCN model, we used _L_ = {6, 48, 72} such that _R_ = _L_\n(hereinafter we will only refer to _L_ ) and _w_ = 10. Notice that\ngiven the sampling interval of the data, six steps represent one\nhour while 72 steps represent half a day. We did not perform\nparameter tuning since the online learning setting demands fast\nre-training of these recurrent models when a new data chunk arrives. It would not be feasible to fine-tune the hyperparameters\nin each iteration since such a process is computationally demanding. Instead, we retained the default parameters reported\non the corresponding Keras layers. In the HMM-based model,\nwe used four hidden states and Gaussian emissions to generate the predictions. These parameter values were arbitrarily selected without further experimentation.\n2 `[https://github.com/hmmlearn/hmmlearn](https://github.com/hmmlearn/hmmlearn)`\nwhere Œ¶ [(] _[k]_ [)] = ( _H_ [(] _[k]_ [)] | _A_ ) such that _AC_ √ó1 is a column vector filled\nwith ones, ‚Ñ¶ [(] _[k]_ [)] denotes the diagonal matrix of (Œ¶ [(] _[k]_ [)] ) [‚ä§] Œ¶ [(] _[k]_ [)], while\nŒª ‚â• 0 denotes the ridge regularization penalty. This learning\nrule assumes that the neuron‚Äôs activation values inner layer are\nstandardized. When the final weights are returned, they are adjusted back into their original scale.\nIt shall be noted that we need to specify _W_ 1 [(0)] and _B_ [(0)] 1 [in the]\nfirst STCN block. We can use a transfer learning approach from\na previous learning process or it can be provided by domain\nexperts. Since this information is not available, we fit a single\nSTCN block without an intermediate state (i.e., _H_ [(0)] = _P_ [(0)] 1 [) on]\na smoothed representation of the whole (available) time series.\nThe smoothed time series is obtained using the moving average\ntuning was performed in our simulations.\nAnother clear advantage of LSTCN over these state-of-theart algorithms is the reduced training and test times. Re-training\nthe model quickly when a new piece of data arrives while retaining the knowledge we have learned so far is a key challenge\nin online learning settings. Recurrent neural models such as\nRNN, LSTM, GRU use a backpropagation-based learning algorithm to compute the weights regulating the network behavior.\nThe algorithm needs to iterate multiple times over the data with\nlimited vectorization possibilities.\nOverall, there is a trade-off between accuracy and training\ntime when it comes to the batch size. The smaller the batch size\nin the backpropagation learning, the more accurate the predictions are expected to be. However, smaller batch sizes make the"
}