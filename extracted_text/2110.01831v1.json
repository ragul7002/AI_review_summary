{
  "introduction": "Among the proposed means of verifying AGI, Goertzel’s 2014 survey [1] listed TheArtificialScientistTest[2],whichstipulatedthatAGIwillhavebeenachieved whenanartificialintelligence(AI) independently producesresearchsufficientto win a Nobel prize. While there is a wealth of research on AGI in general, and the automation of science has been explored to some extent [28], what would be required to satisfy this test remains unclear. This paper attempts to clar- ify what exactly is necessary to create an Artificial Scientist, and how this fits within existing approaches to AGI. A scientist may be many things, but for our purposes a simple and unam- biguous definition is best. The Royal Society’s motto, “nullius in verba”, serves nicely. Translated as “take nobody’s word for it”, it emphasises that a scientist establishes truth through experiment, not testimony. For this, our agent must possess certain qualities. 2 What is Required of an Artificial Scientist? This is not a list of every quality an Artificial Scientist ought to posses, but an attempt to identify what is necessary. Representation of Hypotheses: We’ll define a hypothesis as a statement which has a truth value. A subset of such statements are readily testable, suit- able subjects of scientific enquiry. We’ll not concern ourselves with the specific languageused to representa hypothesis beyond stating that anArtificial Scien- tist must possess a means of representing any particular hypothesis. ⋆ This work was supported by JST (JPMJMS2033; JPMJPR17G9). 2 M. T. Bennett and Y.Maruyama Inductive Inference: The Royal Society’s motto is an explicit rejection of testimony as the basis of any claim.An Artificial Scientist mustnot rely ontes- timony. Without testimony, what is true must be inferred through observation, and so the ability to perform inductive inference seems necessary. Deductive andAbductive Reasoning: Havinginferredsomethingofwhatis anagentmaytransformthisinformation,withoutspeculation,throughdeductive reasoning.Then,fromwhatis,ouragentcouldabductallthatmaybe true,but uncertain.Atestablehypothesisisonesuchthing,abductedfromwhatisknown. It seems necessary then for our agent to engage in deductive and abductive reasoning. Causal Reasoning and Explainability: The purpose of an experiment is to test a hypothesis, identifying cause and effect [20]. Arguably this is desired in order that humans may develop the technology to reproduce or prevent that effect at will. A provisionally accepted hypothesis explains phenomena. An ex- planation is only useful if it can be understood by its intended audience, and so a scientist must be able to communicate their hypotheses and the significance of their",
  "methods": ". Universalist: In the context of an arbitrary task, an agent must map situa- tions to responses. Such an agent could be conceived of as a program. A uni- versalistapproachassumesquite reasonablythatforeachenvironment(ortask) thereexistsatleastoneprogramthatalwayschoosesthebestpossibleresponses (maximising reward),andso theremust exista programthatmaximises reward across all environments (or tasks). To define this program Hutter [6] employed a formalisation of Ockham’s Razor named Solomonoff’s Universal Prior [14,15] which assigns a weight to everyprogramwhich reconstructs what the agent has experiencedofthe environmentthusfar.The samemodelcouldbe expressedby programsofvaryinglength,andsoeachprogramisevaluatedbyitsKolmogorov complexity [13]; the smallest self extracting archive in a specific language. This The Artificial Scientist 5 theoretical agent, named AIXI, has been proven to perform such that there is no other agent which outperforms it in one environment that can also equal its performance in all others (in other words, AIXI may be outperformed in a specific environment by a more specialised agent employing inductive bias that prevents it performing as well across at least some other environments). While suchanapproachencompassesbothlogicistandemergentistapproachesbecause it searches the space of all programs,it is distinct in that it begins with a guar- antee of optimal performance in terms of an arbitrary reward function. The downside; Solomonoff Induction is incomputable. However,working approxima- tions of AIXI have been constructed and this theoretical model provides useful insights about the significance of compression for intelligence. 4 Are Any of These Approaches Alone Sufficient? In the following we discuss whether any of these is sufficient or not. 4.1 Universalist For: By choosing the smallest self extracting archive as its model of the world, AIXI is in effect choosing from among possible hypotheses the most plausible according to what it has experienced so far. Certainly AIXI is capable of infer- ence, and in extracting predictions from its highly compressedrepresentationof the environment it seems reasonable to assume there must be something analo- gous to deduction or abduction taking place. Finally and most importantly, by finding the most compressed representation it must isolate causal relations [7]. Against: Assuming AIXI could be approximatedwell enough,behaviourssuch as explainability must all somehow be specified by the reward function. Creat- ing a function that guarantees such complex behaviour may not be any more achievable than AGI in general. 4.2 Emergentist For: The utility, or at least popularity, of emergentist methods such as deep learning in narrow industrial applications seems almost indisputable. Models such as GPT-3 demonstrate that even complex writing tasks are not beyond reach with existing technology. While such models tend to be difficult to in- terpret, if they can be made to infer the ambiguous rules underlying natural language then perhaps they can eventually be made explainable to a layman. Certainly an emergentist method is easily implemented in a physical robot, be- causethereisnoabstractionrequired.Ifhumanlanguageinallitsinconsistency istobeacquiredbyanAI,thenemergentistmethodsseemapromisingapproach. 6 M. T. Bennett and Y.Maruyama Against: An agent that mimics plausible explanations is of no more practical use as a scientist than an agent that gives no explanation at all. We can only trust explanations as far as we can interpret and verify them. Further, those explanationsmaybeclosertomimicrythanreasonablehypotheses.Forexample, the aforementioned GPT-3 seemed to acquire basic arithmetic, but as soon it was presented with less common sums it started giving responses that were wrong [22]. It appears to mimic arithmetic whilst having failed to grasp its rules.Thismayalsobewhymanypopularemergentistmethodsrequiresomuch datatolearnincomparisontoexistinglogisticoruniversalistmethods;anagent that only mimics must learn all correct responses by rote, while an agent that understandswhatdeterminesthecorrectnessofallresponseswillbeequippedto identifythecorrectoneinanysituation[18].Alloftheseissuesmustbeaddressed before emergentist methods alone might result in an Artificial Scientist. 4.3 Logicist For: A hypothesis represented as a statement within a predefined symbol sys- tem is interpretable. Though SAT is NP-Hard, a suite of existing solvers allows one to searchthe space of possible hypothesis fairly efficiently and with guaran- teedoptimality.Constraintsareeasilyspecifiedincomparisontoothermethods, allowing one to tailor agent behaviour to better suite specific tasks such as ex- perimental design and planning. Finally, not only is the technology to deduct, abductandinferwithsymbolicrepresentationsmature,butcausalrelationsand their computation are typically defined in terms of symbols, and remain easily verifiable after the fact. Against: Abstract symbols must somehow connect to low level sensorimotor stimuli[24].Evenifthisissolved,afixedsetofsymbolschosenbyahumanmay be far from suitable to express explanations for which we require an Artificial Scientist. Even if it were, such explanations are unlikely to be understood by eventhemostqualifiedofhumans[18,4].Whatislessobviousaboutcognitionis why symbols are formed as they are.Which abstractionsare best? This may be themostsignificantaspectofsymbolemergence,thatwhatemergesispartofthe solution to a task, expressing specifically those things of relevance in solving it [18].Whatofextending cognitioninto partsofthe environmentneverconceived ofinthespecificationofthesymbolsystem?Whilelogicistmethodsmaysurpass emergentist in terms of interpretability, causal reasoning, data-efficiency and our ability to control, emergentist methods remain the state of the art by a large margin in terms of computer vision, natural language processing and so on. More, the simple act of representing a hypothesis symbolically does not mean it is the most accurate hypothesis explaining the data. Something akin to the formalisation of Ockham’s Razor employed in universalist methods remains necessary. The Artificial Scientist 7 5 A Unifying Perspective Noneoftheaboveappearsufficientinisolation,atleastinthenearterm,forthe purpose of constructing an Artificial Scientist. However, together they address all characteristics we deemed necessary. A universalist approach reveals what hypotheses are most plausible [6,9,8] and, by virtue of optimal lossless com- pression,isolate causalrelations [7]. A logicistapproachfacilitates interpretable representation of hypotheses, planning, causal reasoning [21,20,3,5,4,17,18] and the ability to tailor behaviour to our needs with ease. An emergentist approach facilitates enactivismandthe possibility ofanemergentsymbolsystemwhichis efficient [18,17], fluid and comparable to natural language [11,10,27]. Two com- plimentary bodies of research may provide a foundation for future work along these lines;the formalisationofanarbitrarytaskandits solutions(named“The Solutions to Any Task”) [18], and a formalisation inspired by the work of Kant [5] (named “Kant’s Cognitive Architecture”). These approaches are similar but based on different premises, providing different insights. We will now briefly summarise and compare them as they pertain to developing an Artificial Sci- entist. Both attempt to infer a hypothesis which explains observed data, from which correct responses to every situation may be abducted. Combined with a SAT solver to decode responses, such an hypothesis qualifies as lossless com- pression. Finally, neither approach relies upon abstract symbols, learning from sequences of sensory data in the case of Kant’s Cognitive Architecture, and a set of sensorimotor sates in the case of The Solutions to Any Task. Kant’s Cognitive Architecture: Taking Kant’s Critique of Pure Reason as its inspiration, this approach asserts that there is no such thing as a specific judgement. Subsequently every rule is universally quantified, “doomed to gen- eralise” as the authors put it [4, p. 31]. Evans introduced notions of unity, a form of inductive bias that specifies which constraints are acceptable in terms of spacial, temporal and causal relations, along with object permanence (static unity). The solution is then made more general by choosing weak constraints. Such notions are well suited to explain all sensory data in general terms, and their implementation in the form of The Apperception Engine performs as one wouldexpect(extremelywell).Theresultinghypothesesaregeneral,perhapsnot the most general, but enough that we can say that to some extent it accounts for the universalist’s notion of plausibility (similar to KolmogorovComplexity). It is arguablyembodied to someextent, being concernedwith sensorydata,but does not account for the motor part of the sensorimotor system as is reflected by its specific form of inductive bias. The Solutions to Any Task: The notion of an arbitrary task attempts to formaliseanythingwemightcallataskintermsofitssolutions(the assumption beingthatitmustbepossibletosucceedorfailatatasktosomedegree,however the task need not be computable). In the domain of possible solutions to a given task there exist two extremes; an Intensional Solution (which may not be unique) and an Extensional Solution (which is unique). Because the task is 8 M. T. Bennett and Y.Maruyama defined in terms of a set of sensorimotor states, situation and response pairs rather than sequences of sensory data, the solution is more general, pertaining to interactive sensorimotor control rather than just the prediction of sensory input, with a much simpler inductive bias. The Intensional Solution is formed of the weakest, least specific rules necessary and sufficient to reconstruct the aforementionedsetofsensorimotorstatesgiventhe complete setofsituations,a formalisation of Ockham’s Razor which maximises the ability to generalise and identifies causal relations. The Extensional Solution is formed of the strongest, representing perfect mimicry with no generalisation. The Intensional Solution or a solution close to it represents intent, and is used to explain both symbol emergence [18] and the modelling of intent in other agents [17]. The Intensional Solution is incomputable in general, but computable if restricted to a specific hardware language (a further inductive bias). Comparison with Respect to Hypotheses: These two approaches are not mutually exclusive, but complimentary. Given a subset of possible tasks per- taining to specific types of sensory input, the inductive bias implemented in the Apperception Engine will result in an Intensional Solution. For some other tasks, it may result in something more intensional than extensional, but for the remainder of tasks it may produce nothing useful (because it assumes “there is no such thing as a specific judgement” [4, p. 31]). Consider a task to reproduce asetofrandombinarysequences,drawnfromauniformdistribution,givenonly part of each sequence. The Apperception Engine would attempt to find what all sequences in the set share in common, and fail. There are no universal rules by which the sequences may be reproduced. In contrast, the Intensional Solu- tion to the task would specify each sequence in detail, effectively rote-learning the set (the Intensional and Extensional Solutions would be one and the same). Ultimately, our Artificial Scientist should prefer hypotheses which are the most plausible regardless of task, meaning the Intensional Solution based on Ock- ham’s Razor rather than the more restrictive inductive bias towards the afore- mentionedclassofsensorysequences.However,aworkingimplementationofthe ApperceptionEngineispubliclyavailable.Itisnotanabstractpromiseoffuture capability. Comparison with Respect to Experimentation: The specifics of experi- mental design are not addressed in either case. To illustrate why observational data alone may be insufficient, consider a task to either multiply two binary numbers, or add them. There are now two correct responses when presented withanytwobinarynumbers.Asetofsituation-responsepairsisobserved,stip- ulating one and only one correctresponse for each situation. A response here is theresultofeitheraddition,ormultiplication,thechoiceofwhichbeingmadeby a fair coin flip (uniformly distributed). This last part is important, as knowing a situation is observed with addition, or with multiplication, would not convey anyusefulinformationwhichwouldallowonetoformtwouniversallyapplicable mutually exclusive rules based on aspects of the situation. The Apperception Engine would fail because there is no single universal rule which is mutually The Artificial Scientist 9 exclusivewithallothers(thereareinfacttwoconcurrentrules).Incontrast,the IntensionalSolutionwouldspecifybothcorrectrules,albeitrestrictedtospecific rote memorised situations (it would be more specific than necessary because it would state that addition is necessary in some specific situations, and multipli- cationin all others).To find anIntensionalSolution that properlydescribes the task (astwoseparateconcurrentanduniversallyapplicable rules)wouldrequire the agentexperiment,to testwhether eachrule holds in specific situations,orif bothapplyinallsituations.AnIntensionalSolutionaloneisinsufficient,because the information necessary to confirm that two mutually exclusive responses are valid in any given situation is not present in the existing set of observations. A hypothesis must be formed, abducted from the data, and tested to confirm whetherthe rulesapplyconcurrentlyorinanalternatingpattern.Asillustrated by the above task with two solutions, regardless of inductive bias experiment remainsnecessaryto guaranteethe mostdata-efficientmode oflearning(to find the most correct hypothesis). Comparison with Respect to Symbol Emergence and Explainability: While Kant’s Cognitive Architecture does not attempt to addresssymbol emer- gence, Evans has proposed the integration with subsymbolic methods such as neural networks to ground abstract symbols [4], similar to the aforementioned work on symbol emergence in robotics [11]. These could then be composed into concepts by the Apperception Engine, albeit limited by the inductive bias to- wards sensory input. This would also seem to imply a constant and unchanging set ofabstractsymbols specified by a human (where the exactmeaning ofthose symbols is determined by learning algorithm), but it is a step in the direction ofan emergentsymbolsystem. However,The Solutions to Any Taskposits that notallsymbolsystemsareequal,thatsomearebettersuitedtodescribewhatis relevanttoataskthanothers.Assuchasymbolsystemisimplicitinthesolution to a task, clustering sensorimotorstimuli in terms of what is relevant to success in that task [18]. The distinction between any two symbols is then fluid, depen- dentuponcontext.Thistheoryattempts toaddressnotonlythe emergenceofa symbolsystem,butthe modelling ofintentinother agents,empathy [17],inaid of constructing explanations in natural language tailored to what the audience understands and considers important. To learn such a symbol system requires that symbols are not learned separately from one’s model of the world, but as part of it so that they have meaning (“significance in terms of a goal”) beyond their relation to other symbols. This is a fundamentally different approach to symbol emergence to the one proposed for the Apperception Engine, sharing in more in common with the notion of concepts discussed in Kant’s Cognitive Architecture, but perhaps requiring a fundamentally different inductive bias to that of the Apperception Engine.",
  "results": "in terms of what its audience values and understands. Mere in- terpretability is insufficient for more complex phenomena, as interpreting even simple symbolic models of well understood subject matter requires a great deal of technical expertise. Pushing forward the boundaries of scientific achievement would produce models of such complexity as to be beyond the capabilities of human interpretation. Fluency in natural language is desirable. Evaluation of Hypotheses: Ahypothesismustatleastbefalsifiable,positing cause and effect. If we assume computational resources are finite, then there is a cost to consider in the search for hypotheses. The question then is which hypotheses ought to be abducted. Hume’s Guillotine tells us one cannot derive an ought from an is, and so we must give our agent an ought by which to judge hypotheses.If one is to choose between severalhypotheses,the truth ofany one of which would serve to explain observed phenomena, then it seems reasonable to assert that one should start by testing the most plausible, the most likely to be true. One must also consider what is gained by proving or disproving any hypothesis.Yes,onemaychoosetoinvestigatewithscientificrigourproblemsof no interest to anyone, but we would hesitate to claim this is accepted practice for contemporary scientists. Hence we assert that an Artificial Scientist must have a means of judging the plausibility of, and potential profit in any line of inquiry; a heuristic to inform its search of the space of possible hypotheses. Experimental Design, Evaluation and Planning: Totestahypothesisone must design an experiment that isolates and tests the hypothesised cause of an effect, ideally controlling for all other variables. Each experiment costs re- sources, and the information gained should be evaluated in terms of expected benefit across hypotheses and future experiments. For example, a valuable ex- periment may not entirely confirm or disprove any one hypothesis, but may The Artificial Scientist 3 provide information allowing an agent to more efficiently select future experi- mentsthatwillconfirmordisprovemanyhighpriorityhypotheses.Therearealso risks to consider in an experiment. An experiment with a high expected utility may threatenthe agent’s continuedexistence, andso some form ofrisk aversion may be necessary (for example, when planning future experiments the geomet- ric mean may be more appropriate than the arithmetic mean when computing utility, because the utility of future experiments depends upon the outcome of preceding experiments and their impact on available resources and capability). An agentmust identify what novelinformation wouldconfirm or disprovethose abductedhypothesesofthegreatestexpectedutility.Itmustdesignexperiments thatwillconveysaidnovelinformationandcompareandplanexperimentsbased on opportunity cost and risk. Enactivism: To performexperiments, an agentmust possess a means of inter- actingwiththeenvironment.Theprocessofexperimentationcouldbeperceived as enactive cognition [25], which posits cognition arises through the interaction of an organism with its environment. It assumes cognition is embodied, embed- ded to function within the confines of a specific environment, enacted through what an organism does and, finally, extending into that environment to store and retrieve information. All of this seems obviously necessary to conduct ex- periments in the environment. We are not offering an unqualified endorsement of embodied cognition; after all it is arguable that even a laptop has a body [16]. However, experimentation has certain physical requirements, and if one is able to performtargetedexperiments thatobtainspecific novelinformationand isolate causalrelations, the process of learning may proceed much faster than if one is forced to wait until that same novel information is observed by chance. 3 Three Relevant Approaches to AGI ForthefollowingwedrawheavilyuponGoertzel’s2014surveyofthefield[1],de- viating slightly to include recent developments and adjust the categoriesto suit ourpurposes.Tostandardisethetermswithwhichwecomparetheseapproaches we employ a model of an arbitrary task, we treat the application of intelligence asprediction,andsodefine eachoftheseapproachesastryingtopredictthe ap- propriateresponser givenasituations.Forthesakeofbrevityappropriateness, situationandresponsecanbereadusingtheircommonlanguagedefinitions,but more concrete definitions are available if the reader is curious [18,17]. Logicist: We’ll use the term logicist as a catch all for approaches that employ symbolic knowledge representation and inductive logic programming for learn- ing. A finite set of symbols are used to describe discrete environment states, actions and so forth. Symbols may be joined by logical connectives to specify statements that have a truth value within any given state. Degrees of belief in the truth of a statement, probabilities, may be assigned to statements where the environment is stochastic or partially observable. Statements may also be 4 M. T. Bennett and Y.Maruyama employed as constraints to define what behaviour is permitted; the rules of a task.An agentmay infer suchconstraintsfromexamplesthroughwhatis called inductive logic programming. As with Goertzel’s symbolic category [1] a logi- cist approach typically subscribes to The Physical Symbol System Hypothesis [23,19], meaning the abstract symbols it employed are assumed to be grounded in hardware, but how is a matter left for implementation. We go a step further and say that a logicist approach is perhaps better characterised as employing a constantandunchangingvocabularyofsymbolswhichareinsomemannerspec- ified by a human. Such an agent typically learns rules that determine whether a chosen response r is correct in a situation s, from which a correct response r may then be derived for a given a new situation s, allowing the agent to gener- alise. This is as opposed to modelling r as a function of s directly. As a result the choice of response is technically interpretable, but only in a limited sense, because the meaning of symbols is dyadic, exact, andparasitic on the meanings intheheadofthehumaninterpreter[24].Thisisasopposedtoemergentnatural language, in which meaning is fluid and open to interpretation. Emergentist: Emergentist approaches take as their premise that complex be- haviour and what we call abstract symbol systems may emerge through sub- symbolic processes, such as the interaction of neurons. This process is called symbol emergence [10], and typically uses approaches such as latent dirichlet allocation to cluster multi-modal sensorimotor stimuli into perceptual [26] or sensorimotor [27] symbols with fluid definitions akin to natural language. For example, researchers created an agent able to associate the sound of the word “cup” with the image and other characteristics of a cup as experienced by that agent [11], but success in constructing more complex symbols such as “opera” or “belief” remains elusive. Such an emergent symbol system could be used in conjunctionwithalogicistapproach,whichcouldthenlearnthe rulesofagiven task in terms of these symbols. However,we extend this category to encompass approaches that bypass the construction of an abstract symbol system entirely in favor of directly modelling correct responses as a function of situations. For example, a neural network performing image classification. This is assumed to implicitly model the rules that determine the correctness of responses, but as a result is not as readily interpretable as logicist"
}