{
  "abstract": "- This white paper describes the LSST Dark Energy Science Collaboration (DESC), whose goal is the study of dark energy and related topics in fundamental physics with data from the Large \n- Synoptic Survey Telescope (LSST). It provides an overview of dark energy science and describes the current and anticipated state of the field. It makes the case for the DESC by laying out a robust analytical framework for dark energy science that has been defined by its members and \n- the comprehensive three-year work plan they have developed for implementing that framework. The analysis working groups cover five key probes of dark energy: weak lensing, large scale structure, galaxy clusters, Type Ia supernovae, and strong lensing. The computing working groups span cosmological simulations, galaxy catalogs, photon simulations and a systematic software and computational framework for LSST dark energy data analysis. The technical \n- working groups make the connection between dark energy science and the LSST system. The",
  "methods": "- parameter estimation. The white paper describes several high priority tasks identified by each of the 16 working groups. Over the next three years these tasks will help prepare for LSST analysis, make synergistic connections with ongoing cosmological surveys and provide the dark energy \n- community with state of the art analysis tools. Members of the community are invited to join the DESC, according to the membership policies described in the white paper. Applications to sign up for associate membership may be made by submitting the Web form at \n- `http://www.slac.stanford.edu/exp/lsst/desc/signup.html` with a short statement of the work they wish to pursue that is relevant to the DESC. \n3 \n## **Contents** \n|1|LSST as a Dark Energy Experiment<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . .|7|\n|---|---|---|\nThe combination of all precursor surveys still constitute a dataset that is an order of magnitude smaller than the one expected from LSST. In particular LSST will study galaxies at significantly higher redshifts than precursor surveys. LSST will need to incorporate hard-won advances and lessons from these earlier surveys, and prepare to go significantly beyond them in its control and mitigation of systematic errors. \n14 \n_1.6 Gains from LSST_ \n**==> picture [213 x 198] intentionally omitted <==**\n16 \n## **2 The LSST Dark Energy Science Collaboration** \n## **2.1 Need for the Collaboration** \n## **3.1.2 Analysis steps** \n## _**Processing of LSST data**_ \nAlmost every step in the process of extracting shear information from the images of galaxies has the potential to induce systematic errors, so it is worth briefly reviewing those steps. After the basic data processing (bias and overscan subtraction, flat fielding, cross-talk correction, interpolation over defects, etc.), the first step is to detect the stars and galaxies in the image. This requires estimating the level of the (spatially varying) sky background, convolving the image by some target kernel, and then finding peaks in the convolved image at some level above the background. Crowded regions present a challenge; since the light in any one pixel may come from multiple objects, some \n29 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n“deblending” algorithm is required to determine how much of each pixel’s flux corresponds to each detected object. \nThe objects are next classified as stars and galaxies, typically by deriving the probability of each source being a star. Once a robust sample of stars is identified, these stars are used to estimate the point-spread function (PSF). We try to obtain as much information as we can about the two-dimensional structure of the PSF, since all aspects of the PSF profile will end up affecting the shapes of the galaxies. Thus, complicated models are often used, such as a set of 2-d basis functions, or models of the telescope optics including the relevant Zernicke functions. \nThe PSF varies nontrivially across the image due to both the telescope optics and random atmospheric turbulent variation. Since the galaxies have different locations than the stars we use to measure the PSF, we must interpolate the PSF to the location of each galaxy. The simplest way to interpolate the PSF is to fit a function to the measurements in each image separately. However, since the PSF patterns are correlated from one image to the next (being caused by the same optical system), it is often helpful to incorporate information from multiple images (potentially all LSST images) to better constrain the variation. In addition, the PSF varies with wavelength. Since galaxies have different spectral energy distributions (SEDs) than stars, the PSF also needs to be interpolated in color space, a detail that has generally been ignored in surveys to date, but which will be critical for LSST. \nIn addition to being convolved by the PSF, galaxy shapes are also distorted by the telescope optics and atmospheric refraction. This distortion is determined from the world-coordinate system (WCS), which maps each pixel location to a physical location on the sky. The WCS solution comes both from comparisons of the astrometric positions of stars to an astrometric catalog (e.g., Gaia[1] ) and from overlapping images where the objects are known to be in the same true position (modulo parallax and proper motion), even if that position is unknown. \nFinally, we also care about the brightness of each galaxy and its colors (both for estimating photometric redshifts and for measuring cosmic magnification). The optimal algorithm for this step is still being developed, but it may involve using all of the images at once, similar to how the shape is estimated. Furthermore, since a galaxy is nearly the same shape in each band pass, the optimal color measurements may use the images in all the different band passes at once to produce the best estimate of each color. These fluxes and colors must also be corrected for effects like variable sky transmittance and extinction by Galactic dust. \n> 1http::/www.rssd.esa.int/gaia/ \n30 \n_3.1 Weak lensing_ \n## _**Additional quantities needed**_ \nIn order to properly use the galaxy shape measurements (or sizes/fluxes for cosmic magnification), some extra information is required. First, the predicted gravitational lensing effect depends on the distance of each galaxy from us; we use redshift as a proxy for distance. Since most galaxies observed by LSST will not have spectroscopic redshifts, we must use photometric redshift (aka photo- _z_ ) estimates. For weak lensing, we care less about the specific redshift of each galaxy than we do about having unbiased estimates of the probability distribution _p_ ( _z_ ) of each galaxy’s redshift. Thus, our requirements on the photo- _z_ algorithms tend to be somewhat different from other probes (see Section 3.7). \nWe also need an estimate of the full redshift distribution of the galaxies we are using for our statistics (including the rate of catastrophic outliers). For some ranges of color/redshift, it can be challenging to obtain unbiased photo- _z_ estimates, so we may exclude such galaxies from our samples. This can be an acceptable trade-off between statistical and systematic errors, but we need to properly account for such selections by calibrating the final redshift distribution of the resulting sample to produce the cosmological predictions. \nThe galaxy shapes require a similar calibration. The effect of shear on a galaxy’s shape depends on the initial shape. A given shear cannot make a highly elliptical galaxy much more elliptical, so its effect is less than the same shear acting on a round galaxy. Thus, we need to know the intrinsic shape distribution of the galaxies in our sample, either to use as a prior during the shape measurement process, or to apply a “responsivity” correction after the fact. \n## _**Constructing cosmological statistics and covariances**_ \nWorking from galaxy catalogs containing positions, photo- _z_ or _p_ ( _z_ ), and shear estimates for each galaxy, we can construct cosmological 2-point statistics[2] . In a tomographic lensing analysis, there is a galaxy density field and a shear field in redshift bins (typically between three and six bins), and the most general analysis that permits us to marginalize over systematic errors is to compute all possible auto- and cross-power spectra, i.e., the galaxy auto-correlation (galaxy clustering), shear auto-correlation (cosmic shear), and galaxy-shear cross-correlation (galaxy-galaxy lensing), both within and across redshift bins. Theoretically, this is best done in Fourier space given that different modes are uncorrelated; however, the presence of survey boundaries and masks within the survey complicates matters, resulting in the frequent use of real-space statistics and the development of pseudo-Fourier treatments. In addition to the cosmologically-motivated statistics, there are also statistics (e.g., _B_ -modes, and star-galaxy cross-correlations) that are mostly sensitive to systematic errors; we must measure those in order to directly detect and remove systematics in the cosmologically interesting statistics. \nWhile two-point statistics are the only ones included in the DETF figure of merit for LSST, they are not the only cosmologically interesting statistics. Although they are not explicitly associated with a task in Chapter 5, it is important to explore how other statistics besides 2-point functions can enhance dark energy constraints from LSST. For example, lensing 3-point functions contain \n31 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nadditional cosmological information and are sensitive to systematic errors such as intrinsic alignments in different ways than the 2-point functions. Thus, 3-point lensing statistics can be used both to get additional cosmological information and also to constrain the impact of intrinsic alignments on the 2-point statistics. Local maxima in the shear field in lensing maps (called “lensing peaks”) due to single or multiple galaxy clusters along the line of sight provide information similar to that provided by galaxy clusters but are a direct observable of LSST. If systematics can be adequately controlled, lensing peaks will substantially improve the constraining power of LSST, since they include otherwise inaccessible information beyond that in 2-point statistics. Also, because of the great depth of LSST, lensing peaks due to multiple halos along the line of sight will be prevalent; these have a large share of the constraining power, and techniques to exploit them will not be required by shallower near-term surveys. Finally, we consider lensing magnification of fluxes and/or sizes. As discussed below, the different sensitivity to systematic errors makes lensing magnification complementary to shear in several ways. \nWe must construct not only the cosmological statistics, but also their covariance matrices (including cross-covariances between different statistics). For this purpose, it is important to include nonGaussian effects, which can significantly increase the diagonal terms of the covariance matrix on the scales used for cosmological parameter constraints. Furthermore, we must include offdiagonal terms, including those from correlated shape noise, masking effects, etc. This will require a combination of simulation and analytic theory, including halo-model based covariance estimates; for more detail, see the Theory section (Section 3.6). \n## _**Cosmological parameter estimation**_ \n**Photo-** _z_ While the magnitude of the weak lensing shear depends on distances to both the lens and source, the kernel is quite broad in redshift and thus relatively wide redshift bins can be used to study the evolution of the weak lensing signal. This means the accuracy of individual photometric redshifts is not very important, as long as the distribution of photometric redshift errors is known to high precision. Therefore, one must accurately determine the mean redshift and catastrophic outlier fraction of galaxies in each redshift bin, along with the effective bin width. To achieve this we ideally require an unbiased likelihood distribution of possible redshifts for each galaxy. \n35 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nTo inform the photometric-redshift team of appropriate requirements for their algorithms, we must determine to what precision the photo- _z_ distribution must be known in order to avoid degrading the constraints on dark energy parameters. Then, given this precision, we need to investigate the resulting requirements for spectroscopic observations to calibrate the photometric redshifts. The spectroscopic sample must be robust, complete, and accurate so it does not itself introduce bias on the cosmological constraints (Cunha et al. 2012c). It must also be drawn from a wide enough area of sky to minimize sample variance in the photo- _z_ calibration (Cunha et al. 2012a). \n**Measurement issues associated with galaxy shape measurement and PSF correction** Several systematic errors are related to measurement of the galaxy shape and correction for the effects of the PSF, which we recognize here for completeness, but which we deem low priority for the LSST DESC in the next three years because progress by Stage III surveys and other groups is likely to be sufficient: \n- Multiplicative and additive shear errors: in addition to progress by Stage III surveys, lensing groups worldwide are participating in an ongoing series of blind challenges (GREAT08, GREAT10, GREAT3) to identify and minimize these errors. These challenges have not yet assessed systematics in magnification analyses, but are likely to soon. \n**Lensing peaks** Lensing peaks in shear maps will be an important probe for constraining dark energy significantly beyond the LSST baseline figure of merit if systematics can be adequately controlled. Near-term surveys are not likely to address issues for lensing peaks. If lensing peaks are to be a key probe for LSST, we need to quickly establish this fact and the computational model of how the analysis will be done, since these decisions might drive computational requirements. To establish lensing peaks as a key probe for LSST it is necessary to delineate the effect of LSSTspecific systematic errors on lensing peaks: shear errors, photo- _z_ errors, masking, variable depth, baryon effects, and intrinsic alignments, and to determine the effect of these errors on cosmological \n36 \n_3.1 Weak lensing_ \n**Predictions** Theoretical predictions for lensing statistics are challenging for three partially related reasons: the nonlinearity of the underlying mass density field on scales of interest, the need for predictions of quantities that go beyond the power spectrum, and the inclusion of physics that is not well understood such as gas physics in galaxy clusters and intrinsic alignments. The common \n37 \n_Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Systematics from theory**_ \n**Beyond linear models.** The convolution described at the end of Section 3.2 highlights the importance of increasing the precision of our measurements of the mildly nonlinear bias between \n42 \n_3.2 Large-scale structure_ \nthe galaxy samples and the underlying dark matter, and of the photometrically determined redshift distribution of the source population. \nOn shorter time scales, a software framework is needed that will (i) give scientists easy access to project simulations and (ii) allow for parameter extraction from value-added catalogs. The parameter extraction routines will be developed over the coming decade, but installing a basic set of tools that can be used across working groups is essential in order to quantify the effects of systematics on dark energy in an “apples-to-apples” fashion. \n## _**Ancillary data needs**_ \nAncillary information about host galaxy properties will also play a role in the analysis; for example, SN Ia light curves may be better standardized with knowledge of global host properties such as age, star formation rate, and stellar mass (Kelly et al. 2010; Sullivan et al. 2010; Lampeitl et al. 2010; Gupta et al. 2011; D’Andrea et al. 2011). These are typically derived from multicolor photometry of the host (though sometimes with spectroscopy, and sometimes locally at the SN site), so it is \n46 \n_3.3 Supernovae_ \nlikely that the host data from the fully combined stack of LSST (and any other available surveys, especially coordinated with the deep-drilling fields) will be a direct input into the SN cosmology analysis. \nFor both cases, with or without spectroscopy inputs, it is important to quantify systematic uncertainties and biases that might be introduced in these analyses. \n## _**Quantifying systematic uncertainty**_ \nThe bulk of LSST data are regressive, in the sense of being of poorer quality compared to LSST subsets specifically targeted for detailed study or lower-redshift supernovae external to LSST. We can quantify the systematic error in inferred absolute magnitude that arises due to our inability to model the object due to missing or poor-quality data: for a subset of supernovae with excellent data quality we calculate the bias in distances determined through analysis of higher-quality data and the same data degraded to LSST-quality. \nWe must estimate LSST systematic uncertainties based on current data and models. Conversely, we must quantify the “calibration” data necessary (from LSST or elsewhere) to constrain systematic uncertainties to our goals. This will be an ongoing activity as our understanding of SNe Ia evolves. \n## _**Fitting cosmologies**_ \nThe combined data set consisting of SN light curves, classifications (which may be a probability rather than a binary determination: Ia or not), redshifts, host galaxy properties, and any other ancillary data will be globally analyzed to determine cosmological constraints in a blind analysis. \nWork must be done to understand how to propagate or quantify typing uncertainty into dark-energy parameter uncertainties. \nThe analysis chain just described takes input data and propagates them into dark-energy parameters of interest, incorporating statistical and systematic uncertainties. Building elements of this chain quickly is critical for our interactions with the Project, quantification of systematic uncertainties, and prioritization of critical path DESC activities. \n## **3.3.3 Systematics** \nThe LSST DESC seeks to maximize the exploitation of the LSST supernova survey by minimizing statistical and systematic uncertainties. In this section, we review the critical issues that must be addressed from which tasks to be performed in the three years are identified. \n47 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Projecting the survey to dark energy parameters**_ \nMost of the activities of the SN working group focus on the optimization of survey strategies and the addressing of systematic uncertainties. We have an overarching need for a software tool that projects LSST SN surveys into dark-energy measurements with high-fidelity statistical and systematic uncertainty propagation. No such end-to-end tool exists: a new software project is needed. We can leverage the vast experience of DESC members in simulations and data analysis. We will take advantage of the simulation products provided by the Project; indeed we want realistic supernova signals in the Project pixel-level simulations. Note that the deliverables of most of the SN tasks will either be a contributed module for this project, or depend on the running of the software. \nThis is a high-priority activity because most of our studies need such a tool to make the connection with survey performance. \n## _**Photometric calibration**_ \nThe construction of robust catalogs of observed clusters requires identifying and employing survey observable(s) that are simple to measure and have minimal, and well understood, scatter with mass. For LSST cluster catalogs these observables will involve accurate measurements of galaxy colors and spatial concentration or “richness”. The cluster catalogs produced should be as pure and complete as possible, and have their residual impurity and incompleteness, as well as the survey selection function, modeled robustly. Accurate photometric redshift estimates for the clusters are another key survey requirement. \nConstructing and understanding the relationships between survey observables and mass is the most difficult and complex stage of the process. Generally this requires the use of intermediate scaling relations, involving multiwavelength mass proxies with minimal and well understood scatter with mass, gathered from follow-up observations. The absolute mass calibration of these relations must also be established accurately and precisely, with this latter step driven by LSST weak-lensing measurements. \nAll of the above information must then be combined within a robust, self-consistent framework that utilizes fully the information available, while accounting for the impacts of survey biases, systematic uncertainties, and their covariance with all quantities of interest (e.g., Allen et al. 2011). \nGalaxy clusters offer an ensemble of additional constraints on cosmology and fundamental physics. Tomographic measurements of the redshift-dependent lensing of galaxies behind clusters provide a geometric test of dark energy (e.g., Gilmore & Natarajan 2009). Measurements of the baryonic mass fraction in clusters tightly constrains the mean matter density, and provides a powerful, complementary probe of the expansion history (Rapetti et al. 2008). The combination of X-ray and mm-wave measurements provides another redshift-independent distance measurement (e.g., Bonamente et al. 2006). More generally, the availability of both geometric and structure growth tests for clusters allows one to separate the effects of dark energy within the context of General Relativity from deviations due to alternate descriptions for gravity (Copeland 2007; Frieman et al. 2008; Rapetti et al. 2012; Lombriser 2011). \n51 \n_Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nAs the most massive collapsed structures, galaxy clusters can additionally be used to probe nongaussian signatures of inflation (see, e.g., Chen & Wang 2010 and Komatsu 2010 for recent reviews), and the neutrino mass hierarchy, through the effects of neutrinos in suppressing structure growth on the scale of clusters and smaller (Lesgourgues & Pastor 2006; Mantz et al. 2010a; Reid et al. 2010). \nIn all of these aspects, cluster cosmology will exploit the full potential of LSST, and the rich ensemble of multiwavelength data available in 2020. \n## **3.4.3 Systematics** \nThe major, recognized challenges to obtaining robust dark energy constraints using galaxy clusters observed with LSST can be summarized as follows. \nA first challenge is the accurate theoretical prediction of the distribution and clustering of galaxy clusters as a function of mass and redshift. In particular, robust predictions for the mass and bias functions will be required, spanning the full suite of cosmological models and mass and redshift ranges of interest. These predictions should be formed in a way that facilitates fast model comparisons (see below), for example, by utilizing “universal” forms for mass and bias functions where possible. Particularly important and challenging will be understanding the impact of baryonic physics on these predictions. This task will require cosmological simulations, building on the infrastructure developed for Stage III projects such as DES. \nA second major challenge involves developing optimized ways to find clusters and form catalogs, both using LSST data alone and in combination with auxiliary data. (As discussed in the analysis section, the combination of optical and X-ray data has been shown to provide better performance for cluster finding than optical data alone; e.g., Rykoff et al. 2012). The challenge will be to develop algorithms that generate cluster catalogs with maximum purity (i.e., no false clusters) and completeness (i.e., no clusters missing). Equally important, however, will be the ability to quantify and model residual incompleteness and impurity precisely, so as to be able to marginalize over these fully in the cosmological analysis. This task will rely heavily upon simulated galaxy catalogs and images that accurately represent the galaxy properties of real clusters, which is a topic of intensive investigation by the DESC Cosmological Simulations Working Group. This task will also require extensive cosmological simulations, building on tools developed for Stage III projects. \nWhile theory can predict the distribution of mass quite robustly as a function of cosmological parameters, surveys are based on _observables_ – measured quantities that relate to mass with complex scaling relations and scatter. For LSST cluster cosmology, the primary survey observables (in addition to photometric redshifts) will relate to optical richness, likely augmented by information in the X-ray and mm-wave (Sunyaev-Zel’dovich effect). The single most pressing challenge for cluster cosmology is to understand the statistical relationships between galaxy cluster observables and mass. There are two main aspects to this work: 1) understanding the form, scatter and evolution of the _scaling relations_ linking key observables and mass; and 2) obtaining robust absolute mass calibrations for these relations. \nFor the first aspect, nature fortunately provides several observable quantities that are straightforward to measure and which correlate tightly with mass, exhibiting minimal scatter across the mass \n52 \n_3.4 Clusters of galaxies_ \nand redshift range of interest. These _low-scatter mass proxies_ provide a critical intermediate step, allowing one to first statistically link survey observables to these mass proxies, and then the mass proxies to mass. Studies have shown (e.g., Mantz et al. 2010b; Wu et al. 2010) that the addition of robust, low-scatter mass-proxy measurements for even a small fraction (a few percent) of the clusters in a survey can boost the overall cosmological constraining power by factors of a few or more. Major challenges for this work include identifying the optimal combinations of mass proxies for LSST cluster science; determining how to distribute targeted follow-up measurements of mass proxies across the survey flux/redshift range; identifying where thresholding and binning can be effectively applied; quantifying and modeling the impacts of systematic uncertainties; and actively gathering the mass-proxy data. This last challenge will involve collaborations with other agencies, as many of the best mass proxies are provided by X-ray observations. \nGalaxy clusters are crowded environments hosting the highest densities of galaxies in the Universe. A challenge for cluster cosmology will be mitigating and modeling the systematic effects associated with source confusion, and contamination of lensing and other signatures by the cluster-member galaxy population. Being the most massive objects in the Universe, clusters also exhibit the strongest lensing signals. Understanding how to utilize higher order lensing effects, as well as tomographic lensing signals in both the weak and strong lensing regimes, represent additional challenges for the DESC clusters team. \nAs with other LSST probes, certain aspects of cluster cosmology, and particularly those related to lensing measurements, are sensitive to photo- _z_ systematics. An important and pressing challenge is to quantify the impact of these uncertainties on cluster mass calibration, as a function of cluster mass and redshift. These studies should explore techniques that minimize the need for exhaustive spectroscopic followup. Current studies by DESC members that utilize the full photometric redshift information for the lensed galaxies in cluster fields provide some promise in this regard ((von der Linden et al. 2012; Kelly et al. 2012; Applegate et al. 2012). Conversely, areas where the unique \n53 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nenvironment of clusters introduces additional demands for external calibration data should be identified urgently. \nEncompassing the above is the overarching challenge of developing a self-consistent statistical framework to model the distribution and properties of clusters, including the mass-observable scaling relations, accounting for survey biases and covariances, and marginalizing fully over systematic uncertainties. Members of the DESC Clusters Working Group have been at the forefront of this work in current cluster cosmology. Required extensions include the incorporation of clustering into the analysis framework, along with the related nuisance parameters. These issues will be addressed to some extent by Stage III projects, but LSST has substantially higher accuracy requirements, so parallel efforts within the DESC are almost certainly desirable. End-to-end testing, from cosmological simulations to cosmological constraints, will be required for this effort. \nUsing this modeling infrastructure, a final major challenge is understanding the computing needs for cluster cosmology. Currently, using X-ray cluster catalogs with _≈_ 250 clusters and 100 multiwavelength follow-up observations, CPU farms with 500 cores require of order a week of run-time for a complete cosmological analysis that addresses multiple questions. Scaling simplistically (linearly) with the number of clusters, and expecting LSST cluster catalogs to contain hundreds of thousands of clusters, one can envisage significant computing challenges. Understanding the computing needs and identifying optimized ways in which to speed up the analysis while maintaining precision and accuracy, with, for example, improved algorithms and the judicious use of binning, are important challenges. Cluster cosmology also requires significant input from cosmological simulations, which are essential for extracting precise predictions from cosmological models, understanding the interplay between systematic sources of error, and testing the data processing and cosmological analysis pipeline in general. These require substantial computing resources, and significant work will be required in conjunction with the simulations working group to identify these resources. \n## **3.4.4 Urgent Priorities** \nA related urgent issue is to evaluate the performance of current LSST deblending algorithms in crowded cluster environments, and how this performance impacts the mass calibration described above. In particular, we need to evaluate how limitations in the deblending algorithm impact the cosmological constraints from clusters, the degree to which these limitations can be modeled, and the requirements for improvements in the algorithm. Here and elsewhere, the analysis will involve data from sources such as Subaru and HST, as well as ray-tracing simulations. (DES data are not well-matched to this problem because of their depth and image quality.) The work will link the development of the deblending algorithm (carried out by the LSST Data Management team) to requirements determined by its effect on cosmological constraints. \n54 \n_3.5 Strong lensing_ \nEqually important is a detailed, quantitative understanding of the impact of photometric redshift uncertainties on cluster science, and on cluster mass calibration in particular. Linking to this, the development of techniques that minimize the requirements for spectroscopic followup may have cross-cutting benefits and expedite the progress of cluster cosmology with respect to other dark energy probes. Conversely, areas where the environments of clusters place additional demands on external calibration data should be identified urgently, allowing preparations for the gathering of these data to be made. \nThe final important challenge is the development of a self-consistent statistical framework to model the observed distribution and properties of galaxy clusters and exploit fully the cosmological information contained within these data. This framework should enable reliable estimates of the computing needs for LSST cluster cosmology (which may be substantial) and robust predictions of the eventual cosmological constraints from galaxy clusters. Efforts by the DESC in this regard, utilizing the expertise within the collaboration, are also very likely to be of benefit to Stage III projects. \n## **3.5 Strong lensing** \n## **3.5.1 Overview** \nStrong gravitational lensing refers to the multiple imaging of a background object (“the source” or “deflector”) by a massive foreground object (“the lens”) (e.g., Schneider et al. 2006b; Treu 2010). The resulting angular separation, time delay, and morphological distortion all depend on the lens geometry, quantified in terms of combinations of the three angular diameter distances between observer, lens and source (Refsdal 1964; Blandford & Narayan 1992). These observables are also affected by the mass distribution of the lens, which must be modeled simultaneously using high resolution follow-up imaging and spectroscopy. \nStrong lensing provides at least two independent ways to measure dark energy parameters based on LSST data. The first one, gravitational time delays, has a venerable history and is now, in our opinion, mature for precision cosmology (Refsdal 1964; Suyu et al. 2010, 2012). The second one, the analysis of systems with multiple sets of multiple images (Gavazzi et al. 2008; Jullo et al. 2010), is a more recent development and will require more research and development work in order to quantify precisely its systematics and assess its potential. Therefore, we will spend most of this section addressing the first approach, giving only a brief description and key references for the second. \nStrong gravitational lensing time delays measure a combination of distances that is sensitive primarily to the Hubble constant, but also to the other parameters of the world model: when many \n55 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nlenses with different lens and source redshifts are combined, internal degeneracies are broken and dark energy parameters can be measured (Coe & Moustakas 2009). The time-delay distance provides significant complementarity with other dark energy probes (see Figure 3.1) to determine, e.g., the dark energy equation of state, matter density, Hubble constant, and sum of neutrino masses (e.g., Linder 2011; Collett et al. 2012; Weinberg et al. 2012). Furthermore, the main systematics coming from the lens-mass modeling and line-of-sight dark matter, are interesting astrophysically in themselves: dark matter substructure and galaxy mass profiles (e.g., Treu 2010, and references therein). LSST’s wide-field, time-domain survey will be superb for obtaining large samples of timedelay systems and, with follow up spectroscopy and high resolution imaging for the lens model, delivering distance-ratio constraints. An exciting prospect is the use of time delays to test gravity on precisely those scales where we expect screening mechanisms to become active. \n**==> picture [421 x 204] intentionally omitted <==**\nFigure 3.1: Left panel: Sensitivity of the time-delay distance combination _T_ = _rlrs/rls_ to the cosmological parameters is plotted vs. lens redshift. Unlike supernova distances, sensitivities can pass through zero (indicating no degeneracy with the other parameters) and have opposite signs at the same redshift, giving strong complementarity with positive-only supernova distance sensitivities. Right panel: Complementarity of time delays (SL) with both supernovae (SN) and weak lensing (WL) is shown in the different directions of constraints on dark energy parameters, marginalized over neutrino mass and other parameters. \nWe anticipate that dark energy measurements from strong lensing with LSST will differ from previous or concurrent strong-lens surveys in the following key ways: \n1. A high-yield lens search requires all four of the properties offered by LSST: wide area, depth, high image resolution, and spectral coverage to distinguish lens and source light. Other surveys – current, planned, or concurrent with LSST – will lack at least one of these properties. Only SKA will surpass LSST in terms of yield; Euclid will perform comparably well, but is best thought of as a complementary dataset. LSST should be able to detect _≈_ 8000 time-delay lenses (both AGN and supernovae). \n2. Time-domain information will be available from within the survey, enabling the measurement of time delays for thousands of lens systems for the first time. Precursor surveys such as DES, \n56 \n_3.5 Strong lensing_ \nHSC, and PS1, which should find lensed AGN in large numbers, will be reliant on monitoring follow-up, and so will likely achieve only a few hundred measured time delays, with enormous effort. LSST should provide precision time delays for over a thousand lenses. \nIn short, LSST’s strengths are the size of its lens sample, which should be an order of magnitude more in most lens classes (and even more in others), and its ability to provide measured time delays for the majority of suitable variable-source systems. \nPractical problems to be solved include lens detection (which will be very sensitive to the image quality and the deblender performance); image and light-curve modeling (which can be both CPU and skilled-labor intensive); obtaining and analyzing follow-up data; interpreting the whole sample of lenses in the context of a well-studied subset; and finding sufficient experienced researchers to accomplish all these tasks. We expand on these topics in the sections below. \n## **3.5.2 Analysis steps** \n## _**Searching for strong lenses**_ \nThe first major analysis step required is to _find_ these time-delay lenses in the LSST dataset. Such systems are rare, approximately 0.2 per square degree at LSST single-epoch depth and resolution (Oguri & Marshall 2010). Finding them in ground-based images (or even shallow space-based images) is a highly non-trivial task. A number of algorithms have been proposed based on the morphology, color, and time domain properties of the systems (e.g., Kochanek et al. 2006; Oguri et al. 2006). However, current algorithms require substantial human intervention and follow-up data for confirmation. This will be impractical at the LSST scale and therefore significant effort will need to be spent in the coming years to develop algorithms with higher purity and efficiency, and test them on precursor Stage III survey data. Below we sketch the key features of the time-delay lens detection pipeline. \n**Querying the catalogs.** The most efficient initial selection will be made by querying the object and source catalogs, based on photometric, morphological and time variability criteria together. In some exposures the lensed images will be well-resolved, while in others they will not: all exposures \n57 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nwill contain _some_ information about the variability of the objects. Interpretation and classification of the candidates based on physical lens models can begin during this phase. \n**Modeling the pixels.** With such large samples being investigated, we will need to focus on lens systems that are unambiguously identified using the LSST data. Moreover, we anticipate the majority of the variability information coming from the LSST survey data. These desiderata drive us towards fitting models to the full set of LSST postage stamp images, in order to disentangle the lens and source brightness distributions, and optimally extract lightcurves of the lensed images. Joint analysis with Euclid or SKA at this stage would be very interesting, with the higher resolution space-based images enhancing the sensitivity of the LSST measurements. A probabilistic model comparison between a constrained lens model, and a flexible “nebula” model, would provide us with an unsupervised classification (Marshall et al., in prep.). \n**Quality control.** The basic selection function of the lens sample will be understood during the two automated steps above. Still, human inspection of the candidates provides two quality control services. First, rejection of any residual false positives, increasing sample purity. Second, discovery of particularly interesting systems—such as higher order catastrophes, multiple sourceplane lenses, etc. Beyond this, human inspection has the potential to discover lens systems that the automated algorithm missed. The impact of these procedures on the selection function will need to be quantified and understood. \n## _**Measuring strong lenses**_ \nIn order to be able to infer the time-delay distance for each of a thousand clean lens systems selected from the pool generated in the detection phase, we will need high precision measurements of the lensed images positions and varying brightness, and also of the variable source’s host galaxy (seen distorted into an Einstein Ring). This information will then be combined in the lens modeling analysis; the time-delay distance is one of the parameters of such lens models. \n**Measuring time delays.** This will be done using the LSST time domain data, and in particular the inferred fluxes from the pixel-level modeling described above. The main sources of systematic error in the determination of time delays are finite sampling and microlensing (e.g., Kochanek 2004, and Tewes et al. 2012, in preparation). The extended and well-sampled light curves enabled by the LSST data should be sufficient for this purpose, but significant work will be needed to improve and calibrate algorithms to the level of precision and automation required by the scale of the project. \n**Measuring Einstein rings.** Substantial observational follow-up with non-LSST facilities will be required for the determination of the spectroscopic redshifts of the sources and the lens galaxy, and the stellar kinematics of the lens galaxy (which provides an independent probe of the lens-mass distribution), and to obtain high resolution images of the lensed quasar host galaxy. An efficient way to make these measurements is likely to be with Integral Field Unit (IFU) spectrographs on either Extremely Large Telescopes (ELTs) on the ground or with JWST from space. Which facility is used may depend on the type of lens being studied. \n**Modeling strong lenses.** As well as providing resolved stellar kinematics of the lens galaxy, IFU observations allow a cleaner separation of the background and foreground objects, and of the source-galaxy stellar distribution and active nucleus, and open up the possibility of using the \n58 \n_3.5 Strong lensing_ \n## **3.5.3 Systematics** \n## _**Catalog generation and interpretation**_ \nStrong lensing places strong demands on the LSST DM source detection and characterization software than will other investigations. We are specifically interested in faint, extended objects, with the complex morphologies that are typical of gravitational lens systems. The deflector galaxy will, in the majority of cases, be of greater or comparable brightness to the system of point images arranged around it. The distinguishing features of the lensed images will be their color, and variability: as Kochanek et al. (2006) pointed out, lens systems appear as extended sources in difference images, and the varying quasars and AGN have characteristically blue colors (e.g., Richards et al. 2006). The challenge is to combine these algorithms and distill them into a catalog query that will return a large, high completeness sample of lens candidates that is orders of magnitude smaller than the parent object catalog. Stage III surveys have the same problem, of enabling an efficient initial selection of lens candidates: while they may solve some of the deblending issues, Stage III image processing pipelines are already fairly mature. We have an opportunity with LSST to drive the development of source detection software, and database schema design, such that strong lens candidates can be efficiently selected from the source and object catalogs. In order to do this, we need to be testing the DM system, _as it is developed_ , on simulated strong lens samples. \nMultiply-imaged sources will appear as between 1 and 5 sources at each epoch, depending on the image quality: the LSST source and difference catalogs need to capture this information and allow lens candidate selection in the presence of variable image quality. Most of the lensed sources will have magnitudes close to the detection limit: will difference imaging be sensitive enough to generate a sufficiently high yield of time-delay lenses? Can a robust color selection be made, given the blending of lens and source light? Can the color information in the difference catalogs be used to robustly identify strong lenses? How is this variability information best captured and stored, for the purposes of lens candidate selection? Does it scale to all objects in the survey, or will this need to be a level 3 calculation on pre-selected objects? Possible ways of characterizing variability, \n60 \n_3.5 Strong lensing_ \nand its color dependence, could be explored at Stage III, but only coarse variability quantification will be possible there. Making the most of the LSST data will require dedicated LSST research. \n## _**Automated candidate classification**_ \nEven with optimized catalogs, the false positive rate in the initial lens candidate sample may be high, as we push down to the single epoch magnitude limit. The remaining piece of information that we have is the characteristic pattern of expected lensed image positions (and to a lesser extent, fluxes). Implementing this constraint means comparing the catalog data for a given candidate against the predictions of a lens model. Source catalogs that are faithful to their input images can be used as data in a lens model fit: lens vs nebula model comparison would then provide an odds ratio utility for down-selection. Alternative catalog processing and interpretation algorithms may prove faster and more efficient than such a fit: their exploration is also desirable. \nWe expect to be able to reach only a certain degree of purity working from the pipeline-generated catalogs: we anticipate needing to return to the images to study samples of around 10[5] candidates. This could be enabled via the DM level 3 API. In the first instance, lens-modeling of cutout images will refine the automated classification: a “multi-fit” to all cutouts from each visit will preserve all the information available. Secondly, one of the by-products of such a fit will be optimally-generated light curves and their covariance, for the model-deblended sources. \nAgain, algorithms and code to do this analysis will be developed for Stage III experiments, but since the Stage III survey data will not yield sufficiently long or well-sampled light curves for time-delay estimation, the interest is mainly in the candidate classification. We can expect the principles of the solution to be established during this phase. The greater information content of the LSST data will enable a more exacting classification to be made: can differential dust extinction be detected and used as a lens signature? How flexible does the lens galaxy light distribution model need to be for very accurate lensed source light curves to be derived? Will the lensed source host galaxy ring need to be included in the model? How useful will this additional information be? Will the PSF model need to be refined during the fit? \n## _**Checking the lens samples**_ \nAt the quality control stage, samples of several 10[4] classified lens candidates and their models will need to be inspected to check and refine the models, catch unforeseen failure modes, and rank the systems for follow-up. Who will carry out these tasks? The automated classification needs to be good, in order to make a pure sample, suggesting that the time needed to understand a typical candidate, making sure its model makes sense and potentially re-running it, could be several minutes per system. A large group of analysts may be needed to do this work. One option is to use an online crowd-sourcing platform as a “Lens Zoo,” to open up access to the candidates to large numbers of citizen scientists as well as collaboration members. The Galaxy Zoo user distribution has a long tail of highly skilled and motivated volunteers, who could readily assist with the LSST quality control effort. Such a zoo is under construction in Fall 2012 with the goal of helping find lenses of all kinds, not just time delay or multiple source plane systems, in Stage III data: if successful, this could evolve into a useful facility for LSST strong lensing. \n61 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Accurately characterizing external mass structures**_ \nA lack of knowledge about the line-of-sight mass distribution are the dominant source of uncertainty for time-delay lenses (Suyu et al. 2010). Since surface brightness (or photon number) is conserved, we expect the additional convergence provided by mass along the line of sight to average to zero over a large sample of lenses, provided those lenses are selected in an unbiased way (e.g., Seljak 1994; Fassnacht et al. 2011b). While the size of the associated residual systematic error remains to be estimated, understanding the selection function may become important in correcting for it. Using more information on each lens’ environment should allow the external convergence to be estimated and the uncertainty in the distances decreased: however, care must be taken not to introduce large biases during this process. Residual systematic errors can arise from the recipe for assigning mass to light, and from the choice of calibration cosmological simulation. Modeling the mass in the field on multiple lens planes will require an improved formalism for describing the effect of multiple plane lensing on the time delays. A further complication with modeling the lens environment is how to incorporate small scale invisible structures: these may cause strong perturbations to the image fluxes, but relatively weak perturbations on time delays. Are their populations characterizable as simple mass sheets? Can their clustering around visible galaxies be used? Clustering on the group scale is also likely to be important: can we model groups of galaxies accurately enough to reduce the scatter in the distance estimates without causing additional bias? \nWhile the analysis techniques for modeling line of sight mass structure will evolve during stage III, the 1000 lens sample provided by LSST will place higher demands. We anticipate spending substantial amounts of time before first light in investigating environmental effects, by comparing multiple approaches based on halo models and numerical simulations. We also anticipate the need for theoretical work to properly model the propagation of light rays through a inhomogeneous universe to the level of precision required by the LSST dataset. \n63 \n_Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Establishing the viability of multiple source plane cosmography**_ \nIn both the galaxy-scale and cluster scale multiple source plane lens cases, significant work will have to be undertaken in the coming years to quantify systematic uncertainties and how to mitigate them. Detailed forecasts for DE parameter constraints from plausible samples of LSST lenses need to be developed. Systematics due to lens modeling (including the first source as a perturbing lens, in galaxy scale case) are not yet well understood; nor has the detectability of multiple image systems using LSST images been determined. The small number of available galaxy-scale systems make this a Stage IV, not III, project. Well-studied clusters of galaxies, each with many sources on many different planes, are being studied already, with progress on the systematic uncertainties being made. Galaxy-scale systems should have different sensitivities to some of the modeling systematics, as their background sources are large compared to the scales of the caustic features. Nevertheless, full investigation of the limits of theses systems for dark energy science is needed. \n## **3.6 Theory and joint probes** \n## **3.6.1 Overview** \nLSST will enable transformative improvements in our understanding of dark energy and the nature of gravity on cosmic scales. Realizing optimal constraints on the nature of dark energy from LSST will involve the collaborative efforts of experts in observation, theory, simulation, and data analysis. The same LSST survey data can be used for multiple dark energy probes, which is a crucial advantage. These probes will form multiple lines of attack at the puzzle of dark energy and provide internal cross checks that could potentially detect unknown systematics. Moreover, when analyzed jointly, these probes will enable self-calibrations of systematics, reduce degeneracies between parameters, and strengthen the constraints on dark energy properties. Dark energy studies will also benefit from the combination and cross-correlation of LSST survey data with precursor and contemporary external datasets. It will be a major undertaking to coordinate the connections of all these multiple efforts into the cosmological constraint analysis pipeline. This includes the formation of a software framework to facilitate a common platform for testing and integrating data analysis, simulation and theoretical prediction codes, and data products. \n## **3.6.2 Analysis steps** \nThe general steps of analyzing LSST data for DE investigations are as follows. \n1. Reduce the images to collections of objects and auxiliary data (such as masks, observing patterns, etc.) with relevant properties determined. \n64 \n_3.6 Theory and joint probes_ \nThis is primarily performed by the LSST data management (DM) team with requirements set by relevant working groups (WGs). Crucial contributions from these WGs should be integrated into the DM pipeline well in advance. \n2. Extract statistics or quantities of interest from the objects. \nFor WL and LSS, these would include not only the statistics (such as two-point correlation functions) measured by their respective WG but also cross statistics between the shear and galaxy fluctuations. For clusters, the main statistic is the mass function at different redshifts. For SN and SL, by their very nature, individual objects are studied in great detail to derive distances. \n3. Turning statistics into DE constraints \nIn the usual Bayesian approach, one turns the likelihood of the data given the model into the posterior probability distribution of the model by sampling the model space with tools like Markov Chain Monte Carlos. \nPredicting the likelihood is critical to the analyses. For WL and LSS, the likelihood is determined by theory on linear scales, and simulations and modeling on nonlinear scales. Observational effects such as the photo- _z_ errors also fold into the likelihood. Since clusters are highly nonlinear objects, simulations are needed to accurately predict the likelihood of their mass function (and certainly the mass-observable relation). In the case of SNe and SL, the likelihood is driven by intrinsic uncertainties of object properties and random measurement errors. \nExternal data may be used in all the steps above. For example, infrared image data will have to be processed together with LSST image data to reduce the photo- _z_ systematics. Cross correlations between the CMB and the LSST galaxy distribution will be calculated in step (2) and analyzed with other data in step (3) to include the ISW effect in constraining DE. \n## **3.6.3 Systematics** \n## _**Dark energy forecasting and analysis pipeline**_ \nLSST’s survey data should reveal far more than whether the equation of state parameter for dark energy is _−_ 1, as it is for ΛCDM, or not. In looking to address the theoretical fine-tuning and coincidence problems associated with ΛCDM, a wealth of alternative explanations for acceleration have been considered. These could be distinguished from ΛCDM by looking for evidence of a dynamical equation of state parameter, such as arises for a scalar field; or evidence that dark energy interacts with matter, that it clusters, that it was important in the early universe, or that it derives from a modification to GR on cosmic scales. To fully utilize LSST’s data will require careful determination of theoretical predictions for survey observables for the spectrum of models that LSST can test. This includes model-independent parameterizations, effective field theory approaches to DE and dark energy-dark matter interactions, and models of modified gravity, such as those derived from massive and higher dimensional theories of gravity. \n65 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nLSST’s large survey area and depth will provide an unprecedented opportunity to test dark energy’s properties at many different spatial scales and redshifts. This opens up the possibility of studying the transition from linear to nonlinear scales for evidence of screening mechanisms present in a variety of modified gravity theories. These analyses will require coordinated, detailed simulations to model structure growth that can be quite distinct from that in LCDM models. Combining, and contrasting, LSST’s lensing and galaxy clustering data could provide a distinctive test of GR. We will need to develop a pipeline that allows combination with complementary external CMB intergrated Sachs-Wolfe (ISW) and lensing data, galaxy clustering measurements from spectroscopic datasets, and HI data, amongst others. This will enhance LSST data’s application as a highly sensitive test of gravity, by contrasting relativistic and non-relativistic tracers of the same gravitational potentials (Zhang et al. 2007; Reyes et al. 2010). \nEven neglecting systematic uncertainties, galaxy clustering and lensing are not pristine measurements of dark energy and modified gravity. In comparing observations from halo to horizon scales, and multiple tomographic redshift slices, we will have to include relativistic effects and primordial non-Gaussianity (Dalal et al. 2008), running in the primordial power spectrum (Yoo et al. 2009; Yoo 2010; Bonvin & Durrer 2011; Challinor & Lewis 2011), and the impact of neutrino mass and additional relativistic species, amongst other possible degenerate cosmological parameters, to unambiguously disentangle them from the dark energy signatures we are looking for. \nOne of the most important activities is the joint analysis of probes, incorporating best estimates of the LSST data model from measurements on actual LSST subsystems and other data (at the telescope site, etc.). This DESC effort will couple strongly with activities in the LSST project, both ways, to determine a realistic data model. This will decrease the sensitivity to systematics, and enable the DESC to prioritize which residual systematics need most attention. Likewise, new DE probes and their systematics will be able to be tested for their statistical competitiveness, and their power to break parameter degeneracies, as they are suggested. Multiple source plane strong lens cosmography is one example of an LSST DE probe with as-yet-unexplored potential, that would benefit from study within the joint analysis pipeline. This pipeline will provide feedback into the optimal observing strategy and system design (including design and quality assurance metrics, the data management pipeline, database, which site-specific data to keep, etc.). This critical tie between the LSST DESC and project activities will start early in the DESC’s activities, with staged updates as LSST progresses through R&D, construction, and into commissioning. \n## _**Simulation challenge**_ \nSimulations will be crucial to LSST DE analyses. The challenge is to accurately predict or quantify (1) the observables such as statistics of WL shear and galaxy number density fluctuations, (2) observational effects such as those caused by instrumental effects, observing patterns, complex masks, etc., and (3) the likelihood function of the observables in a given model, so that their uncertainties do not become a dominant source of error in the LSST DE analyses. This will require a major effort in the Simulations WG (see Chapter 4). The Theory/Joint Probes WG will contribute significantly by analyzing this wide range of effects, determining practical requirements on the simulations, exploring ways to reduce the sensitivity to potentially detrimental effects, and iterating with the Simulations WG to make sure that the DESC will meet its goal. \n66 \n_3.6 Theory and joint probes_ \nAs an example, present N-body simulations (see Section 4.1.1) can predict the matter power spectrum with an accuracy approaching 1% on scales _k_ ≲ 1 _h_ Mpc _[−]_[1] (Lawrence et al. 2010a). In order to keep the uncertainties in the matter power spectrum a negligible portion of the errors in the inferred DE equation of state parameter, it will be necessary to predict the power spectrum with an accuracy of _∼_ 0 _._ 5% on scales _k_ ≲ 5 _h_ Mpc _[−]_[1] (Huterer & Takada 2005; Hearin et al. 2012a). Similarly, precise predictions are required for other statistics. Moreover, these levels of precision must be achieved for models that span the relevant cosmological parameter space. \n## _**Galaxy modeling challenge**_ \nA number of DE probes derive DE properties from the matter distribution and its evolution over cosmic time, which are inferred from measurements of galaxy positions (e.g., BAO and clusters) and shapes (e.g., weak lensing). Because of the highly complex and nonlinear nature of galaxy formation and evolution, it is currently not possible to accurately connect galaxy properties to the underlying matter distribution. Significant progress must be made to reduce the uncertainties to the levels acceptable to LSST. \nThe relative spatial clustering amplitude for galaxies and dark matter is referred to as the galaxy clustering “bias,” which is generally expected to be scale-independent and deterministic on large scales (≳ 100 _h[−]_[1] Mpc). But on very large scales, general relativistic corrections and primordial non-Gaussianity could introduce a scale-dependence to the bias (Baldauf et al. 2011; Bruni et al. 2012a). These effects should be taken into account in analyses. On smaller scales, the gravitational evolution of structures creates a scale-dependent and partially stochastic bias. If not properly modeled, the evolution of bias with cosmic time and scale could be confused with the linear growth rate of structures, shifts in the scale of the BAO peak, and “tilts” in the shapes of the _n_ -point correlation functions. The precision required on small scales is comparable to those in the last item. \nWeak lensing measures the correlated ellipticities of galaxies, which are determined by a sum of intrinsic and lensing-induced ellipticities. Galaxy intrinsic alignments (IAs) are created by the formation of galaxies in large-scale gravitational potentials (Catelan et al. 2001). The IA signal can be a significant systematic contamination to the lensing signal (Hirata & Seljak 2004). DE parameters and modified gravity parameters can be biased by more than 100% if no modeling or mitigation of IA contamination is included in the LSST weak-lensing analysis (Kirk et al. 2012; Laszlo et al. 2012). It is thus imperative to establish IA models that can accurately predict the alignments between orientations of galaxies and their surrounding matter distribution. At the \n67 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Measurement on very large scales**_ \nAnalyzing the clustering on scales comparable to the Hubble scale can allow the best constraints on dark energy and modified gravity models and test general relativity itself. \nWe need to improve the theoretical modeling of the clustering on large scales. The Kaiser approximation needs to be modified (Szalay et al. 1998; Matsubara 1999; Szapudi 2004; Papai & Szapudi 2008; Raccanelli et al. 2010; Samushia et al. 2012). General Relativistic corrections (Yoo et al. 2009; Yoo 2010; Bonvin & Durrer 2011; Challinor & Lewis 2011; Yoo et al. 2012; Jeong et al. 2012; Bertacca et al. 2012) and degeneracies with non-Gaussian effects (Bruni et al. 2012b; Maartens et al. 2012) must be included. To extract the dark energy signatures from degenerate cosmological effects, the theoretical predictions that simultaneously account for primordial non-Gaussianity, and relativistic and modified-gravity effects must be refined. \nLarge numerical simulations are required to understand the large-scale structure at high precision or the statistics of rare objects. Typically, simulations covering a substantial fraction of the Hubble volume trace only the evolution of the dark matter in N-body calculations; however, the importance of modeling baryonic effects in large cosmological volumes is well recognized (Di Matteo et al. 2012), including changes in the shape of the power spectrum measured by weak lensing. The simulation must have enough mass resolution to resolve the gravitationally bound dark matter halos that create the potential wells for galaxy formation. \nCurrent state-of-the-art dark-matter-only simulations have used several hundred billion mass tracer particles in volumes many Gpc on a side, requiring thousands of parallel processors and of order 10 million cpu-hours per run (Springel 2012; Alimi et al. 2012). Analysis of the LSST survey data will require many simulations of comparable volumes to explore the different physical models discussed in the previous paragraph, include gas physics in Gpc volumes, and the complicated selection functions (Overzier et al. 2012a). This will yield the required precision for numerical and “semi-analytic” models for galaxy formation to accurately compare with observations. \n## _**Photo-** z_ _**modeling**_ \n## _**Combining with external datasets: theory and systematics mitigation**_ \nMost estimates are that training sets of 20,000 objects or more will be required so that LSST dark energy inference is not dominated by systematics (e.g., Bernstein & Huterer 2009; Hearin et al. 2012b). However, as described in Section 3.7.3, obtaining a complete dataset of this size to LSST depth will pose major challenges. \n**Calibrating systematic biases due to incomplete training sets:** Fortunately, even if our photometric redshift measurements have systematic biases (e.g., due to incompleteness in the training set), so long as we can determine the level of those biases, dark energy inference will generally remain unscathed. A variety of cross-correlation techniques may be used to determine the actual redshift distribution of a sample, allowing us to detect biases that the likely incomplete spectroscopic training sets cannot. \n73 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nthat are at or near that redshift. We can exploit this fact to accurately determine the true redshift distribution of objects in a photometrically selected sample. \n## **3.7.3 Systematics** \nIn this section, we describe the major issues we will need to address in order to utilize photometric redshift measurements for dark energy science. \n**Obtaining large, deep training sets.** Measuring a statistically complete set of _>_ 20 _,_ 000 redshifts to full LSST depth will be a major challenge. The LSST ‘gold sample’ for dark energy measurements extends to _i_ = 25 _._ 3, more than 10 times fainter than those objects for which redshifts may be obtained (with _∼_ (60–70)% completeness) at 8–10 m telescopes in one hour of observing time (Lilly et al. 2009; Newman et al. 2012). At least 100 hours of observation time would thus be required to achieve that success rate for LSST calibration redshifts; with existing instruments, at most 100-300 redshifts can be obtained at a time, while the proposed PFS spectrograph (Ellis et al. 2012) should be able to target _>_ 2000 objects at once. Even a telescope with a 30 m diameter mirror would require _>_ 10 hours to obtain redshifts to LSST depth, and would do so for only a very limited number of objects in a small field of view. We will require a major international effort, which must be scoped and coordinated. This will require a long lead-time to spread telescope-time allocations over many cycles, so must begin now. \n**Incompleteness in training sets.** Faint galaxy surveys have systematically failed to obtain redshifts for a substantial fraction of their targets. For instance, the DEEP2 Galaxy Redshift Survey, for redshift quality classes with _>_ 99 _._ 5% (vs. _>_ 95%) reproducibility, obtained “secure” redshifts for only 60% (vs. 75%) of the _>_ 50 _,_ 000 galaxies targeted (Newman et al. 2012). Other surveys have done worse, with high-confidence redshift measurements for (21–59)% of targeted galaxies (Le F`evre et al. 2005; Garilli et al. 2008; Lilly et al. 2009). Obtaining _>_ 90% redshift completeness for a red galaxy at the LSST magnitude limit would take roughly one thousand hours of observation time (more than 100 nights) on an 8–10 m telescope. Deep infrared spectroscopy from space could lessen these problems, but will not solve them, due to the small field of view (and hence high shot noise and sample variance) of JWST and the limited depth and wavelength range of WFIRST or Euclid spectroscopy. \nIf the sorts of galaxies that fail to yield redshifts are localized in color space, problem galaxies could be excluded from analyses at the cost of reduced sample size; however, this is unlikely to always be possible, and could weaken dark energy constraints. For LSST, we will need to be prepared for the likelihood that we will have only incomplete training sets, both in considering photo- _z_ algorithms \n74 \n_3.7 Photo-z’s and common issues_ \nand in establishing calibration plans. Stage III surveys will need to make progress on this problem, as it is already a major issue at DES depth, but LSST photo- _z_ calibration requirements are _∼_ 2 _×_ more stringent, so additional efforts are necessary. \n**Erroneous calibration redshifts.** Even when considering only objects with ‘robust’, _>_ 95% secure, redshift measurements, existing deep samples have incorrect-redshift rates ranging from (0.3–0.4)% (for DEEP2, based on tests with repeated independent measurements) to ≳ 3% (for VVDS). Since objects with incorrect redshifts are generally at higher redshift ( _z_ ≳ 0 _._ 5), even DEEP2-like error rates may bias photo- _z_ calibration beyond LSST tolerances. New, robust calibration algorithms will likely be needed for LSST. This is also a serious issue for Stage III surveys; we plan to proceed in cooperation with Stage III projects on this challenge. \n**Template mismatch and incompleteness.** In template-based photometric redshift measurements, adding additional spectral templates increases computational requirements and the number of possible degeneracies, so the set is often limited to a small number (6–30) of templates that approximately span the range of expected SEDs. Galaxies that fall between these templates will have slightly biased predicted redshifts, resulting in a contribution to photo- _z_ errors from this “template mismatch variance”. In addition, incomplete sampling of the full galaxy SED distribution will introduce systematic biases in photo- _z_ measurements. Determination of both the size and contents of an optimal template set is an open question that will affect LSST data-processing requirements. \n**Impact of blended objects.** In DEEP2 data, the rate of objects that appear to be a single object from the ground but are resolved into multiple galaxies from space rises above 1% at _z >_ 1 (Newman et al. 2012). This is likely to set fundamental limits on LSST photo- _z_ performance, as no single redshift and SED will describe the blended object. Since the LSST sample will be fainter and at higher redshifts, this will be a more severe problem for LSST than Stage III surveys. \n76 \n_3.7 Photo-z’s and common issues_ \nthat probabilistic object classification should likely occur jointly with photo- _z_ estimation. However, techniques for doing this and the impact on LSST computing needs have not yet been explored. \n**Whether and how to use single-visit information.** Each LSST object will be imaged many times, often in different observing conditions. For filters that are more sensitive to atmospheric conditions we will be measuring flux through different effective bandpasses in each visit; other differences in effective wavelength will occur due to variations amongst the QE curves of the LSST detectors. It should be possible to use these variations to improve photo- _z_ estimates, as we effectively have measurements in more than six bands. However, variations in atmospheric conditions will also affect other aspects of photometric measurements (seeing, sky brightness, etc.) that may be difficult to disentangle. LSST Data Management currently has no provisions for storing filter information for individual visits; if this information would yield significant improvements in photo- _z_ ’s, we must know that soon to scope any data-storage requirements. \n1. As shown by Gorecki et al. 2012 (in prep), there can be gains from estimating photo- _z_ ’s with multiple, different algorithms, so we may wish to store a number of PDFs for each object, or else we will need to determine how to optimally combine them. \n2. Given the large size of LSST samples, we will need to perform all per-galaxy calculations rapidly; this may be challenging if single-visit measurements are incorporated. \n3. It will be necessary to develop optimized techniques and store information required to allow automated rejection of galaxies likely to have poor photo- _z_ estimates (Ben´ıtez 2000; Gorecki et al. 2012, in prep.). \n77 \n## **4 Simulation Tools and Technical Infrastructure** \n**Available codes and tools.** The collaboration owns a large suite of simulation and analysis tools that will be readily available to carry out the tasks identified for the LSST Development Phase, described in later sections. During the Construction Phase, these codes and tools will be extended and improved as required. \nFor the next set of simulations, up to multi-trillion particle high-resolution N-body runs will be required. To achieve this scale, one of the major codes to be used is the Hardware/Hybrid Accelerated Cosmology Code, HACC (Habib et al. 2009). HACC has been shown to work on a variety of high-performance computing architectures and to scale up to the largest systems currently avail- _∼_ able ( 750,000 cores on Argonne’s new BlueGene system Mira), enabling simulations with more than one trillion particles (Habib et al. to appear in the proceedings of SC12, see Figure 4.2). The code has been chosen as one of the Gordon Bell Award finalists this year (the award will be given in November 2012). We will also make extensive use of the P[3] M and Particle-Multi-Mesh (PMM) codes (Trac & Pen 2006), which have generated N-body simulations with up to 30 billion particles. All codes are equipped with halo finders and merger trees are constructed on the fly. In addition, we will use the publicly available TreePM code GADGET-2 (Springel 2005). \nFor hydrodynamic simulations, we will use a range of codes, including the Adaptive Mesh Refinement (AMR) Tree Code, ART, the AMR codes FLASH (Fryxell et al. 2000), RadHydro (Trac \n81 \n_Chapter 4: Simulation Tools and Technical Infrastructure_ \n**==> picture [253 x 255] intentionally omitted <==**\nFigure 4.2: Visualization of the density field from a 1.07 trillion particle simulation carried out with HACC on Mira, the new BG/Q supercomputer at Argonne National Laboratory. \n& Pen 2006) and Nyx (Almgren et al., submitted), as well as the publicly available SmoothedParticle Hydrodynamics (SPH) code GADGET-2 and the AMR code Enzo[1] . Both ART and FLASH are well-established codes, which include various additional physics models and built-in analysis tools. The two codes have been extensively used to investigate the physics of clusters of galaxies and galaxy formation. RadHydro combines a cosmological hydro code (moving frame hydro plus particle-mesh N-body) with an adaptive ray-tracing radiative transfer algorithm (Trac & Cen 2007) to simultaneously solve the coupled evolution of dark matter, baryons, and radiation. Nyx has been developed very recently with a first science focus on Lyman- _α_ simulations. A suite of analysis and visualization tools exist (Woodring et al. 2011; Turk et al. 2011; Behroozi et al. 2011) and several of the codes have built-in analysis capabilities. \nWith these codes and tools at hand, we will address the major cosmological simulation challenges for the LSST DESC. These challenges cover four key areas. \nFor the generation of mock data sets (as distinguished from the image pipeline), the physics that LSST will be able to measure must be included, along with its correlations. For instance, galactic \n> 1http://lca.ucsd.edu/portal/software/enzo \n82 \n_4.1 Simulations_ \nproperties that will be used (e.g., shapes, luminosities, colors for photometric redshifts) should be included, which can be done either in a dark matter or a hydro simulation. The procedures will necessarily require phenomenological assumptions at some level. Several simulations exist as starting points (e.g., the data from the Millennium simulation (Springel et al. 2005) has a ’start to finish’ implementation, but is small and still being improved; an impressive effort under development is the upcoming release of the Millennium Run Observatory (Overzier et al. 2012b)). There are dark matter simulations with large volume and high resolution, but in which the identification with observable properties is still under development. These efforts will require extensive calibration from observational data as well. The higher the dependence upon simulations for complementing the data, the more accurate they need to be. In particular, they need to accurately include the selection effects of the survey. \n**Prediction tools.** LSST measurements that will further our understanding of dark energy will probe deep into the nonlinear regime of structure formation. Precision predictions in this regime will therefore be essential and can only be obtained from detailed simulations over a wide range of cosmological models, augmented by observational inputs. These predictions will be important for all analysis teams and their associated tasks; these will be discussed in each of the sub-sections in Chapter 3 in the context of each probe. \nFor theoretical predictions, optimal strategies for generating a simulation campaign in order to explore a range of different proposed models exist (Heitmann et al. 2009). It has been shown that accurate prediction tools, so-called emulators, can be built from a (relatively) limited set of high-quality simulations (Heitmann et al. 2006; Habib et al. 2007; Heitmann et al. 2009; Lawrence et al. 2010b). A full set of simulations covering the model space of interest for LSST DESC is not yet in hand. Many proposed modifications of gravity and models for dark energy have not yet been simulated, and new proposals are still appearing regularly. \n**==> picture [468 x 120] intentionally omitted <==**\nFigure 4.5: An example of a single amplifier image (2k by 512 pixels). We have previously simulated millions of these images. Every photon used to construct these star and galaxy images has been simulated through detailed atmosphere, telescope, and camera physics models. \n89 \n## _Chapter 4: Simulation Tools and Technical Infrastructure_ \nellipticity from atmospheric turbulence by comparing with existing telescope data, (ii) the typical ellipticity de-correlation patterns due to the different wedges of atmospheric turbulence, (iii) the typical astrometric jitter on small spatial scales, (iv) the spot diagrams produced through optical designs from alternative benchmark raytrace codes, and (v) charge diffusion patterns similar to actual laboratory measurements. \nIn many cases, the fidelity in the simulator is sufficient for many scientific purposes. For example, the simulator could be used to estimate the approximate photometric depth of an exposure; produce images with reasonable PSF shapes, size, and patterns that LSST might have; and, model the photometric response and its variation across the field. However, the DESC will use the simulator to make detailed quantitative statements about dark energy measurements, so the fidelity of the models will have to have fully validated physical and numerical details. In addition, we expect that the wide range of uses for image simulations outlined for Weak Lensing (Section 5.1, Tasks 5.1.1:H1, 5.1.1:H-2 and 5.1.1:H-3), Large Scale Structure (Section 5.2, Tasks 5.2.1:H-1 and 5.2.1:H-2), Supernovae (Section 5.3, Tasks 5.3.1:H-2 and 5.3.1:H-3), Clusters (Section 5.4, Tasks 5.4.1:H-2 and 5.4.1:H-3), and Strong Lensing (Section 5.5, Tasks 5.5.1:H-2 and 5.5.1:H-3) require us to extend its functionality to support a variety of different uses. Thus, in this collaboration we expect we will need to improve both its fidelity further and extend its usability to meet the various analysis challenges associated with sensitive dark energy measurements. In Section 5.9.3, we discuss detailed plans to adapt its use for the DESC, and outline the main areas for fidelity improvement. \nThe main advantage of using a physics-based simulation approach outlined above is that the detailed physical models of this simulator can then be used to probe the limitations and systematics with dark energy measurements. To see this, consider that there are multiple image quality characteristics that describe an image: the point-spread-function (PSF) size and shape, the astrometric scale, and the photometric response. The physics in the simulator affects each of these in different ways. The atmospheric turbulence, charge diffusion in the detector, and thermal/mechanical perturbations of the optics, for example, affect the size and shape of the point-spread-function; whereas, the opacity in the atmosphere, the coatings on the optical surfaces, and the photo-electric conversion affect the photometric sensitivity in a wavelength-dependent manner. The dark energy measurements depend on each of these. For example, in Chapter 3, we discuss how weak and strong lensing depend critically on understanding the shape of the PSF to measure galaxy shapes accurately as well as the photometry response needed to generate photometric redshifts of galaxies. Weak lensing also depends on the ability to combine different exposures with the range of observational destails, as well as the non-ideal astrometric scale. Supernovae measurements, clusters, and large-scale structure depend critically on PSF and background details that affect faint source detection as well as the photometric response and calibration. Large scale structure also depends on the ability to combine exposures with different dithering patterns, sky brightness patterns, field-dependent opacity variation patterns, and details large-scale PSF wings from bright stars. Thus, the astrophysical observables can be directly linked to the relevant atmospheric and instrument physics in the simulator. We can then assess what improvements in algorithms are needed to obtain the sub-percent level of precision cosmology. High fidelity photon-based image simulations play an essential role in that process. \n90 \n_4.2 Computing infrastructure_ \n**==> picture [361 x 271] intentionally omitted <==**\nFigure 4.6: Simulations of a single star with successively more physics turned on in the photon simulator. The images overlay simulations in the u,r, and y filters in their respective RGB colors, which highlights the wavelengthdependence of the simulations. The images demonstrate the multiple physics models that contribute to the size, shape, and centroid of the PSF as well as the photometric response. \n## **4.2 Computing infrastructure** \n## **4.2.1 Development of a software framework** \nThe ultimate goal of the Dark Energy science working groups is to transform the data into constraints on dark energy. This over-arching challenge will be present at all stages of the project, starting from 2012, when the working groups begin to assess the effects of systematics on the science, all the way until 2030 when the full data set is complete. Even at the outset, one of the most important challenges is to construct a pipeline that transforms simulated data into dark energy constraints. The vision is that any working group member can generate a simulation using project tools such as ImSim; insert a systematic effect into such a simulation; run a pipeline to quantify the extent to which this systematic will bias the dark energy parameters; develop an algorithm that accounts for this systematic; and, run the pipeline again with this new algorithm to see if the bias has been removed and by how much the dark energy constraints have weakened (they will necessarily get weaker as nuisance parameters that account for systematics are allowed to vary). \nTo turn this vision into a reality, the collaboration will develop a unified software framework that can be used by all the science working groups. The framework will enable scientists to easily use project simulation tools, develop and run analysis code that will be used for all tasks taken on by the DESC. The framework will incorporate ImSim and the Data Management tool suite within \n91 \n## _Chapter 4: Simulation Tools and Technical Infrastructure_ \na flexible and robust environment. Every simulation will be tracked and stored in a common repository or repositories. Failures or transient delays will be monitored and aggressively attended to, with the goal of mitigating problems. Collaboration members will also have the capability to import external data sets (CMB, X-Ray, radio) in their simulations. The goal is to place the power of the project simulations in the hands of the scientists. The framework can be used in any of three deployments: High performance machines typically found at national labs; university and small-scale clusters; and laptops. \nThe framework will help literally every task listed in Section 5.1, as each will use the project simulations to test systematics. Some of them will not use the full level 3 toolkit, but even this will become relevant for more and more of the tasks as progress is made and metrics are required. \n## **4.2.2 Development of a computational model** \nThe Working Groups will require a wide range of computing resources to reach their analysis goals. A typical analysis sequence starts with significant processing on supercomputers to create cosmology simulations, followed by thousands of processing hours on mid-range computers to trace simulated photons through to the camera, rounded out by processing for source localization. The needs of the Working Groups will evolve with time from less intensive tasks in the first few years to more heavy-duty tasks in intermediate years. Once LSST is operating, needs will be dominated by significant reprocessing of images to satisfy DESC. \nThe LSST Data Management team produces semi-annual “Data Challenges” in which simulated data or real data from another project are processed with the latest LSST analysis pipeline to test specific functionalities in each Challenge. The DESC will need processed data sets to address questions that are not necessarily a match to the priorities of DM. This will require generating and processing both end-to-end simulations for short-turnaround studies and large-scale simulations similar in size to the Data Challenges of the Project. \nThe Data Management team produces semi-annual LSST “Data Challenges”, in which the LSST analysis pipeline is run on either simulated images or images from existing telescopes. To satisfy the specific needs and address the priorities of the DESC, we will produce data products beyond the LSST Data Challenges. \nThe Working Groups have identified some 30 high priority tasks to address in the next three years. There are clear patterns in the types of simulations and post-processing steps called for in their work plans. Table 4.1 summarizes which simulations and toolchains (both standard and customized) are called for by each WG. Clearly, ImSim is a central element of the Working Group agenda. \n## **H-1. WL: Estimate importance of PSF effects of currently unknown size** \n- _Motivation:_ One of the largest sources of systematic error in weak lensing measurements is improper estimation of the point-spread function (PSF). Most aspects of this problem are fairly well understood; however, there are several that may be of particular importance for LSST. We must determine how important these effects are and whether LSST Data Management needs to change their algorithm(s) for dealing with these issues. (1) \n97 \n## _Chapter 5: Detailed Work Plan for the Next Three Years_ \nWe need to determine how best to deal with the fact that stars and galaxies have different spectral energy distributions (SEDs), which means that the measured PSF from stars is not the correct PSF to apply to the galaxies. (2) We need to determine how well we need to know the small-scale stochastic PSF variation and whether additional information can be useful to improve the PSF interpolation between stars. (3) We need to determine how best to deal with the fact that the atmospheric PSF includes a centroid shift. \n   - _Activities:_ We will quantify the error in the inferred galaxy shapes by (1) using the stellar PSF rather than the correct PSF for galaxy SEDs, (2) interpolating using only the PSF at the locations of the stars, and (3) ignoring the centroid shift in the PSF. We will then explore algorithms for dealing more effectively with these effects by (1) using the photometric color information ( _r − i_ , _g − r_ ) as a proxy for the galaxies’ SEDs, (2) incorporating additional sources of information about the small scale PSF variation such as the wavefront and guidance sensors, and (3) including the centroid shift as part of the PSF, to be interpolated along with the rest of the PSF. \n   - _Deliverables:_ We will inform LSST Data Management of the requirements on the PSF estimation routines given what we learn about each of these effects; delivery of specific guidance on algorithms to ameliorate these effects is an enhanced goal. \n- **H-2. WL: Place requirements on Data Management algorithms that impact weak lensing analysis** \n   - _Motivation:_ As described in Section 3.1.3, the requirements for the star-galaxy separation algorithm and the solution of the world coordinate system (WCS) are somewhat uncertain, so we need to provide Data Management with more specific requirements for the algorithms they are developing. In particular, for the former, we need to know how many non-stellar objects (small galaxies, binary stars, stars with cosmic rays, etc.) may be permitted to be used for PSF estimation. For the latter, we need to define more precisely the accuracy required for the Jacobian of the WCS and for determining the location of the same point from one observation to another. \n   - _Motivation:_ For several major weak-lensing–related systematic errors (e.g., related to PSF correction of galaxy shapes), we anticipate major progress to be associated with Stage III surveys in the next few years. It is important to assess whether new algorithms that are being developed now and in the next few years can be used for LSST, and/or whether the more stringent requirements on systematic errors in LSST require these algorithms to be enhanced. \n   - _Activities:_ As new algorithms are developed for Stage III surveys, we will test them on simulated LSST data (ImSim), and assess the level of systematics in that context. In nearly all cases, LSST will be in a different regime from Stage III surveys in terms of galaxy population, depth, and number of exposures, so we will evaluate how those differences could affect Stage III survey algorithm performance in an LSST context. \n   - _Deliverables:_ Recommendations for which algorithms from Stage III surveys should be adopted, either as-is or with recommended modifications; recommendations for which steps of the analysis process might require development of completely new algorithms. \n## **5.2 Large scale structure** \n## **5.2.1 High-Priority Tasks** \nThe large scale structure task list was constructed based on several considerations. Of primary importance are those tasks that (a) may cause a change in survey plans (for example, dithering pattern, survey strategies, hardware); (b) involve developing tools that are of long-term value for the LSST DESC (for example, developing software to reach the maximum DE constraining power via large scale structure); or, (c) provide stringent requirements on the telescope system or data processing steps (DM). Since photometric BAO is a recently matured technique, our experience and knowledge is based largely on analysis performed on SDSS data. Therefore, there are many unknowns in our analysis steps, since photometric BAO has not been done in any datasets except SDSS. We decided to encompass a large range of systematics when designing our tasks. Thus, our \n100 \n_5.2 Large scale structure_ \ntasks may appear to be over-encompassing, and not necessarily focused on one or two systematics, but very often on a long list of systematics. This is merely a reflection of our belief that the LSST data may offer a significantly different challenge from SDSS data. We expect the focus of the LSS working group to evolve considerably over the coming decade. \n- **H-1. LSS: Tools to estimate, mitigate and remove key known potential systematics, more specifically sky brightness, stellar density obscuration and contamination, extinction, and seeing** \n   - _Motivation:_ In order to deliver the promised Dark Energy constraints from Baryon Acoustic Oscillations in LSST, we need to develop ways to not only detect but also mitigate currently known systematics in photometric BAO analysis. We will need to understand the extent of their effects and mitigate or remove them in order to extract the best constraints from photometric BAO in LSST. \n   - _Deliverables:_ Existing systematics detection and/or removal pipelines, new systematic removal codes; Full-sky simulations of stellar contamination and obscuration, along with other key systematics, as described in Section 3.2.3 . \n- **H-2. LSS: Analyze image simulations of multiple contiguous LSST pointings** \n   - _Motivation:_ While there will be _∼_ 100 visits in multiple filters, there may be print-through of residual systematics in magnitude zeropoints and PSF on dither scales. This may affect the LSS signals in BAO, WL shear, and WL magnification. A sky area of several contiguous fields will reveal all these systematics. \n- _Deliverables:_ A portable, open-source pipeline with documented installation and running instructions, and reproducible benchmarks on simulated and precursor survey data, to be made available to the entire DESC collaboration. \n## **5.3 Supernovae** \n## **5.3.1 High-Priority Tasks** \nWe have prioritized three pressing high-priority tasks specific to supernovae. \n   - _Motivation:_ LSST is faced with a “new” source of uncertainty as most objects will not be observed spectroscopically to determine the redshift and classification. We currently have no motivated estimate of the full science reach of the LSST SN program due to a lack of ability to fully predict the power of the photometric sample. \n   - _Deliverables:_ Algorithms for photometric-only analysis with corresponding tools that calculate Hubble Diagram biases and covariance matrices due to photometric classification and/or redshift determination. These algorithms shall be implemented as modules that plug into the framework established by Task 5.3.1:H-2. \n- **H-2. SN: Design an end-to-end analysis pipeline that starts with survey properties and finishes with cosmology projections** \n   - _Motivation:_ We will continuously need to project the impact of the LSST survey on measuring dark energy parameters. Although we have monolithic tools and some pieces of code to address these questions now, we anticipate that the analysis will evolve between now \n103 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \nand the commissioning of LSST. We need a flexible, modular analysis pipeline that can support this evolution. Code used for projections can also be used for the real data analysis. Writing good software requires careful and thoughtful design; it is never too early to begin this process. \n- _Activities:_ Identify and collect current existing codes. Identify major gaps to fill in interfaces and usability. Design prototype modular system. Provide these to DESC supernova group for review and testing. Establish standard formats and analysis in communication with Stage III projects. Coordinate with those involved in Tasks 5.3.1:H-1, 5.3.1:H-3 and 5.3.2:LT-1 to define standards and include those new modules when available. \n_Deliverables:_ A reviewed design of the analysis pipeline with a prototype implementation. \n- **H-3. SN: Supernova realizer for simulations** \n   - _Motivation:_ A lesson learned from DES: realistic SNe must be designed into project simulations. The supernova population is an initial input needed for all of the other supernova tasks, and is therefore urgently needed. \n   - _Activities:_ Implement realistic supernova populations into LSST ImSim. This will start with basic models and will grow to include more complicated models of host galaxy and supernova properties and covariance. Collaborate with the ImSim team to test and integrate this code into the LSST ImSim. \n   - _Deliverables:_ Code that realizes supernovae and furnishes their intrinsic SED at specified observing epochs. \n## **5.3.2 Longer-Term Tasks** \nWe fundamentally need to understand supernova better as distance indicators in the cosmos to fully exploit the potential of LSST to measure dark energy. This will require sustained effort and different investigations over the next decade. \n- **LT-1. SN: Develop theoretical/numerical/empirical SN models to better describe or improve the distance indicator** \n   - _Motivation:_ Improved supernova models can reduce statistical uncertainty and quantify systematic uncertainties. They are therefore important in determining the error budget and projecting LSST science reach. \n   - _Deliverables:_ Code for SN models that can work within the analysis pipeline; written reports of application of improved models to LSST SN cosmology. \n104 \n_5.4 Clusters_ \n## **5.4 Clusters** \n## **5.4.1 High-Priority Tasks** \n- _Motivation:_ Accurate absolute calibration of the key mass-observable scaling relations using weak lensing techniques is critical to the extraction of robust cosmological constraints. The shear signals of background galaxies viewed near the centers of clusters are larger but more sensitive to systematics originating from projection effects (due to, for example, triaxiality, large scale structure) and mis-centering. Determining the optimal radial ranges and shear profiles to fit, and quantifying the expected accuracy of mass calibration, will impact on the entire analysis strategy. \n- _Activities:_ The task will employ cosmological simulations spanning the mass and redshift ranges of interest, with realistic galaxy populations superimposed (to estimate miscentering effects). Ray-traced shear patterns for the simulated clusters will be analysed to quantify the accuracy and precision of the mass calibration obtainable, as well as the scatter (size and shape) about the mean, as a function of radial filter and fitting profile. \n- _Deliverables:_ The deliverables will be tabulated mass calibration precisions and accuracies, and measurements of the scatter (size and shape) about the mean, as a function of cluster mass and redshift, for the various algorithms employed. \n## **H-2. Cl: Extending shear calibration programs into the cluster regime** \n   - _Motivation:_ The weak lensing shears associated with galaxy clusters exceed those of typical large scale structure. Shear measurement algorithms must be calibrated in the cluster regime. The innermost regions of clusters also exhibit higher-order lensing effects that will impact on shear measurements, if unaccounted for. \n   - _Activities:_ The task will develop STEP-like simulations spanning the shear regime of clusters and enabling blind tests of the shear measurement algorithms in this regime. The LSST image simulation codes will also be extended to incorporate higher order lensing distortions such as flexion and multiple imaging that will be detected near cluster centers. \n   - _Deliverables:_ This task will be deliver STEP-like simulations spanning the shear regime of clusters and enabling blind tests of the shear measurement algorithms. This will enable robust quantification of shear biases (and therefore mass biases) as a function of algorithm, shear and other parameters of interest. \n- **H-3. Cl: The impact of photometric redshift uncertainties on cluster mass calibration** \n   - _Motivation:_ Photometric redshifts are important both for determining cluster redshifts and for estimating the redshift distributions of background galaxies employed in weak lensing mass calibration. However, the cluster environment is not typical and the photo- _z_ calibration requirements for clusters may differ from the field. \n105 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Deliverables:_ Code to generate the above prior PDFs; quantification of the systematic error floor dictated by line of sight mass structure. \n## **H-2. SL: Automated lens candidate detection in the LSST catalogs and images** \n- _Motivation:_ Automated lens classification by database search followed by image modeling is the key process that will define strong lens yield. What purity, completeness and rejection rate can be obtained with LSST? How does this depend on model PSF quality and the deblender performance? Byproducts from such an analysis include image lightcurves and covariance matrices, which are the key inputs to time delay inference. The sensitivity of the lens sample size to the performance of the level 2 LSST Data Management software suggests that, to optimize the strong lensing DE science output, we need to play a supporting role in its development. Image-based automatic classification will be implemented via the DM level 3 API, and could require a significant amount of computing resources to run. Early prototyping will allow this risk to be evaluated and reduced, and will provide feedback to the LSST Project on the level 3 interface. \n- _Activities:_ Use ImSim (DC and custom) and Stage III (PS1, DES, HSC) catalogs and images to (a) test the level 2 deblending software, (b) develop algorithms for fully-automated catalog-based lens candidate selection, (c) develop algorithms for fully-automated imagebased lens classification and measurement, and (d) implement these algorithms using the level 3 API. \n- _Deliverables:_ A set of requirements on the source de-blender level 2 software, in the form of mock datasets for unit tests, and a set of write requirements on the level 3 API; new algorithms and their prototype level 3 implementations, with their performance assessed. \n## **H-3. SL: Time delay estimation** \n- _Motivation:_ Time delay uncertainties (statistical and systematic) need to be a few percent or lower for each lens in the cosmographic ensemble for them not to dominate the error budget. It is not known what fraction of LSST time delay lenses will meet this criteria. The basic feasibility of high-precision cosmography with time delay lenses is determined by the number of well-measured systems: this needs to be determined as soon as possible in order to identify the required improvements in the observing strategy of analysis software. \n- _Activities:_ Use simulated lightcurves in 6 filters, sampled at realistic OpSim-generated cadence, to probe the available time delay precision and accuracy, using both “standard” and newly-developed algorithms. Realistic microlensing and intrinsic variability must both be included, as must realistic observing conditions. \n107 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Deliverables:_ Analysis code for inferring time delays from LSST data, and a pipeline for computing time delay uncertainty metrics given OpSim inputs, to allow requirements on the observing strategy to be explored. \n## **5.5.2 Longer-Term Tasks** \n- **LT-1. SL: Explore multiple source plane cosmography as a competitive DE probe** \n   - _Motivation:_ Preliminary studies suggest that multiple source plane compound strong lens systems could provide valuable additional information about dark energy, but detailed forecasts for DE from plausible (but as yet poorly understood) samples of galaxy- and cluster-scale systems need to be developed to assess the impact of systematic errors associated with the mass modeling. This analysis needs to be done soon, to enable planning for a high-precision analysis to begin if it is justified, but depends on the joint analysis pipeline developed in Task 5.6.1:H-1. \n   - _Activities:_ Simulate (a) realistic compound lens mass distributions, including line-of-sight structures down to subhalo mass scales, and (b) realistic LSST imaging data; model them, in order to estimate (a) the useability and (b) the detectability of multiple source plane lenses for dark energy science. \n   - _Deliverables:_ A mock catalog of compound lenses, and a simple pipeline plug-in to test its cosmographic information content. \n## **5.6 Theory and joint probes** \n## **5.6.1 High-Priority Tasks** \nTo fulfill the mission of the DESC, we must build a comprehensive analysis pipeline that extracts as much DE information as possible from LSST with enhancement of external data. Working toward this goal, we plan a set of tasks that will set up the architecture of the analysis pipeline and deliver many of the building blocks. \n## **H-1. TJP: Dark energy analysis pipeline** \n- _Motivation:_ To design the best dark energy experiment with LSST, we must be able to quantify the impacts of the LSST data model and our control of systematic uncertainties. We also need to parameterize, and account for, the many different effects that dark energy can have on LSST survey data. The effort will be responsive to evolution in the survey and developments in dark energy theory, as they occur. \n- _Activities:_ Characterization of the LSST data model, including instrumental and site-based atmospheric uncertainties and the most up-to-date survey information, in collaboration with LSST personnel; develop a comprehensive list and parameterizations for key astrophysical systematics; develop likelihood techniques; build a suite of software tools \n108 \n_5.6 Theory and joint probes_ \nto integrate systematics, dark energy theory/modified gravity models to predict all observables for LSST, external datasets and cross-correlations. \n- **H-2. TJP: Exploring LSST DE science capability – galaxy modeling** \n   - _Motivation:_ Improve and validate predictions for the galaxy bias and intrinsic alignments using numerical simulations and precursor surveys \n   - _Activities:_ Perform large-volume, baryonic cosmological simulations to develop and test parameterized phenomenological models of baryonic effects. Develop analysis algorithms that reduce the sensitivity of DE probes to uncertainties in the galaxy bias and intrinsic alignments. Tie in with the large number of high-resolution N-body simulations for representative DE models to further reduce the uncertainty of matter power spectrum on nonlinear scales. \n   - _Deliverables:_ Large scale baryonic simulations and phenomenological models for parameterizing baryonic effects on galaxy clustering statistics. \n- **H-3. TJP: Understanding statistics over very large scales** \n   - _Motivation:_ Unprecedented LSST survey volume enables DE studies over very large scales, though one must disentangle real DE signal from observational and other theoretical effects. \n   - _Activities:_ (1) Analyze how the clustering on large scale depend on the dark energy/modified gravity parameters. (2) Develop an accurate theoretical model for analyzing the clustering on very large scales, including wide-angle and GR corrections. (3) Identify optimal statistics to extract information from clustering analyses on those scales. (4) Investigate degeneracies and dependencies of the improved model. (5) Develop simulations that incorporate those very large scale effects. (6) Use simulations to improve our understanding of large-scale clustering signatures and systematics for the range of plausible dark energy models of interest. \n   - _Deliverables:_ (1) Theoretical understanding and precise modeling of clustering on very large scales. (2) Codes for analyzing clustering on large scales that include a realistic description of the geometry of the system, wide-angle and general relativistic corrections. (3) A simulation code and simulated data that incorporate effects on very large scales. \n- **H-4. TJP: Exploring LSST DE science capability – photometric redshifts** \n   - _Motivation:_ Photo- _z_ systematics impact all LSST DE probes and must be well understood and modeled for DE analyses. \n109 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Deliverables:_ A realistic model of the photo- _z_ error distribution and integration into LSST DE forecast tools. \n## **5.7 Photometric redshifts** \n## **5.7.1 High-Priority Tasks** \n## **H-1. Phz: Calibration strategies** \n   - _Motivation:_ Calibration of photo- _z_ algorithms requires a “truth” set of secure redshifts for a representative subsample of galaxies. Incompleteness in training data will lead to biases in LSST redshift estimates, which will propagate directly into the dark energy constraints if not accounted for otherwise (see also Task 5.7.1:H-3). Obtaining training sets must begin soon due to the large investments of telescope time required. \n   - _Activities:_ We will develop a detailed plan for targeted spectroscopic observations sufficient to meet LSST calibration/training set requirements, and reach out to potential partners. We will also explore obtaining very deep multi-wavelength imaging that enables very accurate photo- _z_ estimates of faint galaxies for which spectroscopic measurements are difficult. We will investigate synergies with upcoming large space based missions (EUCLID, WFIRST) that could be mutually beneficial. \n110 \n_5.8 Cross-working group tasks_ \ninvestigating tradeoffs between photo- _z_ performance and sample size, and investigate approaches that optimize dark energy errors. \n   - _Deliverables:_ Realistic simulation code and outputs; improved algorithms to identify problematic areas of parameter space; updated Science Requirements. \n- **H-3. Phz: Testing cross-correlation techniques** \n   - _Deliverables:_ Comparison of actual to predicted reconstruction errors; assessment of residual reconstruction errors from bias evolution and from covariance of photo- _z_ errors and galaxy properties. \n## **5.7.2 Longer-Term Tasks** \n_Deliverables:_ Existing systematics detection pipelines, new systematic detection pipelines. \n## **H-4. CWG: Deblending for weak lensing and cluster cosmology** \n- _Motivation:_ As described in Section 3.1.3, the high degree of galaxy overlap in LSST stacks requires new deblending algorithms and will likely introduce some level of bias to the resulting shape measurements. Potential biases need to be studied and quantified, resulting in an initial set of deblender requirements for the data management (DM) group, who are scheduled to begin work on deblending algorithms in 2013. In particular, galaxy clusters are among the densest source environments that LSST will encounter and pose some of the severest challenges to its deblending algorithm. Limitations in the deblending algorithm can impact the cosmological constraints from clusters – e.g., by inducing density-dependent errors in shear measurements, photometric calibration, and possible misidentification of the central galaxy. \n- _Activities:_ We will characterize the level of blending expected in LSST, using realistic ImSim runs, in terms of the distance, size, and magnitude of objects falling into the typical aperture used by shape measurement algorithms. We will determine to what level these interloping objects bias the shape measurements of galaxies and thus the level to which they need to be correctly removed. We will use the ImSim software to generate realistic simulated cluster images (based on deep multi-band HST images of known clusters of galaxies) spanning a wide range of richness and redshifts. These will be used to determine the biases in shape and photometric measurements – and therefore mass calibration – induced by the deblending algorithm, as a function of magnitude and overdensity (radius). \n113 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Deliverables:_ We will provide LSST DM with specifications of the expected level of overlaps, and performance targets for either masking or weighting overlap pixels as a function of the relevant parameters of the blended objects. We will quantify the performance of the deblending algorithm in the high galaxy density environments of clusters, enabling calibration of its deficiencies. A list of specific “error modes” will be provided, identifying the limitations of the deblending algorithm that have the greatest impact on cluster cosmology and the highest priority for improvement. \n## **5.8.2 Longer-Term Tasks** \n## **LT-1. CWG: Enhancing LSST DE science with external CMB lensing maps** \n- _Motivation:_ External CMB lensing datasets can be very useful in determining galaxy bias, and in mitigating LSST galaxy lensing systematics. We need to find an optimal way to analyze all the datasets and know how well they perform in combination. \n- _Activities:_ Explore the utility to LSST cosmological parameter estimation and systematic mitigation of high-precision CMB lensing potential maps reconstructed from next-generation CMB polarization maps covering nearly the entire LSST survey footprint. Work will be done with a Fisher matrix analysis in a staged progression of complexity of the description of systematic error sources, eventually to be combined with larger LSST simulation efforts. Statistical quantities that will be studied are the various two-point functions possible with the collection of lensing maps (one CMB + multiple LSST maps) and galaxy-count maps. \n- _Deliverables:_ (1) Tools to explore cross-correlation of CMB lensing and LSST galaxy lensing maps. (2) Assessment of impact of CMB-galaxy cross-correlations on reducing uncertainties in astrophysical and observational systematics and improvements on dark energy constraints. \n## **LT-2. CWG: Enhancing LSST DE science with external galaxy-cluster datasets** \n   - _Motivation:_ External cluster datasets can be very useful to help provide important complementary cluster mass estimates when analyzed jointly with LSST data. We need to find an optimal way to analyze all the datasets and know how well they perform. \n   - _Activities:_ Investigate opportunities to improve cluster scaling relations and cosmological constraints from combined analyses of LSST cluster lensing and CMB SZ and X-ray cluster data, with particular focus on fields overlapping with the LSST survey footprint. \n   - _Deliverables:_ Assessment of improved cluster cosmology constraints and cluster-mass scaling relations from combining CMB SZ, X-ray and LSST cluster surveys. \n- **LT-3. CWG: Enhancing LSST DE science with external spectroscopic galaxy data** \n   - _Motivation:_ External spectroscopic data can be very useful when analyzed jointly with LSST data. We need to find an optimal way to analyze all the datasets and know how well they perform. \n114 \n_5.9 Simulations improvements_ \n- _Activities:_ (1) Cross-correlation with spectroscopic surveys: Develop algorithms to measure clustering statistics jointly between lensing, photometric, and spectroscopic galaxy data. (2) Develop the theoretical framework to interpret these measurements for dark energy and modified gravity theories. (3) Develop tools and models that enable us to push as far as possible into the nonlinear regime, including the behavior of deviations from the simple, separable power spectrum, with well-understood limitations. Work in collaboration with the simulation effort to model redshift-space clustering in simulations. (4) Work with the cross-cutting photo- _z_ effort to use spectroscopic galaxy data to study how imaging data can be used not only to estimate photo- _z_ ’s but to identify groups of photometric galaxies with a “controllable” distribution of biases. \n- _Deliverables:_ (1) Cross-correlation with spectroscopic surveys: rudimentary code to make multi-tracer clustering measurements on overlapping mock datasets. (2) Theoretical fitting code implementing our current best understanding of how to deal with the onset of nonlinearity and other complications in the various tracers. A by-product of this will be more realistic, comprehensive projections of constraining power. (3) Forecasted constraints on prospective modified-gravity/dark-energy constraints from a combination of relativistic and non-relativistic tracers. \n**LT-4. CWG: Enhancing LSST DE science with external supernova datasets** \n- _Motivation:_ External datasets such as spectroscopy and NIR imaging can be used to improve supernova distance measurements and cosmological analyses, increasing the power of LSST SNe. We need new tools to identify the optimal uses of outside resources and advanced planning to acquire them. \n- _Activities:_ Extend the end-to-end analysis tool from Task 5.3.1:H-2 to include additional surveys or information. Model additional information such as spectra of supernovae or host galaxies and how that will affect the systematics error budget for distance determination. Investigate opportunities to coordinate with complementary surveys such as Euclid to develop and test joint strategies. \n- _Deliverables:_ Code that takes as input characteristics of LSST and external-survey observations and outputs the resulting figure of merit. Assessment of improved supernova cosmology constraints as a function of amount of external spectroscopy and NIR imaging. A written report on findings. \n## **5.9 Simulations improvements and development** \n## **5.9.1 Cosmological Simulation Tasks** \n## _**High-Priority Tasks**_ \n## **H-1. CoSim: Simulations for mock catalog generation** \n115 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n## **H-3. CoSim: Astrophysical systematics** \n- _Motivation:_ Understanding, quantifying, and mitigating astrophysical systematics on the observables of interest will be an enormous challenge for the simulation working group. In particular, focus topics include: understanding the theoretical systematics issues in measurements of various two-point (and higher) statistics, cross-correlations between probes, and cluster masses. Without substantial progress in this area, it will be very difficult to probe deeper into the nonlinear regime of structure formation and obtain the measurement accuracy on dark energy parameters aimed at by the LSST DESC. \n- _Activities:_ This task will require improvements in our modeling and simulation capabilities to accurately capture the baryonic physics that influence the observables of interest. Together with the analysis groups, we will develop a plan on how to address these uncertainties, map out at what level of accuracy they have to be understood, and create a first test simulation campaign including different astrophysical effects. This will be particularly important for weak lensing and clusters. Since it will be impossible to simulate all of these effects from first principles, new approaches have to be developed to allow us to model some of them in post-processing. \n## **5.10 Development of a software framework** \n## **5.10.1 High-Priority Tasks** \n## **H-1. SW: End-to-end simulator capable of running at small scales** \n- _Motivation:_ Every user in the collaboration should be able to exploit the powerful simulation tools built by the project in order to test a wide variety of systematics. A framework that makes importing and using these tools as easy as possible for even the casual user will greatly enhance the science reach of the collaboration.",
  "introduction": "|1.2|Description of the LSST system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|9|\n|1.3|Planned survey strategy and delivered data products . . . . . . . . . . . . . . . . . .|11|\n|1.4|Dark Energy overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|12|\n|1.5|Precursor surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|13|\n|1.6|Gains from LSST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|15|\n|2|The LSST Dark Energy Science Collaboration . . . . . . . . . . . . . . . . . . . . . . .|17|\n|2.1|Need . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|17|\n|2.2|Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|18|\n|2.3|Working Groups<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|21|\n|2.4|Policy development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|22|\n|2.5|Membership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|22|\n|2.6|Overview of work plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|26|\n|3|The Analysis Framework and Key Systematics for Investigation . . . . . . . . . . . . .|29|\n|3.1|Weak lensing<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|29|\n|3.2|Large-scale structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|38|\n|3.3|Supernovae<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|44|\n|3.4|Clusters of galaxies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|50|\n|3.5|Strong lensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|55|\n|3.6|Theory and joint probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|64|\n|3.7|Photo-_z_’s and common issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|72|\n|4|Simulation Tools and Technical Infrastructure . . . . . . . . . . . . . . . . . . . . . . .|79|\n|4.1|Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|79|\n|4.2|Computing infrastructure<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|91|\n|4.3|Technical coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|95|\n|5|Detailed Work Plan for the Next Three Years . . . . . . . . . . . . . . . . . . . . . . .|97|\n|5.1|Weak lensing<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|97|\n|5.2|Large scale structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|100|\n|5.3|Supernovae<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|103|\n|5.4|Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|105|\n|5.5|Strong lensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|106|\n|5.6|Theory and joint probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|108|\n|5.7|Photometric redshifts<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|110|\n|5.8|Cross-working group tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|111|\n|5.9|Simulations improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|115|\n|5.10|Software framework<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|123|\n|5.11|Computing model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|125|\n5 \n_Contents_ \n- 5.12 Technical coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 \n6 \n## **1 LSST as a Dark Energy Experiment**",
  "results": "The LSST project is a partnership among the National Science Foundation (NSF), the Department of Energy (DOE) Office of Science, and public and private organizations in the United States and abroad.[1] The NSF is responsible for site and telescope development and the data management system, while the DOE is responsible for development and delivery of the large-format camera. Private contributions have already been used for fabrication of the mirrors and site preparation in Chile. In April 2012, the camera project received “Critical Decision 1” approval by the DOE to move into the detailed engineering design, schedule, and budget phase. In July 2012, the National Science Board of the NSF approved the LSST as a Major Research Equipment and Facilities Construction (MREFC) project, allowing the NSF Director to advance the project to the final design stage and include funds for LSST construction in a future budget request. If all continues as planned, construction will begin in 2014 and is anticipated to last five years, followed by a two-year commissioning period before the start of the survey in 2021. \nThe telescope, camera, and data management system are designed and built by the LSST Project Team, which is responsible for producing the facility but not for the scientific analysis of the data, which will be made public to the US and Chilean communities and some international partners. Hence, the Project Team is not a scientific collaboration in the usual sense. In 2008, eleven separate quasi-independent science collaborations were formed to focus on a broad range of topics in astronomy and cosmology that the LSST could address. Five of the science collaborations have relevance to the study of dark energy (DE) and have provided invaluable guidance to the Project Team; however, they have not been formally organized or funded to pursue the full range \n> 1 The total construction cost of LSST is estimated to be about $665M, approximately 70% from NSF, 24% from DOE, and 6% from private donors to the project. \n7 \n## _Chapter 1: LSST as a Dark Energy Experiment_ \nof investigations required to guarantee that the most sensitive constraints on the nature of dark energy can be derived from the LSST data. \nIn this White Paper, we describe in detail how the various DE analyses that we expect to perform are sensitive to a large number of potential systematic uncertainties that must be identified, quantified, and minimized in order for the DE investigations to achieve their full potential. In addition, new algorithms must be developed and tested to enable those analyses, and an efficient computational and software framework must be established to perform the necessary calculations. All of these activities require an extensive, coordinated research effort well in advance of the onset of data taking. To address that need, we have created the LSST **Dark Energy Science Collaboration** (DESC). \nThe DESC is organized around five probes of dark energy enabled by the LSST data:[2] \n1. Weak gravitational lensing (WL) – the deflection of light from distant sources due to the bending of space-time by baryonic and dark matter along the line of sight. \n2. Large-scale structure (LSS) – the large-scale power spectrum for the spatial distribution of matter as a function of redshift. This includes the Baryonic Acoustic Oscillations (BAO) measurement of the distance-redshift relation. \n3. Type Ia Supernovae (SN) – luminosity distance as a function of redshift measured with Type Ia SN as standardizable candles. \n4. Galaxy clusters (Cl) – the spatial density, distribution, and masses of galaxy clusters as a function of redshift. \n5. Strong gravitational lensing (SL) – the angular displacement, morphological distortion, and time delay for the multiple images of a source object due to a massive foreground object. \nThese include the four techniques (WL, LSS, SN, Cl) described in the 2006 Report of the Dark Energy Task Force (DETF, Albrecht et al. (2006)). The DESC will identify and work to minimize the most significant systematic uncertainties (hereafter referred to as “systematics”) that limit the sensitivity of each probe, beginning with those that are most time-urgent. \nThe DESC will also address high priority tasks that are common to all five probes: \n- Calibration strategies for photometric redshifts (Phz). \n- Cosmological simulations (CoSim), simulated catalogs (CatSim), and photon-level simulations (PhoSim) with the fidelity needed to fully assess and exploit each probe of dark energy with the LSST. \n- Cross working group tools for data quality assessment and detection of systematics (CWG). \n- Realistic data model (DM), software framework (SW), and computing model (CM) to fully address DE science. \n- Technical coordination tasks related to the instrument model, calibration, and survey operations (TC). \n> 2The keys WL, LSS, SN, Cl, SL, Phz, CoSim, CatSim, PhoSim, CWG, DM, SW, CM, and TC will be used to identify tasks in Chapter 5. \n8 \n_1.2 Description of the LSST system_ \n- Theory and framework for combining and jointly interpreting dark-energy probes (TJP). \nIn the remainder of this chapter, we describe the LSST project, the survey strategy, and the data products that will be delivered by the project. We give an overview of DE science, with a focus on the theoretical challenges to understanding the accelerating expansion of the Universe and the types of measurements that are needed to distinguish between competing hypotheses. We summarize precursor imaging, spectroscopic, and time-domain surveys that are relevant to DE science and are expected to have mapped parts of the sky to various redshift depths by 2020 – “Stage III” projects in the parlance of the DETF. We conclude this chapter with an assessment of the gains that the LSST will bring not only in the DE figure of merit defined by the DETF, but also in mitigating systematic uncertainties through the statistical power and combination of probes provided by the LSST. \nIn Chapter 2, we describe in more detail the need for the LSST Dark Energy Science Collaboration, introduce the minimal governance structure that is being used to get the DESC off the ground, and present the general structure of the work plan for the next three years. \nIn Chapter 3, we describe the major analyses and the primary sources of systematic uncertainties for each DE probe. We describe the technical tools and framework that will be necessary to fully address DE science with the LSST in Chapter 4. \nWe present a compilation of all time-urgent, high-priority tasks (designated by H) and important longer term tasks (designated by LT) in Chapter 5. The tasks are arranged according to the DE probes and cross-cutting issues listed above. For each task, we describe the motivation, the planned activities, and the expected deliverables. We have identified the important systematic uncertainties and have prioritized addressing those that could still inform the final design of the project or survey strategy, or for which the systematic uncertainty is unlikely to be addressed by Stage III surveys, or for which the systematic is of unknown size or may need new strategies to address. \nThis White Paper as a whole provides an integrated picture of the DE science analyses we expect to tackle with the LSST data set and the extensive preparatory work that must be accomplished to fully exploit the statistical power of the LSST data. Our goal in producing this White Paper is to assist the funding agencies and reviewers in assessing proposals from individual principal investigators who wish to contribute to the investigation of dark energy through the study of scientific opportunities with the LSST, and the investigation of dark energy probes with existing astronomical data sets as they pertain to understanding and optimizing the potential of the LSST. \n## **1.2 Description of the LSST system** \nThe LSST system is designed to achieve multiple goals in four main science themes: inventorying the Solar System, mapping the Milky Way, exploring the transient optical sky, and probing dark energy and dark matter. These are just four of the many areas on which LSST will have enormous impact, but they span the space of technical challenges in the design of the system and the survey and have been used to focus the science requirements. LSST will be a large, wide-field groundbased telescope, camera and data management system designed to obtain multi-band images over a substantial fraction of the sky every few nights. The observatory will be located on Cerro Pach´on \n9 \n## _Chapter 1: LSST as a Dark Energy Experiment_ \nin northern Chile (near the Gemini South and SOAR telescopes), with first light expected around 2019. The survey will yield contiguous overlapping imaging of over half the sky in six optical bands ( _ugrizy_ , covering the wavelength range 320–1050 nm). \nThe LSST telescope uses a novel three-mirror design (modified Paul-Baker) with a very fast f/1.234 beam. The optical design has been optimized to yield a large field of view (9.6 deg[2] ), with seeinglimited image quality, across a wide wavelength band. Incident light is collected by the primary mirror, which is an annulus with an outer diameter of 8.4 m and inner diameter of 5.0 m (an effective diameter of 6.5 m), then reflected to a 3.4 m convex secondary, onto a 5 m concave tertiary, and finally into three refractive lenses in a camera. This is achieved with an innovative approach that positions the tertiary mirror inside the annular primary mirror, making it possible to fabricate the mirror pair from a single monolithic blank using borosilicate technology. The secondary is a thin meniscus mirror, fabricated from an ultra-low expansion material. All three mirrors will be actively supported to control wavefront distortions introduced by gravity and environmental stresses on the telescope. The telescope sits on a concrete pier within a carousel dome that is 30 m in diameter. The dome has been designed to reduce dome seeing (local air turbulence that can distort images) and to maintain a uniform thermal environment over the course of the night. \nThe LSST camera provides a 3.2 Gigapixel flat focal plane array, tiled by 189 4k _×_ 4k CCD science sensors with 10 _µ_ m pixels. This pixel count is a direct consequence of sampling the 9.6 deg[2] field-ofview (0.64 m diameter) with 0.2 _×_ 0.2 arcsec[2] pixels (Nyquist sampling in the best expected seeing of _∼_ 0.4 arcsec). The sensors are deep depleted high resistivity silicon back-illuminated devices with a highly segmented architecture that enables the entire array to be read in 2 seconds. The sensors are grouped into 3 _×_ 3 rafts; each contains its own dedicated front-end and back-end electronics boards. The rafts are mounted on a silicon carbide grid inside a vacuum cryostat, with an intricate thermal control system that maintains the CCDs at an operating temperature of 180 K. The entrance window to the cryostat is the third of the three refractive lenses in the camera. The other two lenses are mounted in an optics structure at the front of the camera body, which also contains a mechanical shutter, and a carousel assembly that holds five large optical filters. The sixth optical filter can replace any of the five via a procedure accomplished during daylight hours. \nThe application layer is organized around the data products being produced. The nightly pipelines are based on image subtraction, and are designed to rapidly detect interesting transient events in the image stream and send out alerts to the community within 60 seconds of completing the image readout. The data release pipelines, in contrast, are intended to produce the most completely analyzed data products of the survey, in particular those that measure very faint objects and cover \n10 \n_1.3 Planned survey strategy and delivered data products_ \nlong time scales. A new run will begin each year, processing the entire survey data set that is available to date. The data release pipelines consume most of the computing power of the data management system. The calibration products pipeline produces the wide variety of calibration data required by the other pipelines. All of these pipelines are architected to make efficient use of Linux clusters with thousands of nodes. There will be computing facilities at the base facility in La Serena, at a central archive facility, and at multiple data access centers. The data will be transported over existing high-speed optical fiber links from South America to the USA. \nThe LSST data system is being designed to enable as wide a range of science as possible. Standard data products, including calibrated images and catalogs of detected objects and their attributes, will be provided both for individual exposures and the deep incremental data coaddition. About 2 billion objects will be routinely monitored for photometric and astrometric changes, and any \n> 3Available from www.lsst.org/lsst/SciBook \n11 \n_Chapter 1: LSST as a Dark Energy Experiment_ \nAcceleration represents a serious dilemma for fundamental physics. The most prosaic explanation is that vacuum energy, which constitutes about three quarters of the energy density in the Universe, is driving the acceleration; however, naive predictions based on the Planck scale would predict vacuum energy densities roughly 100 orders of magnitude larger than observed, implying that some unknown physics suppresses such predicted vacuum energy, but leaves a very small residual that is driving the current expansion rate. Other explanations are even more radical, ranging from a new scalar field akin to the recently discovered Higgs boson but 44 orders of magnitude less massive, to additional dimensions of space, and many others – all competing models for the phenomenon referred to as dark energy. \nDistinguishing competing hypotheses for acceleration will require precise measurements of the _cosmic expansion history_ and the _growth of structure_ . The cosmic expansion history is the observational signature most closely connected to acceleration, and future observations must map the expansion history accurately out to higher redshifts. However, expansion history may not offer sufficient discriminating power between different theoretical models and will have to be combined with measurements of the growth of structure. Structure formation involves a balance between gravitational attraction of matter over-densities and the rapid expansion of the background. Thus, quantifying the rate of growth of structures from early times until the present provides additional tests of the energy contents of the Universe and their interactions. \nOne alternative explanation for cosmic acceleration is that gravity on cosmic scales is not described by Einstein’s general relativity (GR). Such alternatives to the conventional smooth dark-energy hypothesis are referred to as modified gravity (MG) scenarios. MG theories are still being developed, but some observational implications have already emerged. Since MG theories change the gravitational interaction on large scales, one generically expects the growth of structure to be altered compared to a smooth dark energy model with a similar expansion history. Moreover, wide classes of MG models predict a deviation in how massive structures bend light (observed through gravitational lensing) and accelerate other stars and galaxies. Thus, a combination of lensing and dynamical studies can test the MG hypothesis. A second approach is to work on smaller scales and exploit the transition an MG theory must make to GR on laboratory and Solar System scales (where it is well tested). \n12 \n_1.5 Precursor surveys_ \nTesting models of dark energy and determining whether Einstein’s theory needs to be modified will therefore be carried out using many different combinations of probes. For smooth dark energy models, the effect on the expansion rate and growth of structure is commonly characterized using two parameters, _w_ 0 and _wa_ , that describe the equation of state of dark energy (if dark energy is simply a cosmological constant, then _w_ 0 = _−_ 1 and _wa_ = 0). The _w_ 0 _− wa_ parametrization has the drawback of not being physically motivated and also implicitly dis-favoring models in which dark energy played a role at early times. A more general parametrization, which allows the equation of state to vary freely in a succession of redshift bins, is one way to address this drawback. As discussed below in Section 1.6, LSST is especially powerful for characterizing dark energy by a larger set of parameters that go beyond the _w_ 0 _− wa_ description. \n## **1.5 Precursor surveys** \nBy the time LSST starts taking data, a number of surveys will have mapped parts of the sky to various redshift depths, yielding important datasets and greatly enhancing the expertise of the dark energy science community. Herein, we summarize these precursor surveys, restricting ourselves to projects that have some relevance to LSST and that have secured the bulk of their funding or other project resources. \n**Precursor imaging surveys.** SDSS I-III has mapped nearly 10,000 square degrees in the Northern hemisphere to a limiting magnitude of _g ≃_ 21 with median redshift _z ≃_ 0 _._ 4; luminous red galaxies are mapped out to _z ≃_ 0 _._ 8 and quasars to higher redshifts. SDSS-Stripe 82 and CFHT Legacy Survey are significantly deeper but only cover about 200 square degrees. Pan-STARRS (PS1) and KIDS have begun their surveys and expect to reach similar depth as SDSS-Stripe 82. The Dark Energy Survey (DES) and the Hyper Suprime-Cam (HSC) on the Subaru telescope have achieved first light. DES will map 5000 square degrees in five filters to image galaxies beyond _z_ = 1 and obtain photometric redshifts. HSC aims to map an area of a couple of thousand square degrees. \n13 \n## _Chapter 1: LSST as a Dark Energy Experiment_ \nbetter than 10%. In parallel, surveys such as DES will use stacked weak lensing to internally calibrate cluster masses. Successful and well characterized mass-calibration will greatly enhance the constraining power of cluster number counts. \n**Precursor spectroscopic surveys.** The recently-completed WiggleZ survey has mapped 200 _,_ 000 galaxies below _z_ = 1 over 1000 square degrees. Between 2014 and 2017, the HETDEX survey will map some 800,000 Lyman-alpha emitters at _z ≈_ 2 _−_ 3 _._ 5 over approximately 500 square degrees. By 2014, Sloan Digital Sky Survey III will have surveyed 10 _,_ 000 square degrees, mapping galaxies out to _z_ = 0 _._ 8 and quasars at higher redshift. After Sloan 3 (AS3) by 2018 will have mapped 3100 square degrees out to greater depth; other planned projects with greater power include the PFS (Prime Focus Spectrograph) on the Subaru telescope, BigBOSS and DESpec. \nThe current measurements from SDSS and WiggleZ have already allowed a measurement of the distance at _z_ = 0 _._ 35 at 2% and at _z_ = 0 _._ 6 at 4% respectively. SDSS III will improve on these by more than a factor of 2. It will also measure BAO at _z_ = 2 _._ 5 using the Lyman alpha forest, as will HETDEX using Lyman alpha emitters. AS3 and other upcoming surveys will make equally precise measurements at higher redshifts. \nThe combination of lensing measurements from imaging surveys and dynamics from spectroscopic surveys is a powerful probe of modified gravity theories. This test of gravity has been carried out using SDSS data (Reyes et al. 2010) and will be performed at higher precision in the coming years. Spectroscopic surveys also provide us with better cluster redshift determination, which is crucial in cluster counts cosmology (since we use _dN/dz_ as the cosmological probe). The spectroscopic surveys can also provide cluster velocity dispersions to help calibrate cluster mass more accurately. \n**Precursor time domain surveys.** For dark energy cosmology, Type Ia SN are the primary probe that requires time domain information. Having led to the discovery of dark energy in 1998, SN cosmology has advanced significantly in the last decade. Current measurements constrain the dark energy equation of state parameter (if taken to be constant) to better than 10%. These constraints are obtained with a combination of ground based data at low redshift and HST at _z>_ 1. Upcoming surveys that will impact SN cosmology cover a wide range in redshift, and aim to tackle systematics such as correlation of SN brightness with host galaxy. These include: SDSS, SN Factory, QUEST, PS1, DES and HST based observations. Redshifts of SN for projects such as DES will be obtained through follow up spectroscopy of host galaxies. \nPrecursor Stage III and Stage IV spectroscopic surveys will provide training sets to calibrate LSST’s photometric redshifts and mitigate other systematics. Working out the detailed requirements for spectroscopic galaxy samples is an important goal for the near future. It is especially challenging to obtain adequate spectroscopic samples for the faintest galaxies imaged by LSST. A coordinated effort will be needed to make advances in both the techniques for calibrating photometric redshifts and in obtaining the needed spectroscopic samples. \n## **1.6 Gains from LSST** \nHistorically, our understanding of the cosmic frontier has progressed in step with the size of our astronomical surveys, and in this respect, LSST promises to be a major advance: its survey coverage will be approximately ten times greater than that of the Stage III Dark Energy Survey. \n15 \n## _Chapter 1: LSST as a Dark Energy Experiment_ \nFuture surveys will go beyond the simple _w_ 0 _− wa_ parameterization of the dark-energy equation of state; these surveys can determine _w_ ( _z_ ) in bins of redshift, effectively measuring _modes_ – linear combinations of _w_ ( _z_ ) over the redshift range of interest – with some degree of accuracy. Viewed in this way, the dark energy constraining power of LSST could be several orders of magnitude greater than that of a Stage III survey (Albrecht & Bernstein 2007). \nThere is also the tantalizing possibility, discussed earlier, that Einstein was wrong – that general relativity (GR) is not the true theory of gravity on the large scales probed by cosmology. The true observational test of all modified-gravity (MG) models will be the growth of structure, since MG models generically predict differences in the two scalar potentials (which are equal to one another at late times in general relativity) and modifications to the Poisson equation that relates the gravitational potential to over-densities. These modifications are generally functions of both space and time. Again, no survey will probe these deviations at all redshifts and at all scales, but LSST will go much further than any of its predecessors in its ability to measure growth in tens or even hundreds of spatial-temporal bins and constrain dozens of modes. Disagreement of even one of these modes with the GR prediction would signal inconsistency with Einstein’s theory of gravity. Of course, it is essential to address potential systematics in a wide variety of probes to ensure that any observed disagreement is robust. \nWhile these projections for LSST statistical significance are compelling, they probably do not capture the true nature of the revolution that LSST will enable. The sheer statistical power of the LSST dataset will allow for an all-out attack on systematics, using a combination of null tests and hundreds of nuisance parameters. For example, multiple scans of the same region of sky can be combined to create modes that are sensitive only to atmospheric contamination of the ellipticities. As another example, multiplicative error in measurements of the ellipticity, instead of being ignored, will be detected and studied in as much detail as the signal from cosmic shear. Systematics will be mitigated by combining probes. In the case of BAO and WL, a joint analysis of the shear and galaxy over-densities for the same set of galaxies involves galaxy–galaxy, galaxy–shear, and shear– shear correlations, which enable some calibration of systematics that would otherwise adversely impact each probe (Zhan 2006a). \nBeyond tests of systematics, there is a growing sense in the community that the old, neatly separated categories of dark energy probes will not be appropriate for next generation surveys. Instead of obtaining constraints on dark energy from cluster counts and cosmic shear separately, LSST scientists may use clusters and galaxy-galaxy lensing simultaneously to mitigate the twin systematics of photometric redshift error and mass calibration (Oguri & Takada 2011). Magnification (e.g., Jain 2002) may emerge as just as powerful a tool to measure mass density as galaxy ellipticities; for this, a joint analysis of the density and ellipticity fields is crucial. A homogeneous and carefully calibrated dataset such as LSST’s will be essential for such joint analyses. \nTo get things started, a number of quasi-independent scientific collaborations were convened by the Project beginning in 2008. Their role was to provide advice on technical issues as they relate to specific scientific investigations, and to help articulate the scientific case for LSST as the project made its way through the approval process with the federal funding agencies. They were largely responsible for authoring the LSST Science Book (Abell et al. 2009), which was released in 2009. At present, there are eleven separate scientific collaborations spanning a broad range of astronomical and cosmological topics. Five of the eleven (Weak Lensing, Supernovae, Large-Scale Structure/Baryon Oscillations, Strong Lensing, and Informatics and Statistics) have clear relevance to the study of dark energy. \nWhile these collaborations have been productive, and have provided invaluable service to the Project over the past few years, they have not been formally organized to pursue the full range of investigations required to guarantee that the most sensitive constraints on the nature of dark energy can and will be derived from the LSST data. As we emphasize below, the various dark energy analyses that we expect to perform are sensitive to a large number of potential systematic uncertainties that must be identified, quantified, and minimized in order for the dark energy investigations to achieve their full potential. In addition, new algorithms must be developed and tested to enable those analyses, and an efficient computational and software framework must be established to perform the necessary calculations. All of these activities require an extensive, coordinated research effort well in advance of the onset of data taking. \nTo address that need, we have created the LSST Dark Energy Science Collaboration (DESC). The goal of the DESC is to develop a high-level plan for the study of dark energy with LSST data. This will include the development and optimization of a complete set of dark energy analyses that \n17 \n_Chapter 2: The LSST Dark Energy Science Collaboration_ \nwill be performed with the data, the detailed study of systematic issues that may compromise those analyses, the clarification of the sensitivity of those analyses to various technical aspects of the LSST system design, the generation and refinement of simulations and other tools required to validate the analyses, and the identification and assembly of the computational resources to support such investigations both before and after the onset of data taking. \nAn initial “kick-off” meeting to establish the DESC was held at the University of Pennsylvania in mid-June 2012. Since that meeting, over 175 scientists, coming from 49 distinct institutions, have joined the Collaboration. Their collective expertise covers a wide array of issues relevant to the study of dark energy, ranging from fundamental theoretical cosmology to the detailed characteristics of the LSST hardware and software systems. Our membership includes not only key scientific leaders of the LSST Project Team, but also key personnel from all of the relevant precursor surveys that will collect data in advance of LSST, and several of the complementary surveys that will be operating in a similar timeframe. \nA large fraction of the members of the five existing collaborations with relevance to dark energy have joined the DESC. However, we expect that these other collaborations will continue to operate, investigating, for example, issues that are not related to dark energy. Maintaining significant overlap in membership between the DESC and these various collaborations will help to ensure that our efforts are coordinated and not redundant. \nThe DESC will have both informal and formal connections to the LSST Project. Since many of our members are working directly on the Project, we have direct access to technical information regarding the detailed performance characteristics of the various subsystems. However, if it becomes necessary for the DESC to communicate specific concerns about the LSST system design that might compromise its effectiveness for dark energy investigations, we will do so officially through the formal channels that the Project has established for interactions with the community. \n## **2.2 Governance and organization** \nWith the governance structure of a variety of existing collaborations as a guide, a governance model for the LSST Dark Energy Science Collaboration (LSST DESC) was discussed at the June 2012 meeting in Philadelphia. A three-year plan was presented and adopted, beginning with a one-year initial phase in which the LSST DESC is established with a minimal governance model to get the Collaboration off the ground, produce this White Paper, and begin doing science. The positions needed for the management team in this minimal model were filled by individuals who were ready and able to carry out the tasks for the first year. \nThis document describes the first-year minimal governance model, which consists of a management team and an Executive Board. The management team consists of the Spokesperson (or co-Spokespersons), Deputy Spokesperson, Analysis Coordinator, Computing and Simulation Coordinator, and Technical Coordinator. The Executive Board advises the Spokesperson on all scientific, financial, and organizational matters pertaining to the Collaboration. \nA Governance Working Group has been appointed to continue the deliberations on governance and then propose a longer term, more complete governance model for the LSST DESC that builds on the one-year initial phase. In particular, a selection process will be proposed for identifying \n18 \n_2.2 Governance_ \n**==> picture [469 x 352] intentionally omitted <==**\n**----- Start of picture text -----**<br>\nExecutive Board<br>Management Team<br>Co-spokespersons<br>Bhuvnesh Jain, Steve Kahn<br>Deputy Spokesperson<br>Gregory Dubois-Felsmann<br>Analysis Coordinator Computing and Simulation Technical Coordinator<br> Coordinator<br>Jeffrey Newman Andrew Connolly Chris Stubbs<br>Voting Members of<br>Executive Board LSST Project<br>Patricia Burchat Chief Scientist<br>Craig Hogan<br>Reynald Pain Tony Tyson<br>Michael Strauss<br>(One more TBD)<br>**----- End of picture text -----**<br>\nFigure 2.1: LSST Dark Energy Science Collaboration organizational chart with interim leadership team. The Executive Board consists of five voting members, the management team, and the LSST Project Chief Scientist. \nfuture members of the LSST DESC management team. A Membership Committee will also be established to work on more detailed criteria for both individual and institutional membership and the procedures for admitting new members. We anticipate that a Collaboration Council will be formed to represent the full membership of the collaboration. \nThe initial organizational structure is shown in Figure 2.1; the names correspond to those individuals who will be in their respective roles for the first year. After that, members of the leadership team will be selected for a term of two-years through a process and with roles that will be described in the refined governance model to be developed by the Governance Working Group and ratified by the Collaboration Council (see Section 2.4.3). It is possible that some terms may initially be shorter or longer than two years so that terms for some positions are appropriately staggered. \nThe work of the LSST DESC will benefit greatly from interactions with collaborations carrying out precursor surveys. We have appointed four people as liaisons with projects outside the LSST: Josh Frieman, Jason Rhodes, Natalie Roe, and Michael Strauss. \n19 \n_Chapter 2: The LSST Dark Energy Science Collaboration_ \n## **2.2.1 The management team** \nThe details of the management structure, including the specific roles and responsibilities of individual managers, and the roles and responsibilities of additional Working Groups that may be defined by the management team, will be established in a Management Plan, proposed by the Spokesperson and ratified by the Executive Board. Here, we outline the general roles of the members of the management team. \n1. **Spokesperson:** The DESC will be led by a Spokesperson or Co-Spokespersons, who have overall responsibility for all scientific, technical, organizational, and financial aspects of the Collaboration. They are the contact persons for the DOE Office of High Energy Physics and other funding agencies. The (Co)Spokesperson is the chair of the Executive Board. \n2. **Deputy Spokesperson:** The Deputy Spokesperson will be responsible for overseeing many of the ongoing operations of the Collaboration. \n3. **Analysis Coordinator:** For the first three years, the Analysis Coordinator is responsible for managing and achieving the list of analysis tasks identified in this White Paper as high priority, both in specific science areas and in cross-cutting areas. \n4. **Computing and Simulation Coordinator:** The Computing and Simulation Coordinator is responsible for planning for and coordinating computational resources, and the development of associated software and simulations necessary to carry out the work of the DESC. \n5. **Technical Coordinator:** The role of the Technical Coordinator is to coordinate the interactions between those with technical knowledge of the components of the LSST system and those on the analysis teams who are improving the understanding of the impacts that particular technical issues might have on the science analyses. \n## **2.2.2 LSST Project Chief Scientist** \n## **2.2.3 The Executive Board** \nThe Executive Board will consist of five voting members and non-voting _ex officio_ members who represent the management team. The five voting members will be chosen to bring broad knowledge of the community, the agencies, and the science goals to the collaboration. These members of the Executive Board work with the management team to help define and ratify policy issues as they arise; they do not directly manage the technical and scientific work of the collaboration. The (Co)Spokesperson, Deputy Spokesperson, Analysis Coordinator, Computing and Simulation Coordinator, Technical Coordinator, and LSST Project Chief Scientist are the non-voting _ex officio_ members of the DESC Executive Board. The Spokesperson will chair the Executive Board. \n20 \n_2.3 Working Groups_ \n## **2.3 Working Groups** \nThe areas spanned by the Analysis, Computing and Simulation, and Technical Coordinators are each quite broad. Therefore, the Management Plan may include the definition of a working group structure for efficiently carrying out tasks. The initial Management Plan defines a layer of Working Groups under each Coordinator, and an initial set of conveners. \n- Analysis Working Groups \n   1. Weak Lensing — Michael Jarvis, Rachel Mandelbaum \n   2. Large Scale Structure — Eric Gawiser, Shirley Ho \n   3. Supernovae — Alex Kim, Michael Wood-Vasey \n   4. Clusters — Steve Allen, Ian Dell’Antonio \n   5. Strong Lensing — Phil Marshall \n   6. Combined Probes, Theory — Rachel Bean, Hu Zhan \n   7. Photo- _z_ Calibration — Jeff Newman (acting) \n   8. Analysis-Computing Liaison — Rick Kessler \n- Computing and Simulation Working Groups \n   1. Cosmological Simulations — Katrin Heitmann \n   2. Photon Simulator — John Peterson \n   3. Computing Infrastructure — Richard Dubois \n   4. Software — Scott Dodelson \n- Technical Working Groups \n   1. System Throughput — Andrew Rasmussen \n   2. Image Processing Algorithms — Robert Lupton \n   3. Image Quality — Chuck Claver \n   4. Science Operations and Calibration — Zeljko Ivezic \nThe evolution of these Working Groups and the process for selecting new Working Group conveners will be defined by the management team through the Management Plan. \nWe expect that there will be significant overlap between members of Working Groups under each Coordinator as DESC collaborators contribute to tools, the project, and science. For example, the areas covered by the Computing and Simulation Working Groups are essential to the work of the Analysis Working Groups. The areas covered by the Technical Working Groups parallel the LSST subsystems and also interface in essential ways with the Analysis Working Groups. \n21 \n_Chapter 2: The LSST Dark Energy Science Collaboration_ \n## **2.4 Development and execution of LSST DESC policies and procedures** \n## **2.4.1 Governance Working Group** \nA Governance Working Group has been established and charged with conducting deliberations on and making recommendations for the long-term governance, by-laws, and policies of the LSST DESC. After being ratified by the Collaboration Council (see Section 2.4.3 below), these will go into effect after the one-year initial phase. \n## **2.4.2 Membership Committee** \nA Membership Committee will be established in the initial phase to oversee the membership eligibility requirements for the LSST DESC, described below (see Section 2.5), and to make recommendations for adjustments to the membership process as suggested by experience. \n## **2.4.3 The Collaboration Council** \nWe anticipate that a Collaboration Council will be formed after the initial phase. Representing the full membership of the Collaboration, the Collaboration Council will have overall responsibility for LSST DESC policies and procedures, including \n1. Ratifying the LSST DESC organizational structure, by-laws, and policies proposed by the Governance Working Group. \n2. Approving proposed modifications or additions to the by-laws and policies. Such modifications or additions may be proposed by the Spokesperson or a Collaboration Council member. \n3. Appointing the Committee that proposes the candidate or candidates for Spokesperson, either through nomination or election, depending on the recommendation of the Governance Working Group. \n4. Reviewing and approving policies on membership and appointing a Membership Committee to consider applications for membership. \n5. Developing a publication policy that is consistent with the LSST Publication Policy and appointing a Publication Board to execute that policy. \n## **2.5 Membership** \nThe DESC will have two categories of membership for scientific collaborators: full and associate. \n22 \n_2.5 Membership_ \n## **2.5.1 Associate membership** \nAssociate membership is envisioned as providing a path toward full membership; applicants will be associate members while they develop their detailed application for full membership. \n## _**Becoming an associate member**_ \nAll those who participated in the creation of the DESC in 2012 and signed the membership form are regarded as associate members. The signup process defined in the original DESC outreach is still available at this time,[1] and a similar online signup process for associate membership will be maintained in the future. \nThe applicant will be asked to write a brief proposal (a few paragraphs) for associate membership, which will be considered by the DESC Membership Committee. An individual wishing to join the DESC should use the online process or may contact the Spokesperson(s) or the Chair of the Membership Committee at any time. \n## _**Criterion**_ \nApplicants who identify work they wish to pursue that is relevant to the DESC will be granted associate membership. \n## _**Rights of associate membership**_ \nAssociate membership grants the scientist access to DESC communication tools and documents. Associate membership does not bring with it access to LSST Corporation (LSSTC) communication tools, documentation, or data products. \nThe associate membership period allows scientists to learn enough detail about the activities, needs and coverage of tasks in the DESC to write a proposal for full membership. \nInitially, no time limit for associate membership will be imposed. Beginning one year before the expected start of LSST commissioning, all persons who have been associate members for at least one year will be expected to submit a proposal for full membership or become inactive in the DESC. There will be no minimum time required in the associate member state before submitting an application for full membership. \n> 1Applications for associate membership may be submitted via Web form at `http://www.slac.stanford.edu/exp/ lsst/desc/signup.html` . \n23 \n_Chapter 2: The LSST Dark Energy Science Collaboration_ \n## **2.5.2 Full membership** \nThe transition from associate to full membership is based on a written proposal (a few pages) describing the specific contributions the scientist is proposing to make; these could be tasks outlined in the DESC White Paper, or tasks that are argued to be important for achieving the science goals of the DESC. \n## _**Criteria**_ \nThe criteria for joining as a full member are: (i) the proposed level of commitment, generally expected to be at least 30% of research time over a few years; (ii) the importance of the proposed task(s); and (iii) the need for more effort on the proposed tasks within the DESC. \n## _**Process**_ \nAfter a transitional period in which the initial setup of the collaboration is completed, the process for new applications will be as follows. \nFaculty and full-time staff scientists must apply for full membership and may describe the proposed contributions of students and postdocs in their groups. Additional graduate students and postdocs may subsequently become full members at the request of an existing full member of faculty or senior staff rank at the same institution, as long as the request specifies an commitment as to the tasks to be performed by the new member, consistent with the above criteria. \nProposals may be submitted at any time, and are considered on a rolling basis throughout the year. \nThe proposal is considered by the Membership Committee. The Membership Committee makes a recommendation to the Collaboration Council, which may permit it to take effect without comment, or may, upon request of Council members or the applicant, vote on accepting or denying the application. If the application is denied, the Membership Committee will normally give feedback to the applicant and encourage a resubmission. As a general rule we plan to be able to respond to a membership proposal within two months of its submission. \n**Transitional process** During the one-year initial phase, the 12 members of the Executive Board (i.e., the Management Team plus the five voting members and the LSST Project Chief Scientist) plus the conveners of the 16 Working Groups will become temporary full members of the DESC. The Executive Board will constitute the initial Membership Committee and may seek advice on membership from the leaders of the Working Groups. \nAll associate members who responded to the initial call for participation in the DESC or subsequently joined will be invited to submit applications for full membership. After these are processed, a Collaboration Council will be formed and a Membership Committee will be appointed by the Council. The temporary full members (i.e., the leadership team) will then submit their own proposals for full membership to be considered by the Membership Committee. \n24 \n_2.5 Membership_ \n## _**Rights of full membership**_ \nFull members retain the rights of associate members. In addition, it is intended that they attain the privileges granted to members of LSST Science Collaborations, including access to LSSTC communication tools and documentation, and internal LSST project data such as simulated datasets and the output of Data Management data challenges. \nThe membership policies of the DESC will be submitted to the LSSTC Board to enable its ratification of this relationship. The DESC will regularly inform the LSST Project of the list of its full members. \nFull membership is also a prerequisite for becoming an author of publications in the name of the DESC. \n## **2.5.3 Authorship** \nA full publications policy, including criteria for authorship, will be developed by the Collaboration Council once it has been formed. The guidelines described herein are indicative of how the full policy may be expected to develop. \n## _**Criteria**_ \nThe transition from full membership to authorship status is based on contributions to date to the DESC, demonstrated engagement in the DESC, and the level of current and future commitment, generally expected to be at least 50% of research time over a few years. \nAll full members of at least two years’ standing will be eligible to apply for authorship status, beginning one year before the start of the LSST commissioning period. \n## _**Process**_ \nThe application for authorship will consist of a written proposal describing contributions already made and proposed future contributions to the DESC effort. \n## **2.5.4 Additional topics** \n## _**Members of the Project Team**_ \nThe LSST Publication Policy specifies that “Builders” (engineers, scientists, and other individuals whose contributions have been vital to the development of the LSST infrastructure) will be invited to add their name to the list of contributing authors for key science papers. \nIn addition, members of the LSST project are encouraged to join the DESC. Any member of the project team who wishes to become a member of the DESC should apply for associate membership for access to the DESC communication tools, etc. For further engagement, they should apply \n25 \n## _Chapter 2: The LSST Dark Energy Science Collaboration_ \nfor full membership, and in their proposal may count their contributions to the LSST project in their time commitment to the DESC. Project members will still be asked to make a specific commitment to the life of the DESC, expressed by activities such as attending meetings, assisting with the development of white papers or other reports, and joining one or more of the DESC working groups. \n## _**International affiliates**_ \nMembership (associate or full) for individuals at institutions outside the US and Chile (except for IN2P3 in France) cannot be granted until the international affiliate institution establishes its access to LSST data rights, by MOU with the project. Once a named set of individuals at a foreign institution is awarded data rights, they are welcome to apply for associate membership with the DESC, and we expect that those applications will be accepted. Transition to full membership will require engagement and an application for full membership, even for individuals from international affiliates that have established their data rights. \n## _**Support staff membership**_ \nThere will be a category of membership for nonscientific support staff that provides them access to the collaboration databases and communication tools. \n## **2.6 General structure of work plan for next three years** \nAs indicated above, key elements of the DESC work plan will involve the investigation of systematic issues that may compromise dark energy analyses, the development and implementation of algorithms in an appropriate software infrastructure to enable those analyses, the development and optimization of large-scale simulation tools necessary for testing such algorithms, and the development of a computing environment capable of supporting all of these calculations. \nThis is a large and complex effort, and we have found it necessary to prioritize our activities, especially in the early years, when many of our collaboration members are still ramping up their involvement. Our prioritization is driven principally by considerations of the time urgency for various tasks to be completed. Time urgency arises from the following considerations: \n1. While the LSST system design is fairly mature, some elements are undergoing refinement, and there is still opportunity for input based on the implications for dark energy research. However, since our primary concerns involve subtle systematics issues, these implications are not easily addressed. In general, they require full end-to-end simulations to evaluate the scientific impacts. It is very important that this work be performed early, while there is still time to provide feedback on the design. A coordinated program to carry out the required simulations, focussing on targeted design issues, is required. \n26 \n_2.6 Overview of work plan_ \n2. For many analyses, the development and testing of an appropriate set of algorithms and associated software tools will involve a multi-year effort with many steps that must be performed serially along the way. The initial steps in these chains must occur early in order to guarantee completion prior to the onset of data taking. \n3. LSST will deliver statistical errors that are lower than all precursor surveys by a large factor. This means that systematic errors and subtle physical effects that have not been studied in any detail by the community become relevant for LSST. There is a need for a research effort to scope out and quantify a variety of effects that may be relevant for LSST. Follow up work can then be carried out in subsequent years for the effects that need to be mitigated or studied in more detail. \n4. It is important, early in our program, to develop an informed sense of the full suite of dark energy analyses that we will pursue. This is crucial for adequately scoping the overall effort and for ensuring that we are budgeting our resources appropriately. The field of dark energy research has been evolving rapidly over the past few years, and a plethora of interesting new dark energy probes have been suggested. We need to carry out basic feasibility studies to clarify the practicality of some of these new probes, and their relative power for constraining dark energy and modified-gravity parameters. A mix of theoretical efforts and quick simulations are required to make such assessments. \nOther investigations are not only less urgent, but will benefit from the additional experience that the dark energy community will gain from working on precursor projects, several of which are only beginning to take data now. In general, we have categorized the explicit tasks we have defined as either “high priority” (designated H) or “longer term” (designated LT) to distinguish between these two classes. While we expect most of our effort over the first three years to be devoted to the H-tasks, some investment in LT-tasks is also important for balance. \nHowever, the observed statistics include not only cosmological information; there are also “nuisance” effects. These include observational systematics such as multiplicative and additive errors in the shear, and photo- _z_ errors; and theoretical uncertainties such as nonlinear galaxy bias (the relation between observable galaxy clustering and invisible dark matter clustering), intrinsic alignments of galaxy shapes with the density field, and baryonic effects on dark matter halo profiles. The process of handling these nuisance parameters includes: \n- Identifying models to describe them. For example, these could be models based on perturbation theory for the nonlinear galaxy bias, or atmosphere and/or optics models to describe how correlations in PSF anisotropies scale with angular separation; or, for some nuisance effects, we may adopt a flexible but non-physically motivated model with a significant number of free parameters. \n- Identifying the range of scales for which we believe our combined cosmological plus systematics model is valid. \n32 \n_3.1 Weak lensing_ \n- Incorporating the nuisance effects into the emulator to calculate how they affect the measured statistics, as a function of the values of the nuisance parameters. The procedure for doing so will depend on the nature of the nuisance effect. \nWe can compare these predictions with the data (given the covariances) to constrain cosmological parameters while marginalizing over systematic errors. Given the cosmological parameters and a flexible model for systematic errors, the parameter space could easily have hundreds of dimensions. Parameter constraints in such situations are often done using Markov Chain Monte Carlo (MCMC) to handle the many-dimensional likelihood surface, though this is by no means the only possible way of doing so. \n## **3.1.3 Systematics** \n## _**Major systematics and/or ones of unknown magnitude associated with tasks**_ \nFor the next few years we plan to focus on systematic uncertainties that satisfy at least one of the following conditions: they are of sufficient importance that we need strategies to ameliorate them, but Stage III surveys will not address them at the level required by LSST (or at all); the systematic is of totally unknown magnitude but could conceivably cause significant issues for LSST; or the systematic could motivate a change in LSST survey strategy (either hardware, observing strategy, or data management). \n**PSF centroid bias** We are concerned about several potential systematic errors in determinating the PSF. Since the galaxy images are effectively deconvolved by the PSF to determine their shapes, systematic errors in the PSF translate into systematic errors in the shear (or size). One error that likely will be important for LSST, but which surveys to date have not needed to address, is the centroid of the PSF. Typically, when measuring the shapes of stars, one centroids the stellar image before measuring the PSF shape. This is normally adequate, since we do not care about any translation of the galaxy positions by the PSF. However, LSST will be combining many exposures to measure the galaxy shapes, so the translation component of the PSF will cause each galaxy image to be shifted slightly, leading to an effective additional blurring (convolution). If this is not accounted for, then the shape estimate will be systematically rounder than the true shape, leading to a bias. The solution is to allow the PSF to include a small centroid shift as part of the convolution, but this requires a substantially different algorithm for PSF estimation. One would have to fix the centroid of each star either using an astrometric catalog for the true position, or just using one single fitted position for all observations of that star. However, we are uncertain whether this effect causes a large enough shear bias that such a change in PSF estimation algorithm is required. Since we need to inform LSST Data Management whether the more complicated algorithm is required, estimating the magnitude of this effect is one of our short-term priorities. \n**Small-scale PSF variation and interpolation** The variation of the PSF on scales greater than about one arc-minute is generally well constrained by the stars observed in the image. However, both the telescope optics and atmospheric turbulence include non-negligible smaller-scale PSF variation. The atmospheric contribution is particularly difficult to estimate, since the effect \n33 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nis random, so existing PSF estimation algorithms provide no information about the pattern at locations between the observed stars. Optimal estimators, as well as statistical prior knowledge of atmospheric properties, may be able to help in this reconstruction. It is also possible that additional information, obtained from the LSST wavefront and guidance sensors, may help constrain the PSF pattern on scale smaller than the mean stellar separation. \n**Chromatic PSF effects** The PSF is wavelength dependent, which means that PSF models constructed from stars are strictly speaking correct only for objects with the same SED, and not for arbitrary galaxies. The problem is further complicated when the galaxies have multiple components with different SEDs, such as bulge-disk differences, star-bursting regions, large HII regions, etc. These chromatic effects may introduce systematic errors in shear measurements. \n**Star/galaxy separation** Weak lensing measurements depend critically on the accurate measurement of the shapes of galaxies. As described above, part of this process is PSF deconvolution, which has its own large set of associated systematic issues. In order to deconvolve the PSF, however, the PSF must be reconstructed using a very pure sample of stars, since interloping galaxies in the sample do not have the same shape as the PSF. The star selection efficiency is also important. The number of stars required to reconstruct the PSF to the level required for LSST WL systematics will depend on the algorithm used to model the PSF (see Paulin-Henriksson et al. (2009) for an approach to this based on the complexity of the chosen PSF model). We must evaluate the star sample purity and efficiency that will be required for optimal performance of the WL algorithms. \nGalaxy selection is also important here – if the galaxy sample is contaminated by misidentified stars then this will have an impact on the cosmological information we can extract from the data. The selection efficiency for galaxies is less important, as long as the effective number of galaxies remains high enough to support the planned WL measurements to the required precision. \n**Photometric calibration** Cosmic magnification is measured by computing the cross-correlation between the density variations of foreground (lens) and distant background (source) galaxies, selected to be physically unrelated. There exist a number of systematic/observing errors that can \n34 \n_3.1 Weak lensing_ \nmimic this signal. An important example is overlap of the redshift distributions of the lens and source populations, leading to a much larger cross-correlation than expected. This overlap can arise due to inaccurate photometric redshifts or catastrophic outliers. Other potential sources of systematic biases include seeing variations, stellar contamination and dust extinction. The cosmic signal, however, does present specific dependence on both the redshift of the two populations and on the source galaxy counts slope that we can use to help separate out the impact of these systematic biases. This remains an open area of study. Important areas where magnification drives analysis requirements are photometric calibration (in particular spatial variations of residuals), photometric redshift determination, and star-galaxy separation. \n- Noise rectification biases: because shear estimation is a nonlinear process, pixel noise causes not just scatter but also shear biases, which must be controlled (Melchior & Viola 2012). \n- Charge-transfer inefficiency (CTI): space-based surveys have led the way in mitigating the effects of CTI. Ground-based surveys are less affected because the higher background usually fills the traps in the sensor. This is an area of low risk for LSST. \n## _**Systematics without tasks in Chapter 5**_ \nBelow we briefly discuss some weak lensing systematic errors that are not explicitly associated with tasks in Chapter 5. \n**Object detection** The detection of galaxies for shape estimation is subject to a number of potential systematics that can bias weak lensing measurements. One bias arises from a correlation between the shape of the PSF and the shape of the galaxy. A galaxy whose ellipticity is aligned with the ellipticity of the PSF will be detected at a higher S/N than an otherwise identical galaxy whose major axis is 90 degrees misaligned with that of the PSF (Kaiser 2000; Bernstein & Jarvis 2002). This “PSF selection bias” can be mitigated by defining a shape-independent galaxy selection criterion. \nMasks around field boundaries, bright stars, and image artifacts can also introduce biases by rejecting elliptical objects that overlap the mask boundaries. For example, a bad column or stellar saturation bloom will intersect more galaxies whose major axes lie parallel to rows in the image than it does galaxies whose major axes are aligned with columns (c.f. Huff et al. 2011). The large number of exposures of each object in LSST will help to mitigate many biases from image artifacts. \n## **3.2.3 Systematics** \nHere we describe the major challenges to obtaining dark energy measurements that the LSS working group must address by 2020, emphasizing those in need of immediate attention. \n## _**Major astronomical systematics and/or those of unknown magnitude**_ \n**Dither pattern.** Dithering refers to utilizing a series of slightly offset telescope pointings to fill in gaps between CCD sensors to create a complete image of nearly uniform depth. Large dithers of half the 3.5 _[◦]_ field of view could also be used to make the survey coverage more uniform as the sky is tiled with various pointings. Because the LSST camera has an instrument rotator, rotational dithers provide an additional mechanism for varying systematics caused by anisotropic PSF, filter nonuniformity, and detector asymmetry. A fundamental question being explored for LSST is whether to utilize all of these dithers to achieve a nearly uniform exposure map upon completion of the 10-year survey, or whether this is outweighed by the advantages for transient searches of nearly fixed pointings. Dithering is a significant concern for LSS, as any spurious structure that is introduced on scales on the order of the LSST field of view could alter the BAO feature, which occurs on similar angular scales. \n**Atmospheric conditions: throughput, sky brightness, and seeing.** The LSST observing cadence will visit 9.6 square-degree patches of the sky in a dithered way on different days in different wavelength bands and at different zenith angles under different observing conditions. The resulting raw overlapping images will have the discontinuous effects of variations in sky brightness, temporal and spatially varying glints and ghosts from bright stars, total throughput variations due mostly to clouds, and variations in the PSF and its angular correlations. The LSST pipeline, automated data quality assessment (ADQA) monitoring, and calibration protocol will correct for all these effects down to some residual. The largest surviving effect will be the patchy _variances_ in PSF and limiting surface brightness. We need to study the impact on LSS science of the power spectrum of these residuals and their covariances. For example, patchy limiting magnitude and color errors will affect photo- _z_ and WL magnification in patches correlated on the sky. \n**Stellar contamination and obscuration.** LSST is a photometric survey, and thus we depend strongly on the color information of each object for its identification. The color of stars can mimic the color of other extragalactic objects. Since the distribution of stars has significant angular correlations on much larger angular scales than galaxies, when stars contaminate our extragalactic \n41 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nsamples, we will observe an excess in the large scale. On the other hand, imperfect background estimation around bright stars increases the threshold for detection of nearby objects. This will induce an anticorrelation between stars and galaxies, and hence another source of anomalous largescale power of extragalactic objects. We need to study, estimate, and reduce the impact on LSS science of the power spectrum of these effects. \n**Photometric calibration.** The need for photometric calibration in LSS is driven by the precision in large-scale power required for the survey. Thus, we need to study, estimate, and reduce the impact of photometric offsets in LSST by utilizing large scale simulations that include templates for photometric offsets. The templates can be created using the small scale simulations in ImSim to investigate the effects of varying conditions on the throughput of each of the photometric bands in combination with using OpSim to understand the large scale variations across the sky even after the calibration. \nThis is a high-priority activity because the LSST Project now seeks feedback on the calibration strategy for their planning. Calibration is of importance as it is the limiting source of uncertainty in current experiments. While previous experiments have examined zeropoint uncertainties, none have incorporated in their cosmology analysis the calibrated transmission functions and their covariances. We therefore need to develop algorithms and software that correctly propagate these uncertainties. \n## _**LSST Hubble diagram uncertainties**_ \nLSST is confronted by a novel source of uncertainty in the Hubble diagram covariance matrix. Construction of a SN Ia Hubble diagram requires 1) the classification of objects that enter the analysis; 2) their redshifts; 3) their distance moduli. Traditional analyses use spectroscopic data for the typing and redshifts, while new work shows that the determination of distance moduli can depend on UV through NIR data as well as spectral features; LSST provides only optical photometry from which the three Hubble-diagram ingredients are inferred, not the full variety of data that can be used to constrain the SN Ia model. Inferred absolute magnitudes will be subject to large statistical uncertainties due to lacking data, or systematic biases if poorly constrained SN parameters are marginalized in an effort to reduce those statistical uncertainties. \n48 \n_3.3 Supernovae_ \nThis is a high-priority activity because this is a “new” source of uncertainty that has yet to be quantified. We currently have no reliable estimate of the science reach of an LSST-only (i.e., photometry-only) SN program. The Dark Energy Survey faces the same problem and will be developing approaches to minimize and quantify these uncertainties. The DESC needs to monitor their progress and incorporate their gained knowledge in our analysis. \n## _**Survey optimization**_ \nThe final number of supernovae identified in the LSST data, and their redshift accuracy and photometric signal-to-noise ratio, will depend on the strategy for both main and deep-drilling surveys. The SN program’s projected figure of merit is sensitive to choice of exposure times, cadence, and solid angle. Trade-offs between quantity and quality of light curves will have implications for both statistical and systematic uncertainties in dark energy parameters; these have yet to be fully explored. The DESC must have the capability to calculate the figure of merit over the parameter space of survey possibilities. \nTo highlight the importance of this activity, the general current consensus among members of the LSST SN Science Collaboration is that the current plan for most main survey fields (in terms of cadence and filter in repeat observations) will not be sufficient for SN Ia light curves of high cosmological utility, and that only the deep drilling fields will provide useful data. \nThis is a high-priority activity because we suspect that the baseline main survey is not useful for supernova cosmology. We must confirm these findings and provide viable alternative strategies to the Project. DESC members already have software to simulate LSST light curves. Improvements in the configurability of the Project-provided OpSim are necessary to enable these studies. \n## _**Value-added external data**_ \nNon-LSST observations can reduce the statistical uncertainty in absolute magnitude of a single object, while such observations of an unbiased subset of objects can calibrate the absolute magnitude bias when only LSST data are used. Examples of supplemental data include supernova typing and subtyping spectra, UV and NIR supernova photometry, and host-galaxy imaging and spectra. Euclid NIR light curves and BigBOSS or DESpec spectroscopy to get host redshifts are already known to have excellent potential to expand the science reach of LSST SNe. \nThe DESC must identify planned resources (or develop new ones) that can leverage LSST data, and quantify the resulting benefit. The same tool used to determine the LSST error budget must allow inclusion of new data and be used to re-optimize the LSST survey and the use of external resources. This is a high-priority activity because we anticipate that complementary non-LSST data can leverage the power of LSST-discovered supernovae. Planning for use of other resources (such as Euclid) requires advance coordination. \n49 \n_Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Refining the distance indicator**_ \nThe full range of SN Ia heterogeneity that is correlated with absolute magnitude is not captured in current models. Biases in inferred absolute magnitudes are manifest in the identification of peculiar SNe Ia, and in correlations of Hubble residuals with light-curve shapes, colors, UV and NIR data, spectral features, and host-galaxy parameters. Indeed, the imperfect modeling of supernova colors (coupled with calibration) is a leading source of systematic uncertainty in current SN-cosmology analyses (Kessler et al. 2009). Improvements to the SN Ia model will decrease both their intrinsic dispersion and suppress systematic uncertainties. \nWe will engage in the study of precursor SN data and theory to improve the model for determining absolute magnitudes and quantify model uncertainties. There is ongoing work run by distinct groups who have access to their own data. Almost all relevant groups are represented in the DESC and we will work to pool our collective knowledge to formulate SN Ia models that coherently combine the collective knowledge. While this is not a time-critical activity that must be done in the next three years, it is very important for LSST DESC scientists to remain active in the cutting-edge advances in supernova standardization. \n## **3.4 Clusters of galaxies** \n## **3.4.1 Overview** \nGalaxy clusters provide a powerful toolset with diverse applications in cosmology and fundamental physics. The observed number density and clustering of galaxy clusters as a function of mass and redshift are sensitive to both the expansion history and growth of structure in the Universe, enabling powerful constraints on dark energy and providing critical distinguishing power between dark energy and modified gravity models for cosmic acceleration. Measurements of the baryonic mass fraction in clusters, and of the tomographic lensing signatures through clusters, provide additional ways to measure cosmological parameters. Galaxy clusters provide sensitive probes of the physics of inflation and, in combination with CMB data, currently provide our best constraints on the species-summed neutrino masses. \nAs with all cosmological probes, the key to extracting robust cosmological constraints from galaxy clusters is the control of systematic uncertainties, particularly those associated with finding clusters and relating the observed properties of clusters to the underlying matter distribution. This requires a coordinated, multiwavelength approach, with LSST at its core, and the application of rigorous statistical frameworks informed by cosmological simulations. (For a recent review of galaxy cluster cosmology see Allen et al. (2011).) \n## **3.4.2 Analysis steps** \nThe primary way in which galaxy clusters probe cosmology is through measurements of their spatial distribution, as a function of mass and redshift. Traditionally, this approach has been split into measurements of the mass function and its evolution (e.g., Vikhlinin et al. 2009), and the \n50 \n_3.4 Clusters of galaxies_ \nclustering of galaxy clusters (e.g., Fedeli et al. 2011). However, significant gains can be realized by modeling all aspects of the distribution simultaneously (Sartoris et al. 2012). \nThe four main steps in extracting robust cosmological constraints from the observed number density and clustering of galaxy clusters are 1) predicting the statistical spatial distribution of massive halos as a function of mass and redshift; 2) constructing the catalogs of observed clusters used for cosmological work; 3) measuring the relationships between survey and follow-up observables and mass; and 4) extracting the cosmological information of interest using a self-consistent statistical framework. \n## _**Inferring cosmological parameters from strong lenses**_ \nThe final stage of the analysis is the inference of dark energy parameters from an ensemble of measured time-delay distances. Conceptually, the situation is very similar to that of the supernova Ia probe. Likewise, one might expect to be able to improve the accuracy of the constraints on dark energy parameters by hierarchically inferring the hyperparameters that describe the population of lenses and their sources, in a joint analysis of all systems. In principle this analysis could include many more systems that did not have such high fidelity measurements, but that nevertheless could provide information in a “statistical” analysis. This is a topic that should be explored using mock catalogs and observations prior to the survey. \n## _**Multiple source plane lens cosmography**_ \nFinally in this sub-section, we turn briefly to our secondary probe. As far as cosmography with multiple sets of multiple images is concerned, recent work has shown that in principle it can be competitive, providing enough systems can be analyzed (Collett et al. 2012; Jullo et al. 2010). The main sources of systematic error are once again the mass modeling of the main deflector, and the structures along the line of sight (Dalal et al. 2005; D’Aloisio & Natarajan 2011). From the point of view of LSST it is useful to distinguish between the cases when the main deflector is a galaxy or a cluster. \nIn the case of galaxy-scale systems, the first step is once again finding them. Compound lens systems are rarer than single multiple imaged galaxies. Gavazzi et al. (2008) estimate that 1 in 50 galaxy scale lenses should be identifiable as a multiple source plane system at HST singleorbit depth and resolution, implying that several hundred galaxy-scale compound lenses should be contained in the LSST lens sample. The majority of these systems will have as their first source a faint blue galaxy rather than an AGN; for these to be detected, the automated lens discovery algorithms will need to be extended to focus on extended, non-variable arcs around massive galaxies (e.g., Marshall et al. 2009). If visible at all in the survey images, the second arc system could be detectable via image modeling as described above—especially as the lens perturbation provided by the first source is small, allowing simple models to be used in the automated classification. Synergy with Euclid or SKA could be particularly important here. \nThe time-domain information provided by LSST will not be relevant in all but the rarest cases: all the cosmographic information comes from the positions and morphologies of the two arc systems, \n59 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nfor which redshifts will need to be measured. The follow-up strategy is likely to be very similar to that employed for the time-delay lenses, and the need to model carefully the lens environment also carries over from our primary probe. \n## _**Inferring time delays in the presence of microlensing**_ \nThe light curves of the lensed sources extracted from LSST images will be long, but not necessarily better sampled than the current state of the art (e.g., Courbin et al. 2011). It has been shown by, for example, Burud et al. (2002); Morgan et al. (2010); Kochanek et al. (2007) that the microlensing effects of the stars in the lens galaxy on the images of the background point source can be accounted for given good data, but that the modeling can be very computationally expensive. Can lens light curve modeling be performed more efficiently without introducing systematic error in the time delays? \nThe observing strategy itself could be a source of systematic as well as statistical uncertainty: short seasons leave gaps in the light curve that could be problematic (e.g., Press et al. 1992; Kundi´c et al. 1997). Research is needed to quantify the impact of this on the inferred time delays. The cadence within a season must be high enough to provide sufficient sampling of both the intrinsic variability of the source and the microlensing applied on top of it. In principle, there is a factor of 5 or so in sampling rate available to us by analyzing the light curves in all filters simultaneously. However, to do this well will require accurately modeling the color variability of the source, and the source size and microlensing caustic pattern. \nSoftware developed to implement efficient microlensing correction will be useful through Stage III, where the follow-up monitoring data are taken in a single filter. The multi-filter analysis is an LSST-specific project, requiring development on simulated light curves in preparation for the survey data. The main motivation for studying this now is to provide feedback on the survey strategy, and provide estimates of the distribution of precision in time-delays: the size of the cosmographic sample will be limited by our ability to measure time delays. \n## _**Detailed follow-up of 1000 lenses with JWST and Extremely Large Telescopes**_ \nObtaining and analyzing the deep, high spatial and spectral resolution follow-up observations that the strong lensing program requires represents a significant challenge. The calibration and modeling software will need to be made robust and semi-automated without loss of accuracy, a process that \n62 \n_3.5 Strong lensing_ \n68 \n_3.6 Theory and joint probes_ \n**LSST and other optical/near-IR surveys.** At optical and near-infrared wavelengths, a suite of new ground-based surveys are about to come on-line that will provide crucial pre-cursor datasets in advance of LSST. These include the Panoramic Survey Telescope and Rapid Response System (PanSTARRS); the Dark Energy Survey (DES); the KIlo-Degree Survey (KIDS) and the complementary VISTA Kilo-degree INfrared Galaxy survey (VIKING); and the Subaru Hyper Suprime-Cam survey (HSC). These experiments offer significant potential for a broad range of science including most of the science goals spanned by LSST. \nPlanned, space-based survey missions such as the Wide Field Infrared Survey Telescope (WFIRST) and the European Space Agency’s Euclid also offer outstanding potential and synergies with LSST, complementing the ground-based data in particular by extending search volumes to higher redshifts. Euclid will map the same sky with a broad optical band with lower sensitivity, but higher resolution than LSST. Euclid will provide an independent measurement of cosmic shear. Its measurements will be more sensitive to the shear in the inner regions of a typical galaxy ( _θ <_ 0 _._ 5 _[′′]_ ), while LSST will be more sensitive to shear measured in the galaxy’s outer isophotes. By comparing and cross-correlating these two measurements, we will be able to better characterize systematics \n69 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nin each experiment and obtain a shear power spectrum that will be nearly independent of the systematics in each experiment. Euclid will also complement LSST photometric measurements by making near-infrared measurements with its three broad infrared bands. These measurements should significantly improve LSST photo- _z_ estimates and enable higher precision tomographic measurements (Abdalla et al. 2008). \n**LSST and spectroscopic surveys.** _Breaking cosmological degeneracy._ Spectroscopic redshift surveys like BOSS, eBOSS, BigBOSS, DESpec, or Euclid will complement LSST dark energy constraints by adding independent cosmological measurements, which break degeneracies. The complementarity is enhanced when the surveys overlap on the sky so that their different tracers measure exactly the same density fluctuations (e.g., LSST lensing and galaxy densities, and redshift-survey galaxy density) and permit cross-correlation between the redshift survey and photometric survey data to calibrate photo- _z_ ’s. The redshift survey helps measure geometry through 3-d BAO and more general Alcock-Paczynski-like uses of the broadband redshift-space power spectrum. It also constrains the growth of structure through redshift-space distortions and higher order statistics (e.g., bispectrum) that should break degeneracies in the lensing survey dark energy / modified gravity parameters by providing constraints on nonrelativistic tracers of the gravitational potentials (e.g., Zhang et al. (2007)) and by helping constrain the effects of neutrino masses and inflation parameters on the shape of the power spectrum. \n**LSST and CMB.** Large-scale CMB fluctuations are mostly sensitive to physical conditions at _z_ = 1100 and low-redshift physics, including the integrated ISW effect, for which cross-correlation with LSST tomographic slices will provide a measurement of the growth of structure during accelerative expansion. The small-scale CMB statistics contain a wealth of information about physical effects, all of which can be cross-correlated with the LSST data in a number of interesting ways. \n_CMB deflections and cosmic shear._ Just like galaxy-emitted photons, the CMB photons on their way from the surface of last scattering are gravitationally lensed by the intervening matter distribution. This effect was detected for the first time in cross-correlation techniques with WMAP data (Smith et al. 2007; Hirata et al. 2008) or more recently though the lensing power spectrum by ACT (Das et al. 2011) and SPT (van Engelen et al. 2012). Planck will soon release its temperature maps and its polarization maps in 2014. These maps will provide S/N _∼_ 1 measurements of the cosmic shear field with 2 _[◦]_ resolution. By 2015, ACTPOL and SPTPOL’s wide observations will provide a _S/N >_ 1 lensing map with 20 _[′]_ resolution. By 2020, we anticipate that ground-based CMB polarization maps will provide lensing maps with _<_ 10 _[′]_ resolution. We plan to cross-correlate these CMB lensing maps with LSST lensing maps to trace the evolution of matter fluctuations over a wider range of redshifts. Song & Knox (2004) anticipated that the cross-correlations between Planck lensing and LSST will significantly improve the determination of cosmological parameters. Since small-scale CMB experiments have much higher-resolution lensing maps, the combination of these measurements with LSST will yield even more powerful constraints on the growth rate of structure. This will increase our sensitivity to the early evolution of dark energy and to neutrino mass. \n_CMB deflection and galaxy counts._ While LSST survey data focuses on structure at _z ≈_ 0 _−_ 2, CMB lensing surveys provided by the Planck satellite and higher-resolution ground-based CMB polarization experiments will probe the matter power spectrum at _z ≈_ 1–5. Cross-correlation determines the bias of the tracer galaxies and normalizes the amplitude of the galaxy power spectrum \n70 \n_3.6 Theory and joint probes_ \n(see Bleem et al. (2012) and Sherwin et al. (2012) for recent measurements of CMB lensing crosscorrelations with _z ∼_ 1–2 samples). In conjunction the dark energy figure of merit can be improved by a factor of 2 relative to neglecting CMB lensing. \n_Thermal Sunyaev-Zeldovich (tSZ) effect – measuring the integrated pressure._ Over the next 2– 3 years, the completion of the SPT, ACT, and Planck tSZ catalogs will extend our detailed, statistical knowledge of galaxy clusters out to _z >_ 1, with a well-defined, nearly mass-selected, cluster sample. Together, these projects expect to find _∼_ 1000 new clusters, mostly at intermediate to high redshifts. Used in combination with existing low-redshift X-ray and optical catalogs, they should provide significant near-term improvements in our knowledge of cluster growth, and corresponding improvements in the constraints on dark energy and gravity models. In the LSST era, the development of experiments with improved sensitivity, frequency coverage and spatial resolution will, like X-ray observations, provide an excellent complement to LSST both in cluster finding and additional mass-proxy information. \n_Kinematic Sunyaev-Zeldovich (kSZ) effect - measuring the integrated momentum._ The kSZ effect traces the large-scale velocity fields and has the potential of tracing large-scale velocity flows. The first detection of this effect was recently made by cross-correlating CMB and spectroscopic galaxy data (Hand et al. 2012a). By using external spectroscopic samples (e.g., BOSS, BigBOSS, DESpec, PFS) to determine the large-scale momentum field (Ho et al. 2009) and the LSST data to determine the galaxy positions, we can measure the cross-correlation between the galaxy momentum field and the kSZ signal. This cross-correlation measures the large-scale distribution of electrons around galaxies. By measuring these correlations as a function of redshift, in the context of dark energy, this would provide us with a complementary measure of the gravitational fields and their evolution as a function of redshift; more generally, it will tell us about how galaxy feedback drives gas into the intergalactic medium. \n**LSST and X-ray surveys.** X-ray observations currently offer the most mature and precise technique for constructing cluster catalogs, the primary advantages being excellent purity and completeness and the tight correlations between X-ray observables and mass. \nOver the next 2-3 years, X-ray cluster samples constructed from ROSAT, Chandra and XMM observations will continue to offer important gains in cosmological constraints. The next major advance at X-ray wavelengths, however, will be provided by the eROSITA telescope on the Spektrum-Roentgen-Gamma Mission. Scheduled for launch in 2014, eROSITA will perform a fouryear, all-sky survey that should detect an estimated 50,000–100,000 clusters. The cross-correlation of LSST and eROSITA-detected clusters in particular offers the potential for the construction of large, landmark cluster catalogs with exquisite purity and completeness, and systematic uncertainties controlled to levels far exceeding those possible with either experiment alone. \nX-ray observations provide the best mass proxies for galaxy clusters, with observed scatters of _<_ 15% over the full redshift range of interest. Follow-up observations with high throughput X-ray telescopes will therefore be essential for LSST cluster science. X-ray observatories like Chandra, XMM, Suzaku, ASTRO-H, and eROSITA will be the cornerstones of this work. \n**LSST and hydrogen line (HI) surveys.** LSST by itself will produce deep imaging of a large fraction of the sky and will produce photometric, not spectroscopic, redshifts for the galaxies detected. A complementary approach to large-scale structure, under development, is mapping the 21-cm emission from galaxies (HI surveys). HI surveys contemporary with LSST will map the \n71 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nlarge-scale features in HI emission with a technique called intensity mapping, where individual galaxies are not resolved. With sufficient improvement in angular resolution (say 10 _[′]_ or better) and low enough noise in the HI map (say smaller than 50 _µ_ K) one will be able to resolve nonlinear concentrations of HI corresponding to groups of galaxies at redshifts 1–2. In combination, a Southern Hemisphere HI survey plus LSST could provide a complementary source of redshift information and tell us about the evolution of the HI content by galaxy type, which will be of great use for constraining models of galaxy formation and evolution. \n## **3.7 Photometric redshifts and other common issues across multiple probes** \n## **3.7.1 Overview** \nAll LSST probes of dark energy and related physics rely on determining the behavior of some quantity as a function of redshift _z_ . Distances, the growth rate of dark matter fluctuations, and the expansion rate of the Universe are all functions of redshift that can be readily calculated given a cosmological model; dark energy experiments then constrain cosmological parameters by measuring observables dependent upon these functions. However, it is completely infeasible with either current or near-future instruments to obtain redshifts via spectroscopy for so large a number of galaxies, so widely distributed, and extending to such faint magnitudes, as those studied by LSST. \nHence, LSST will primarily rely on _photometric redshifts_ – i.e., estimates of the redshift (or the probability distribution of possible redshifts, _p_ ( _z_ )) for an object based only on imaging information, rather than spectroscopy (Spillar 1985; Koo 1999). Effectively, multiband (e.g., _ugrizy_ ) imaging provides a very low-resolution spectrum of an object, which can be used to constrain its redshift. Because flux from a relatively wide wavelength range is being combined for each filter, imaging provides a higher signal-to-noise ratio than spectroscopy; however, broader filters provide cruder information on the spectrum, and hence on _z_ or _p_ ( _z_ ). \n## **3.7.2 Analysis steps** \n**Survey Photometry:** The determination of a photometric redshift for an LSST galaxy will begin \n72 \n_3.7 Photo-z’s and common issues_ \nwith measurements of the flux from an object in all LSST bands, provided by data management. Most efficiently, these may be the aggregate fluxes from combining information from all images in a given band. However, the maximum information would be obtained by utilizing all single-visit measurements of an object to estimate the photometric redshift (see also Section 3.7.3). \n75 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \ncould be discarded (e.g., Jain et al. 2007; Nishizawa et al. 2010). Removal of problematic galaxies will reduce outlier rates, but will also reduce the overall number of galaxies useable for science; we need to explore these tradeoffs to be able to predict LSST system performance accurately. \n## _**Sky catalogs**_ \nAn understanding of how measurements of dark energy depend on statistical and systematic uncertainties relies on a detailed knowledge of how the signatures of dark energy are imprinted within the properties of astrophysical sources. To address these questions for the LSST prior to operations, \n84 \n_4.1 Simulations_ \nand thereby optimize the returns of the survey, requires that we simulate the small- and largescale cosmological signatures expected within the LSST data stream (e.g., from galaxy clusters to Baryon Acoustic Oscillations) as well as the astrophysical foregrounds that might systematically bias the cosmological signal. \nMany of the dark energy tasks described in Chapter 3 will be undertaken either using catalog data (with the appropriate uncertainties assign to the measured attributes) or will use catalogs as inputs to the image generation. Development of strategies and algorithms to optimize the performance of the of the LSST will, therefore, depend on how well we can characterize and model the properties of the observable Universe. \nThe fidelity required for mock galaxy catalogs is dependent on the science task at hand. Dark matter simulations incorporate the effects of standard gravity, including the collapse of halos and the persistence of subhalos (usually identified with galaxies). They do not, by definition, include baryonic effects, which must either be included directly using hydrodynamic simulations (see e.g., the code comparison paper by Scannapieco et al. (2012)), or by the use of ad hoc prescriptions. For large-scale structure, positions of galaxies along with luminosities need to be produced. These can be generated by using dark matter positions (e.g., taking galaxies of a given density and assigning galaxy properties to individual dark matter particles, e.g., Cole et al. (1998); Busha & Wechsler (2008); or, through halo and sub-halo models, using observed correlation functions and/or luminosity, to constrain the number of galaxies per halo of a given luminosity and then generating this number of galaxies within each dark matter halo, or using this information to match subhalos to luminosity (e.g., Seljak 2000; Peacock & Smith 2000; Yang et al. 2003; Conroy et al. 2006). In some cases luminosity is assigned first, then color, e.g., Skibba & Sheth (2009). In other cases, the assignment is done in one step by matching to a similar galaxy in observations, e.g., Cole et al. (1998); Busha & Wechsler (2008); Song et al. (2012), or is derived by following the subhalos history (semi-analytic models such as reviewed in Baugh (2006) and Benson (2012)). \n85 \n_Chapter 4: Simulation Tools and Technical Infrastructure_ \n**==> picture [187 x 141] intentionally omitted <==**\n**==> picture [188 x 134] intentionally omitted <==**\nFigure 4.3: Models of the observable Universe include the clustering properties of galaxies, distributions of stars that follow the Galactic structure, and solar system sources. These are designed to reproduce the key observable properties that drive the survey. For example, the left panel compares the simulated density of galaxies (green points) with observed galaxy number counts (crosses) and the right panel compares the predicted size distributions of galaxies (blue line) with that observed from the COSMOS HST survey. \nIn the context of the DESC tasks outlined in Chapter 5, the limitations of the current implementation of the cosmological catalogs arises, principally, from the limited cosmological information present within the data. The catalogs only extend over a single LSST pointing (i.e., large-scale BAO signatures are not present). Larger catalogs are generated by tiling the individual pointings across the sky. Cosmologically variable sources, such as Type Ia supernovae, are not simulated as part of the catalog generation. Weak lensing signatures are also not incorporated within the data; they must be coupled to the underlying dark matter mass distributions. Baryonic modifications to the dark matter distribution need to be either modeled in the analysis or somehow grafted onto the dark matter distribution. Again, the accuracy of these corrections needs to be high enough to not undermine the desired precision of the observations. \nBeyond the cosmological properties of the data, astrophysical foregrounds will limit the information on dark energy that can be extracted from the LSST. Biases in the cosmological signals will arise because of the distribution of stars within our Galaxy (where the halos of bright stars can modulate the efficiency of detecting galaxies); because of differences between the colors of stars used to derive a PSF and the colors of galaxies to which that PSF is applied; because of the modulation of the density and colors of galaxies as a function of position on the sky due to Galactic extinction and reddening; and, because identification of candidate Type Ia supernovae can be contaminated by other variable sources. \nTo model these effects, stellar distributions must incorporate positions, velocities and parallaxes for the 9 billion sources that reside within the LSST footprint Juri´c et al. (2008). Densities of stars (which impact the interpolation of the PSF, how well the telescope can guide, and the depth of the galaxy catalogs in the presence of bright stars) must be modeled as a combination of thin-disk, thick-disk, and halo stellar populations. Current foreground implementations used in the LSST match the densities of stars observed by the Sloan Digital Sky Survey (SDSS) but do not replicate regions of high stellar density close to the Galactic plane. Variability is assigned in an ad hoc manner to approximately 10% of the stars using light curves that model transient and periodic behavior, but not to supernovae. \n86 \n_4.1 Simulations_ \nIn Section 5.9.2, we will describe the enhancements necessary to make the current simulated catalogs applicable to the proposed DESC trade studies and analyses. This includes: making the data accessible to all of the DESC collaborations, coupling the catalogs to the cadence of the observations, incorporating realistic errors within the measured properties, and generating magnitudes and errors that map to the conditions under which the simulated data were “observed”. \n## _**Operations simulators**_ \nThe efficiency and performance of the LSST depends not only on its optical system. The observing strategy, weather, scheduled down time, properties of the site and the mechanical and electronic properties of the telescope and camera will all impact the area and depth of the survey over its tenyear lifespan. To characterize these dependencies and to enable studies that can optimize survey performance the project has developed an operations simulator (OpSim). OpSim has been used extensively throughout the LSST project to compare the performance of different sites, to study the impact of readout time and slew time on survey efficiency, and to undertake trade studies such as the trade-off between field of view and mirror size. \nOpSim simulates sequences of observations based on a parameterized description of the LSST survey. It takes as input historical weather records and measures of the atmospheric conditions for the Cerro Pachon site (taken over a 10-year time period). From this, a sky model is generated that predicts sky brightness (Krisciunas & Schaefer 1991), atmospheric seeing, and cloud coverage as a function of time. Coupled with a mechanical model for the telescope (e.g., one that predicts slew and settle times) and a model for the camera shutter, readout and filter exchange times, estimates of the efficiency of the survey can be made as a function of integration time. \nObserving sequences are simulated in a Monte Carlo fashion. Given specified sets of cadences (e.g., how often an exposure must be repeated), OpSim ranks all potential telescope pointings based on their ability to satisfy the LSST science requirements and the cost of slewing to that location. This process is undertaken dynamically, with the scheduler deciding which patch of the sky represents the best target for each subsequent observation. Figure 4.4 demonstrates the outputs of OpSim. The left panel shows the depth, relative to the nominal survey, as a function of position on the sky in the _i_ band and after 10 years of observations (note the main survey area of 18,000 degree[2] goes deeper than the extension of the survey along the North Galactic Spur). The right panel shows the resulting distributions of seeing values (including the atmosphere, telescope, and camera contributions) that accompany this 10-year simulation. \n87 \n_Chapter 4: Simulation Tools and Technical Infrastructure_ \n**==> picture [187 x 137] intentionally omitted <==**\n**==> picture [188 x 113] intentionally omitted <==**\nFigure 4.4: The left panel shows the depth after 10 years in the _i_ -band relative to the nominal depth of the survey. Areas outside of the main survey reach a shallower depth due to less sampling. The right panel shows the distribution of seeing values for all observations within a single 10-year simulation from OpSim. \nto address the tasks outlined in Chapter 5. These include: the integration of OpSim with the catalog generation and calibration procedures to provide estimates of the uncertainties on the measured properties of sources (e.g., to test the impact of photometric redshifts and measures of large-scale structure); the inclusion of dithering (currently OpSim uses a fixed pattern of fields with only rotational dithering) to enable studies of the impact of dithering on the PSF and large scale structure; the generation of a lookahead model for the scheduler that will enable optimization of survey strategy by predicting the expected survey conditions as a function of time of night and position on the sky. \n## _**Photon simulator**_ \nA significant fraction of all dark energy measurement systematics with LSST will come from the complex physical effects of the atmosphere, telescope, and camera that distort the light from astrophysical objects. This distortion is imprinted in the images, and therefore the most direct and robust way of mitigating and understanding these systematics is through high-fidelity image simulations. We may use codes for simple image simulations through a parameterized point-spreadfunction (e.g., GalSim), but for detailed simulations in the analysis tasks outlined in Chapter 5, we will require significantly more detailed simulations. \nTo transform the catalogs of astrophysical objects into images, the DESC will use a high fidelity simulator developed in cooperation with the LSST Project called the photon simulator (phoSim). The photon simulator does this using a photon Monte Carlo approach. A Monte Carlo approach produces simulated images efficiently and encodes an arbitrary complexity of atmosphere and instrument physics in terms of photon manipulations. The basic approach of the simulator is described below. \nTo simulate images, photons are first sampled from the catalogs of astrophysical objects described in the previous section. The direction of the photons is chosen from the source positions and spatial models, and the wavelengths of the photons are sampled probabilistically from the spectral energy distributions (SEDs) assigned to each object. Then, the photon simulator simulates the atmosphere by propagating the photon through a series of layers placed vertically between the \n88 \n_4.1 Simulations_ \nground and 20 km. Each layer has some probability of eliminating the photon, either through cloud opacity, molecular absorption, or Rayleigh scattering according to atomic physics and atmospheric structure models. Each layer also has some probability of altering the trajectories of the photons by refraction of atmospheric turbulence. The turbulence is modelled by phase screen perturbations having a Kolmogorov spectrum up to an outer scale where the turbulence is driven, and the turbulence intensity as a function of height is modeled for the LSST observing site. The phase screens are drifted during the simulated exposure, so each photon hits a different part of the screen depending on its intial position and arrival time. \nAfter the atmosphere, the photons are then raytraced through the telescope and camera. To do this, every photon is either reflected and refracted through the series of mirrors and lenses correposnding to the LSST optical design. During each photon interaction, misalignments and surface perturbations are modeled for every degree of freedom from the ideal mirrors and lenses. This contributes significantly to the image quality. Photons are removed as they interact with the mirror coatings, lens anti-reflective coatings, and filter multilayer coatings. When the photons enter the Silicon of the CCD, they are refracted, and the position of photon conversion is calculated probabilistically from its wavelength-dependent mean free path. If the photon converts within the Silicon, the path of the photoelectron is followed according to a model of the electric field in Silicon appropriate for LSST’s sensors, then the electrons are drifted to the readout. Saturation and blooming are simulated if the number of electrons exceeds the full well depth. The number of electrons in each pixel is then use to build up an image of the sky. Finally, the image is digitized through simulation of the readout process taking into account charge transfer inefficiency for each amplifier and various sensor defects. An example of a simulated image is shown in Figure 4.5. Figure 4.6 shows simulations with different physics turned on and off. \nFor one visit, phoSim takes roughly 1000 CPU hours to simulate all 6048 amplifier images. Taking into account the catalog constructor, DM pipelines (and variants), as well as different configurations on the same images and custom analyses, we expect about 2000 hours of mid-range compute time per visit. The Weak Lensing group (for example) expects to need very simple sky definitions - these are far less compute-intensive and can be run on single CPUs. The total output of the sims is about 10 to 15 compressed Gbytes per visit in images; reasonable development timeframe simulations of perhaps 2500 focal planes would need on the order of 50 TB of disk space and 3 million CPU hours to produce. Note that cosmological simulations will be run on exascale machines whose cycles are obtained separately. \nIt is expected that the Working Groups will need a range of data products: catalogs and single images, produced with different cosmologies, sky definitions and instrument parameters, including atmosphere and telescope properties. These will demand a wide range of computing resources, from single cores for work involving a handful of stars, to roughly 100 cores for a day for a few amplifier images, on up to thousands of cores employed for weeks to months for the large-scale work. \n## _**Development Phase needs**_ \nIn the first few years, the Working Groups will be using various data products, producing them with different cosmologies and varied parameters of the image simulations. They will need flexible access to the simulation tools to easily run end-to-end simulations for short turnaround studies on up to hundreds of cores for a day, and will need much larger scale community simulations occupying thousands of cores for weeks to months, similar in size to Project Data Challenges that have taken place to date. The large scale simulations need organization and excellent data access to handle the hundreds of thousands of jobs and multi-hundred TBs of data. Very efficient work flow engines are needed to push this scale of jobs through on multiple collaboration sites as well as the GRID, with excellent fault tolerance and retry capabilities to achieve failure rates below 0.1%. Catalog and data access tools are also essential to select and fetch the desired data. Tools (e.g., xrootd) \n94 \n_4.3 Technical coordination_ \nexist already to handle PBs of local and WAN data in logical clusters. Manpower will be needed to run and keep track of produced simulations; once it is all automated with access tools in place, the manpower needed should drop to a fraction of an FTE – Fhence the further need for an early start on this infrastructure. \n## _**Construction Phase needs**_ \nIt is anticipated that Data Challenges would be scaled up by a factor of 2 to 4 to create 5–10k visits, requiring upwards of 10k cores and a few hundred TB of storage per year. By this time, infrastructure should be in place for the efficient management of job submission, datasets and storage. A baseline level of manpower will still be needed to handle the production runs. \n## _**Production Phase needs**_ \nDuring this phase, Level 3 code will be run to reprocess pixel-level data. A full reprocessing of 2 years worth of LSST data (e.g., with different algorithms) will likely take 3 months with an installation of 400–1000 TFlops, and would place an upper bound on the resources needed. \n## **4.3 Technical coordination** \nIn any experiment which pushes the envelope technically and scientifically, it is necessary to get close to the experiment in order to carry out the science. This is particularly true of LSST probes of dark energy, each of which depend critically on understanding the details of the LSST system, operations, and the properties of the resulting data. Until now it has proven convenient to use the LSST baseline system definition and performance requirements to estimate residual systematic errors, and assume that they are Gaussian distributed. However, it is often the case that the nongaussian and non-stochastic tails of distributions in an experiment, coupled with subtle sample selection effects, ultimately determine the achievable precision. Low level systematics which do not average down with the number of exposures will be particularly worthy of investigation. \nUsing end-to-end simulations based on the existing LSST design, together with performance measurements on existing LSST subsystems (hardware and software) and measurements at the LSST site, it will be possible over the next few years to obtain a deeper understanding of the LSST systems, the statistics of relevant parameters, and their impact on each of the dark energy probes. One important early goal will be to incorporate this more realistic information on the expected LSST data in the end-to-end LSST image simulator ImSim, enabling joint analysis of multiple probes and a realistic assessment of LSST’s ultimate capability of constraining models of dark energy physics. \nFor this to happen there needs to be a two-way interaction between members of the DESC and the LSST project. Within the DESC, Chris Stubbs (Harvard) and Tony Tyson (UC Davis, LSST) are charged with supporting and coordinating this interaction bridging the DESC and Project. This will involve inquiries that arise in both directions, and proposed measurements and investigations. The LSST project technical teams will increasingly require input from the science collaborations \n95 \n## _Chapter 4: Simulation Tools and Technical Infrastructure_ \non specific technical design issues that arise as the designs mature. And the DESC working groups will need deep understanding of the LSST system. This will require system-wide involvement to validate algorithms, develop new algorithms, ensure simulator fidelity, test the simulator and system components, assess the calibration plans, and explore operations cadence scenarios and residual systematics. \nOne example is the measurement of the shapes of astronomical objects, which is the key observable for weak lensing dark energy studies. The intrinsic shape of a source is degraded/modified by the combination of 1) atmospheric effects, 2) optical aberrations, and other effects from the telescope and optics 3) the detector’s “shape transfer function”, and 4) the code and algorithms that are used for shape processing . These are not independent, and attaining a detailed understanding of the entire shape measurement chain, from the light hitting the silicon in the detectors to the bits hitting the silicon on the disk, is imperative in order to truly understand the shape measurements and the associated systematic error budget. This endeavor straddles the hardware, the software, and the science analysis. The DESC will achieve the end-to-end understanding that is required in order to make the fullest (and fully informed) use of the LSST data set. \nAnother example is the formulation of dark energy specific system monitoring metrics for data quality assessment. To facilitate this two-way interaction, Stubbs and Tyson in collaboration with DESC members will develop specific joint investigations and will involve key members of the LSST Project, such as the system scientist, system engineer, data management scientist, camera scientist, and image simulation coordinator. Several of these scientists are members of both the LSST project and the LSST DESC. While parts of some of these investigations will occur within the LSST construction activity, the dark energy specific issues will benefit immensely from a coordinated effort. \n96 \n## **5 Detailed Work Plan** \nIn this Chapter, we lay out a detailed work plan for the first three years. The tasks that have been identified are distilled from the more general discussions outlined in Chapter 3, and reflect the prioritization criteria identified in Section 2.6. We use the designation “H” to denote “high priority” tasks, which are especially time urgent, and “L”T to denote “longer term” tasks, which are equally important, but can be pursued on a more relaxed schedule leading up to the onset of data taking. We have organized the analysis tasks by working group, as was done in Chapter 3. However, there are several tasks that are especially “cross-cutting” and are called out separately. The tasks related to simulations, computing infrastructure, and technical coordination with the LSST Project are organized as described in Chapter 4. \n## **5.1 Weak lensing** \n## **5.1.1 High-Priority Tasks** \n   - _Deliverables:_ We will inform Data Management how much (if any) the current star-galaxy and WCS algorithms need to be improved, including useful target requirements. \n- **H-3. WL: Plan ImSim simulations to test WL analysis** \n   - _Motivation:_ Many of the studies of systematic errors that need to be performed by the WL working group will require simulated images, where we have knowledge of the truth values. A well-motivated, staged series of simulations will take significant time to plan and coordinate, considering the amount of computing time necessary to generate and analyze the simulations, and the fact that some capabilities that are eventually required are not currently available in ImSim (see Section 5.9.3). \n98 \n_5.1 Weak lensing_ \n   - _Activities:_ There will be some WL analysis tests that need information or a level of fidelity that is not already present in the ImSim framework, and these should be identified as a priority for development within the ImSim group. We will evaluate each area of study to determine whether the ImSim team has already produced simulations appropriate for the work, or whether further simulations are required. These can be roughly divided into categories according to whether the required simulations are wide, or deep. Where further simulations are required, we will identify the simulation volume and basic requirements (e.g., any unusual inputs). In the longer term, we will want to test not just Data Management but also higher-level codes that use DM outputs to constrain cosmology; we will also consider what simulations are needed, both the cosmological simulations that are the basis for galaxy catalogs and also the phoSim settings that are needed. It is likely that this list can be condensed into a few specific simulation requests to pass to the ImSim team. We will also need to determine whether the WL analysis tests require raw images, processed images, or catalog-level data. \n   - _Deliverables:_ A list of features/inputs that need to be included in ImSim in order to enable WL systematic studies, and a list of simulations that will be necessary for WL analysis work and systematic studies. \n- **H-4. WL: Multi-epoch data processing algorithm and optimal dithering strategy** \n   - _Motivation:_ As described in Section 3.1.3, we need to determine how best to combine information from the _∼_ 100 observations per bandpass for each galaxy while minimizing systematic errors. We also need to develop an optimal dithering strategy such that these observations help to minimize systematic errors in shape measurements. \n   - _Activities:_ We will continue to develop two algorithms for dealing with this, “MultiFit” and “StackFit”, with a focus on validating them using both simulated and real survey data. We will also carry out a suite of image simulations and determine the optimal sets of dithering strategies to maximally reduce PSF systematics. \n   - _Deliverables:_ We will provide LSST Data Management with performance information (related to systematic errors, speed, parallelization, etc.) about the two algorithms, with recommendations for how best to proceed with development. We will continue to work closely with the development team to test the shear accuracy of the algorithms and help to improve them. Requirements on dithering strategies to minimize PSF-related systematics will also be delivered to the LSST Project. \n## **5.1.2 Longer-Term Tasks** \nWe also have several longer-term tasks, for which work will begin during the next three years, but will likely continue for some time beyond that. \n- **LT-1. WL: Develop non-canonical WL statistics that have the potential to improve dark energy constraints** \n99 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n   - _Motivation:_ The canonical WL statistic for constraining dark matter is the two-point shear correlation function. However, there are several other WL statistics that have the potential to add significantly to overall dark energy constraints from the LSST experiment. Examples include lensing peak statistics, cosmic magnification, three-point shear correlations, and multiple source-plane cluster tomography, among others. \n   - _Activities:_ We will investigate how these statistics might impact the total dark energy constraints for LSST and what might be done to help mitigate systematic errors in them, using realistic models for those systematic errors (e.g., photo- _z_ errors) in LSST. \n   - _Deliverables:_ We will produce algorithms to calculate these statistics and to reduce systematic errors in the measurements. As appropriate, we will interface with LSST Data Management concerning requirements or desirable improvements in their pipeline algorithms. \n101 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \ngalaxy-mass correlation, galaxy bias, and BAO signal - even though the BAO scale is similar to the LSST pointing size. \n- _Deliverables:_ An optimum observing strategy, including dither distribution and scheduler optimization using weather and DQA inputs and site monitoring requirements; analysis algorithm for maximum reduction in systematics due to dithering pattern on LSS scales. \n## **H-3. LSS: Setting requirements on systematics** \n- _Motivation:_ In order to provide feedback on the systematic requirement to both the telescope system and Data Management, we need to quantify the expected impact of various systematics on LSS deliverables. \n- _Activities:_ We will employ a multi-pronged approach here. In the first approach, we will pixelate the sky and accumulate the pixelized galaxy counts from full-sky catalog simulations as well as the mean value of the systematic in each pixel. We will transform these data counts and systematic values to over/under densities and calculate the pixel angular correlation function over a wide range of angular scales. By comparing the cross-correlation to the data auto-correlation, we can quantify the effect on angular power spectrum of specific systematics, such as stellar density, seeing, and reddening. We then propagate this into an LSS cosmological parameter estimation code to estimate effects on cosmological parameters directly (see Task 5.2.2:LT-1 below). In the second approach, we will develop parameterized models, incorporate them into a DE forecast tool and quantify their effects on DE constraints. \n- _Deliverables:_ From the first approach: Pixelized angular correlation function and powerspectrum codes, pixelized systematic maps, quantification of angular scales where systematics impact angular-correlation measurements and the BAO signals; from the second approach: Parameterized models of residual systematics, a DE forecast tool which includes model of systematics. From both approaches: we will quantify the requirements on these systematics. \n## **5.2.2 Longer-Term Tasks** \n## **LT-1. LSS: Scalable optimal LSS analysis software development** \n- _Motivation:_ The motivation is two-fold. First, as known systematics are studied and new ones are uncovered, it is essential that mitigation strategies and requirements be developed in the context of the expected ultimate impact on cosmological parameters. Second, with an efficient, standardized, and fully functional pipeline will enable the full LSST DESC LSS collaboration to participate in the BAO analysis efficiently. \n- _Activities:_ We will develop an LSS software pipeline that propagates a photometric catalog through to estimates of cosmological parameters. This activity will include assembling (and possibly upgrading) existing open-source tools and constructing new tools as necessary. The pipeline will be tested on both simulations and precursor survey catalogs. The development schedule will need to be closely coordinated with the activities of \n102 \n_5.3 Supernovae_ \n- _Deliverables:_ Predictions for the impact of photo- _z_ uncertainties on cluster mass calibration across the mass and redshift range of interest; establishment of requirements for external data sets to meet the mass calibration goals. \n## **5.4.2 Longer-Term Tasks** \n## **LT-1. Cl: Optimizing magnification-based cluster mass calibration** \n- _Motivation:_ Magnification information can be used to provide shear-independent mass calibration, with a different sensitivity to systematic uncertainties. Although the statistical precision of magnification measurements may be lower than those of shear, the potential of this technique for mass calibration with LSST has yet to be explored. \n- _Activities:_ The task will use ImSim simulations to assess the precision and accuracy of magnification-based cluster mass calibration, as a function of mass and redshift. It will explore the impact of systematic uncertainties, including photometric calibration uncertainties, for various measurement strategies. \n- _Deliverables:_ The deliverables include a robust assessment of the precision and accuracy of mass calibration achievable with magnification-based measurements, and the improvement in overall mass calibration obtained when combined with shear measurements. \n## **5.5 Strong lensing** \n## **5.5.1 High-Priority Tasks** \n## **H-1. SL: Lens external mass distribution characterisation** \n- _Motivation:_ Estimation of the external convergence estimation is known to be the dominant source of uncertainty in time delay cosmography for overdense lines of sight. The residual systematic errors arising from the choice of mass assignment recipe, calibration simulation and so on could dominate the systematic error budget in all LSST lens samples. This key systematic must be understood as soon as possible in order to allow mitigation plans to be developed. Such schemes could involve long lead-time projects: developing more sophisticated halo model and photometric analysis codes, processing more detailed calibration cosmological simulations, and investigating extensive corollary observations. \n106 \n_5.5 Strong lensing_ \n- **H-2. Phz: Produce realistic tools to test photo-** _z_ **strategies and impact on science requirements** \n_Deliverables:_ Storage requirements for LSST data management. \n## **5.8 Cross-working group tasks** \n## **5.8.1 High-Priority Tasks** \n## **H-1. CWG: Metrics for automated data quality assessment** \n111 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n   - _Motivation:_ The galaxy shear extraction algorithm (and system hardware) must be capable of delivering the level of galaxy shear systematics residual defined in the Science Requirements for individual and dithered exposures. LSST system performance at this level must be continuously validated during the survey. We thus need to develop diagnostics for important systematics (also including photometric and astrometric performance), creating metrics that can be incorporated into the Automated Data Quality Assessment pipeline to be run in real-time. This is important to determine early, since the computational and algorithmic difficulties in running such code as observations are being carried out can be significant. \n   - _Activities:_ Data quality diagnostics of particular relevance to dark energy will be developed. In the next few years this will require fully realistic simulations of PSF systematics based on measured subsystem performance, weather statistics, and dither scenarios. A prioritized list of per-exposure and auxiliary data products will be made, based on their ability to monitor and diagnose data quality issues during the survey. New, currently unplanned, diagnostic data products need to be defined early. \n- **H-2. CWG: Full sky simulations of galaxy density and color systematics using the LSST Operations Simulator** \n   - _Motivation:_ As described in Section 3.2.3, systematics cause correlated variations in galaxy density and color accuracy across the largest angular scales and hence must be studied collectively in full-sky LSST simulations. Coupled variations in survey depth, photometric calibration, and galaxy colors will affect LSS, cluster-finding, cluster “selfcalibration”, cosmic magnification, and photo- _z_ accuracy. \n   - _Activities:_ We will work with the LSST Project to use outputs from the Operations Simulator (OpSim), along with Calibration Simulation (CalSim). The meta-data output by OpSim and CalSim trace additional systematics, including sky brightness, photometric calibration, throughput (atmospheric and telescope), and seeing (PSF). Analytical formulae will be used to estimate the effect on survey depth, photometric zeropoint, and color errors from these systematics. Scaling relations observed in stage III surveys will be used to propagate these quantities into full-sky maps of photo- _z_ accuracy and artificial fluctuations in galaxy density. \n   - _Deliverables:_ For a variety of dither patterns, we will estimate the large-scale angular correlation function and power spectrum of the observed galaxy density and the achieved photo- _z_ accuracy. We will determine the optimal dither pattern for minimizing residual power on the BAO scale and maximizing the performance for clusters, magnification and photo- _z_ . \n- **H-3. CWG: Develop tools to detect, mitigate and remove unknown observational and astrophysical systematics in both LSS and WL to satisfy the DE requirement of LSST** \n112 \n_5.8 Cross-working group tasks_ \n117 \n## _Chapter 5: Detailed Work Plan for the Next Three Years_ \n## **5.9.2 Catalog Improvements** \nAccurate and validated realizations of the Universe are at the heart of the analyses described in Section 3. For simulated catalogs, the properties of extra-Galactic sources, Galactic foregrounds, reddening and extinction, and uncertainties due to the LSST system and its observing strategies must all be modeled. The fidelity of these models must be coupled to the timeline of the studies of systematic and statistical limitations on the measurement of dark energy. Herein, we describe the high-priority developments and tasks that must be undertaken to support the work described in Section 3 and Section 5. \n## _**High-Priority Tasks**_ \n## **H-1. CatSim: Mock catalogs that trace astrophysical properties that impact the DESC** \n- _Motivation:_ A significant number of analysis tasks and use cases described in Section 3 require catalogs that reproduce the observed properties of galactic and extragalactic sources. These properties include: accurate representations of the distribution of sizes, colors, magnitudes, and redshifts of extragalactic sources (with appropriate clustering statistics); the distributions of stellar sources with realistic colors and number densities; accurate realizations of foreground contamination from extinction and reddening, as well as from the variability of Galactic and extra-Galactic sources. Existing catalogs described in Section 4.1.1 need to be improved to better represent the size and color distributions of galaxies, and extended to include supernovae, and gravitational lensing. \n- _Activities:_ The initial component to this work is the definition of a detailed timeline for the inclusion of additional astrophysical components within the mock catalogs. Based on this roadmap, catalogs derived from semi-analytic models, N-body and hydrodynamic simulations, and empirical halo models will be developed. These catalogs will include SNe; a general model for variability (for strong lensing, supernovae, and transient and variable foregrounds); improved representations of galaxy spectral energy distributions; weak gravitational lensing tied to the mass distributions within the underlying N-body simulations; and, the implementation of models for strong gravitational lensing. To accomodate a range of different models (optimized for the tasks described in Section 5), a framework will be require to enable the easy ingestion of different catalogs into the databases that serve the mock catalogs. This work will need to be undertaken in collaboration with the analysis groups. \n- _Deliverables:_ A series of mock catalogs with progressively enhanced fidelity (matched to the needs of the analysis teams) will be delivered together with databases that can serve these data to the analysis teams and to other simulation tools. This work will support the high priority tasks: Weak Lensing (Tasks 5.1.1:H-2 and 5.1.1:H-4), LSS (Tasks 5.2.1:H-1 and 5.2.1:H-2), Supernovae (Task 5.3.1:H-3), Clusters (Task 5.4.1:H1), Strong lensing (Tasks 5.5.1:H-1 and 5.5.1:H-3), Theory (Task 5.6.1:H-4), Photometric redshifts (Task 5.7.1:H-2), Crosscutting (Tasks 5.8.1:H-2 and 5.8.1:H-3), and Technical (Task 5.12.1:H-2). \n## **H-2. CatSim: Validation of mock catalogs** \n118 \n_5.9 Simulations improvements_ \n- _Motivation:_ Mock catalogs used as inputs to the image simulations need to be validated against observational data sets. The fidelity of this validation is dependent on the requirements of individual analysis tasks, but will include requirements on the sizes, shapes, redshifts and colors of galaxies as well as the photometric properties of stars and the impact of the interstellar medium. Validation of mock catalogs is a complex process, since the underlying observational data contain inherent systematics and biases (e.g., the efficiency of measuring a redshift for a galaxy is dependent on its color, redshift and magnitude). Validation of the input and mock catalogs will enable these data sets to be utilized in studies of systematics and the ability to constrain cosmological parameters. \n- _Activities:_ To undertake a detailed validation of catalog data will require the compilation of large ground and space-based observational data sets. For each data set, the inherent selection effects will need to be derived or measured and the source properties corrected for these effects. The observed source properties will be compared with the mock catalogs and outputs of semi-analytic and empirical models in order to characterize the capabilities and limitations of the catalogs (and to determine what corrections are require to mitigate these limitations). \n- _Deliverables:_ A data set will be delivered, consisting of the observational properties of stars, galaxies and transients sources, compiled from existing ground and space based observations; the source properties will also be characterized in terms of their distribution functions (as a function of redshift, magnitude, etc); the comparison and validation procedures will be automated to enable them to be applied to the multiple mock catalogs generated by the DESC. This work will support the high priority tasks: Weak Lensing (Tasks 5.1.1:H-2 and 5.1.1:H-4), LSS (Tasks 5.2.1:H-1 and 5.2.1:H-2), Supernovae (Task 5.3.1:H-3), Clusters (Task 5.4.1:H-1), Strong lensing (Tasks 5.5.1:H-1 and 5.5.1:H-3), Theory (Task 5.6.1:H-4), Photometric redshifts (Task 5.7.1:H-2), Crosscutting (Tasks 5.8.1:H-2 and 5.8.1:H-3), and Technical (Task 5.12.1:H-2). \n**H-3. CatSim: The implementation of errors within the source catalogs** \n- _Motivation:_ Understanding the constraints on cosmology from LSS, SNe and weak lensing will require catalogs of representative subsets of LSST data. Characterizing the impact of systematics on these constraints requires the generation of the uncertainties associated with the measurements. These uncertainties will depend on the astrophysical properties of the sources, the nature of the observations and the nature of the LSST system itself. Of particular importance will be the impact of non-Gaussian tails within these distributions). \n- _Activities:_ Multiple runs of the image simulator will be generated for a broad range of astronomical and observing conditions in order to characterize the uncertainties on the photometric, astrometric and shape properties. These distributions will be propagated to the catalog framework to enable the generation of large-scale catalogs with appropriate error distributions. \n- _Deliverables:_ Distribution functions that characterize uncertainties in the astrometric, photometric, and shape performance of the LSST system will be delivered through this task. The distributions will be characterized as a function of survey properties (e.g., airmass, \n119 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \nflexure in the instrument, signal-to-noise, etc), with an emphasis on non-Gaussian properties. This work will support weak lensing (Task 5.1.1:H-2), LSS (Tasks 5.2.1:H-1 and 5.2.1:H-2), Clusters (Task 5.4.1:H-1), Strong lensing (Tasks 5.5.1:H-1 and 5.5.1:H-3), Theory (Task 5.6.1:H-4), Photometric redshifts (Task 5.7.1:H-2), Crosscutting (Tasks 5.8.1:H2 and 5.8.1:H-3), and Technical (Task 5.12.1:H-2). \n## **5.9.3 Photon Simulator Improvements** \nAs described in Section 4.1.1, the photon simulator was designed to help determine if the LSST hardware and data management software achieve nominal performance goals, but in many cases this is not necessarily sufficient for high precision dark energy measurements. In addition, its detailed use among many users having a wide variety of scientific goals was not the initial focus. Therefore, to achieve the analysis objectives described previously, we must (1) improve the usability of the code for both individual and large-scale grid computing applications, (2) improve the physics fidelity for detailed dark energy measurement studies, (3) validate the simulator for dark energy measurement use, and (4) connect dark energy systematics to simulation physics. \n## _**High-Priority Tasks**_ \n## **H-1. PhoSim: Photon simulation usability improvements** \n- _Motivation:_ Support for the large number of users of the photon simulator in the collaboration and the wide variety of use cases involved entails a significant development effort. We anticipate that individuals will run the code on their laptops, as well as needing large-scale runs on grid computing platforms. The simulator is already capable of running on a wide variety of grid computing systems, such as the Open Science Grid via CONDOR scripts. However, the previous work all centered around using the simulator in single, monolithic and approximately annual data challenges. Since the development of the analysis efforts in the DESC requires rapid prototyping and simulations in a variety of different contexts, we will need to facilitate the simulator’s use by many users in a grid computing environment. This task is necessary to begin the wide range of analysis tasks using the simulator. \n- _Activities:_ In order to improve the photon simulator for individual use, we plan to streamline the installation and remove other hinderances to widespread deployment. We will maintain a released source code respository in the LSST DESC, and pre-compile binaries for various platforms. We also plan to develop a subset of power users that will become proficient at using the code in a complex manner and be aware of major development changes and limitations. We will develop a series of cookbooks for its detailed use. We will also have all DESC users obtain access to the base Universe catalog, as well as make it simple to ingest user-defined catalogs. As we build up the user community for the photon simulator, we expect to reap benefits in functionality (as users provide ideas for enhancements) and robustness (as users identify limitations and errors in the implementation). We expect the simulator will improve significantly from this active user involvement. We expect a large variety of simulations because the scale of the \n120 \n_5.9 Simulations improvements_ \nsimulation and the photon simulator and astrophysical catalog physics will be different for many applications. Therefore, we will install and maintain release versions of the simulator on one or more grid computing submission sites. We will build a user request simulation system that interfaces with these grid computing sites. We expect significant challenges in developing an efficient monitoring system and fully connecting the input and outputs of catalog, photon simulator, and analysis codes. We also plan to maintain an organized and user-friendly storage system for the simulation and analysis outputs. \n- _Deliverables:_ New simulator versions every three months, with code that is more useable for both grid computing and individual applications; complete simulator documentation. \n## **H-2. PhoSim: Photon simulation fidelity improvements** \n- _Motivation:_ In order to improve the fidelity of the code, we will pursue an ambitious development plan. There are several major physics components that need improvement that we know may affect dark energy science. In general, all dark energy science tasks require updates to the relevant physics, which affects both the photometric response (simulated by whether photons make it to the detector) and the point-spread-function size, shape, and astrometric scale (simulated by the particular path of the photon). The ambitious tasks described for Weak Lensing (Tasks 5.1.1:H-1, 5.1.1:H-2, and 5.1.1:H3), Large Scale Structure (Tasks 5.2.1:H-1 and 5.2.1:H-2), Supernovae (Task 5.3.1:H-3), Clusters (Tasks 5.4.1:H-1 and 5.4.1:H-2), Strong Lensing (Task 5.5.1:H-2), Crosscutting (Tasks 5.8.1:H-1 and 5.8.1:H-4) and Technical (Tasks 5.12.1:H-1 and 5.12.1:H-3) require us to improve and assess simulation fidelity for each case. We need to begin this task to be able to start with detailed analysis tasks covering DE systematics. As discussed in Section 4.1.1, the main advantage of using a physics-based simulation is that the detailed physical models of the atmosphere, telescope, and camera can be connected to the astrophysical dark energy measurements. Thus, we expect an active synergy between analysis and simulations, in which the particular limitation of an algorithm might be connected to a particular physical aspect of the simulations. In some cases, this could allow us to build a better algorithm that exploits the particular physical form of the effect; in other cases, we might be able to remove or weight particular exposures for variable effects; in yet other cases, we would simply reach the systematic floor because the effect cannot be removed. \n- _Activities:_ There are several areas of fidelity of the simulator that we already know can be improved for higher-fidelity simulations. These are particularly well-suited for the DESC, where the exact details of the PSF and photometric response matter for dark energy science. One area is the perturbations to the optical elements that are currently set by tolerances and estimates, but not based on a physical model that predicts the perturbations from thermal drifts and the change in gravity vectors from the telescope motion. In addition, the control system feedback loop attempts to compensate for these perturbations. Much of the modeling for this has already been developed by the camera, telescope, and wavefront sensing teams, but we have not collected this into a complete physical model. Another area for improvement is the physics of the multilayers, which can be properly formulated through an EM calculation of the transmission/reflection through the layers. Sensor defects are another large area for improvement that will \n121 \n## _Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Deliverables:_ Release of a new version photon simulator code every three months with improved fidelity; a matrix connecting irreducible physical effects to key dark energy systematics. \n## **H-3. PhoSim: Photon simulation validation** \n- _Deliverables:_ The small scale simulator will be delivered as a core part of the software analysis framework. \n## **H-2. SW: Requirements for software framework** \n123 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Motivation:_ The software framework will be developed to address the needs of all science working groups. However, scientists are not necessarily well-versed in framework development, so a collaboration of computer professionals and scientists will work together to generate requirements for the framework. As the needs of scientists evolve, changes in requirements will be managed to address these needs. \n- _Activities:_ Develop the requirements for the software framework based on the use patterns of the analysis group. Every WG will develop use cases in conjunction with computing professionals. The collaboration will then define a list of packages to be included in the framework, a set of dependencies, and guidance for usage by the scientists. A series of workshops will be run in order to ensure the face-to-face time needed for this collaborative work. \n_Deliverables:_ A set of requirements for the software framework to be used by the DESC. \n- **H-3. SW: Level 3 software framework** \n   - _Motivation:_ A Level 3 software framework will enable the science WG’s to assess the significance of the systematics under consideration. The collaboration will then be able to rank systematics based on quantitative apples-to-apples comparisons of their effects on dark energy constraints and to make educated choices about which algorithms to use to address every systematic effect considered. \n   - _Activities:_ During the development phase of the project, these tools will look very different than the final versions that will be used during the production phase. However, even at this early stage, the Working Groups will need a uniform, robust way to quantify the effects of systematics and the algorithms used to mitigate them. The challenge is to build this toolkit in close coordination with the WG’s, starting with a relatively simple, yet robust, set of modules, and gradually incrementing, improving upon these, keeping careful track of the different versions. Analysts will have access to all code developed by members of the collaboration and to previous systematics tests. \n   - _Deliverables:_ Over the next 3 years, the Level 3 toolkit will start with: a suite of two-point function estimators; simple analytic covariance matrices; a theory code such as camb; and a Fisher estimator of bias and errors. These will gradually be extended. \n- **H-4. SW: Repository for code and data** \n   - _Activities:_ The repository will be established so that it is easily accessible to everyone in the collaboration, and the various WG’s will be prompted to add analysis code. Data from simulations will likely be housed in data stores, but the framework will link these so users can import both and operate on any data set with any set of code. External data sets, such as WMAP, Planck, DES, will also be easy to import when conducting analyses. \n124 \n_5.11 Computing model_ \n- _Deliverables:_ The deliverable is a documented repository with easily categorized modules for analysis, linked to other stores and repositories that contain external data sets and code packages. \n## **5.11 Development of a computing model** \n## **5.11.1 High-Priority Tasks** \n## **H-1. CM: Defining and updating computing models** \n- _Motivation:_ There will be a ramp of computing needs ranging from the relatively modest needs for the early studies through to significant resources needed for Level 3 reprocessings of large chunks of observatory data. We need to understand the roadmap of needs over a 10+ year period to ensure the necessary resources can be obtained and funded. \n- _Activities:_ We will collaborate with the working groups, via workshops and other discussions, to build a model of needs vs time. This will require understanding their activities as well as tracking usage to assist in realistic projections. \n- _Deliverables:_ We will provide projections of significant cycle and storage needs spanning a running 10 year period. These roadmaps would be refreshed annually. \n## **H-2. CM: Providing computing resources to the working groups** \n- _Motivation:_ As described in Task 5.11.1:H-1, a profile of computing resources will be needed over the collaboration’s life. In addition to planning for the resources levels, we will need to provision the collaboration for mid-range cycles and storage at each phase in time. Workflow engines and data catalogs and access tools will be needed. Additionally the usual operational resources needed by a collaboration will be required: code repositories, automated builds and quality testing. \n- _Activities:_ A period of understanding working group needs will be needed to start. We will need to address both the scale of computing required along with the ways analyses are done. This will allow us to craft the workflow engines and access tools, as well as the development tools, to suit the needed usage patterns. \n- _Deliverables:_ We will provide early versions of workflow engines and data catalogs, as well as code development environments in the first year. These will need to be iterated as we develop the software framework and understand the analysis patterns. \n## **H-3. CM: Providing storage and infrastructure for catalogs** \n- _Motivation:_ Realistic mock-catalogs and ImSim simulations will require non-trivial storage. It is important to provision for multi-TB databases as well as hundreds of TB of data files. Large databases require significant tuning for efficient data ingest and to maintain query performance, while large collections of files must be distributed appropriately with capable servers, filesystems and networking to survive significant load, minimizing failure rates. \n125 \n_Chapter 5: Detailed Work Plan for the Next Three Years_ \n- _Activities:_ Reasonable scale catalog databases and file collections exist for the LSST project. We will explore these to develop our own tools and techniques to support them. \n- _Deliverables:_ We will deliver well-managed and tuned databases and file systems within the first year or two. We will then need to keep up with increasing volumes of data to ensure that the solutions scale. \n## **5.11.2 Longer-Term Tasks** \n## **LT-1. CM: Providing resources for N-body simulations** \n- _Motivation:_ We anticipate very large n-body simulations runs, requiring significant compute cycles and storage. We need to understand the time profile of such simulations to project resource needs, as well as working with the supercomputing centers for efficient access to the produced data. We will also need to understand what entities will be charged with obtaining the cycles. \n- _Activities:_ We will work with the cosmology simulations experts, both in the collaboration and in external large cosmology collaborations to understand the scales of cycles needed as well as under what auspices they may be obtained. \n- _Deliverables:_ We will create a timeline of resources needed for cosmology simulations and identify the entities who will be charged with delivering the resources for the collaboration. \n## **5.12 Technical coordination** \nThe technical coordination tasks lie at the interface between the DESC science objectives and the LSST Project’s technical and engineering team. The success of the Dark Energy programs with LSST depend on a combination of a deep knowledge of the apparatus and software, and a clarity of scientific focus. Each of the priority tasks listed below span the dark energy science goals and the technical aspects of the LSST project, and for diverse reasons they require progress in the near term. The coordination of these tasks and the facilitation of the interaction with the LSST project team will be undertaken by C. Stubbs (DESC Technical Coordinator) and J. A. Tyson. \n## **5.12.1 High-Priority Tasks** \n## **H-1. TC: Wavefront sensing and pointspread function characterization** \n- _Motivation:_ The corner rafts of the LSST focal plane each contain a wavefront sensor, which obtains simultaneous images on either side of the telescope’s focal surface. These sensors are used to measure the shape of the arriving wavefront at these positions. This information is important for two reasons: First, we use it to make an ongoing set of quasi-static corrections to the optical train, especially the shape of the primary mirror. Second, the out-of-focus images provide a means to measure the in-focus point spread \n126 \n_5.12 Technical coordination_ \nfunction on the focal plane. The determination of the point spread function and its variation across the field is essential for the weak lensing analysis, and can also facilitate the determination of the optimal frame subtraction kernel. \n- _Activities:_ Working with the Project, we will study the implementation of the out-of-focus wavefront sensors, the choice of their spacing, the development and validation of wavefront characterization algorithms, and the incorporation of this information into the downstream weak lensing shape analysis and frame subtraction photometry (for SN measurements). This work will involve collaboration between the technical experts on these subsystems working on the Project, and representatives from each of the analysis working groups who can assess the scientific impacts of various design choices. \n- _Deliverables:_ Informal feedback to the Project, plus detailed scientific assessments of itemized design choices. \n## **H-2. TC: Assessment of the approach toward photometric calibration** \n- _Motivation:_ Achieving precise flux measurements is essential to all aspects of LSST dark energy science. At present, band-to-band photometric normalization uncertainties dominate the systematic error budget for Type Ia supernova cosmology. Moreover, obtaining precise photometric redshift measurements requires a consistent and precise definition of flux measurements across the entire survey area. \n- _Activities:_ The LSST project is in the final design stage for both the overall photometric calibration philosophy and its implementation in both hardware and software. The choices include how to best measure the focal-plane-position–dependent instrumental sensitivity function _S_ ( _λ, x, y_ ) for the various LSST passbands, how to best determine the optical transmission function of the atmosphere _A_ ( _λ_ ), and how to determine and correct for Galactic extinction _G_ ( _λ, α, δ_ ) at right ascension _α_ and declination _δ_ . The DESC team will bring both experience and insight to bear on the assessment of this calibration strategy, and help inform the design choices. DESC scientists will be fully engaged in the assessment and refinement of the LSST calibration plan, and with testing, validating, and implementing the LSST calibration approach using a combination of ground-based and space-based observations. \n- _Deliverables:_ Informal feedback to the Project, plus detailed assessment of expected residual photometric calibration errors, and their impact on dark energy science. \n## **H-3. TC: Instrument model development** \n- _Motivation:_ The “instrument model” is a predictive tool, or collection of predictive tools, that can be used to quantitatively estimate the response of the LSST system to monochromatic radiation at an arbitrary wavelength emanating from an arbitrary position on the sky with respect to the optical axis. It will comprise our complete understanding of all physical effects within the camera and telescope that can contribute to the imaging and throughput performance of the system. An effort will be made to embed as much of the instrument model as possible into the photon simulator, however it may prove more convenient and/or feasible to also utilize other computational approaches. The instrument model will evolve as we acquire additional calibration and characterization data on the \n127 \n## _Chapter 5: Detailed Work Plan for the Next Three Years_ \nas-built components. Clearly, our understanding of subtle systematics affecting dark energy analysis will be critically dependent on the fidelity of this instrument model. \n   - _Activities:_ The requirements for dark energy analyses present the most stringent requirements we currently know of for our knowledge of the system behavior. Therefore, the DESC will play a driving role in working with the project to define a set of measurement and analysis requirements for component level testing. Experts from the hardware teams will work with representatives from the analysis working groups to make these determinations. Continuing assessments will be made of the extent to which modifications can and should be made to the photon simulator to reflect our evolving knowledge of the system. \n   - _Deliverables:_ Informal feedback to the Project, along with detailed scientific assessment of impacts associated with identified physical effects. \n- **H-4. TC: Cadence and operational simulation, and optimization of main and deepdrilling surveys** \n   - _Motivation:_ The decisions of where to point the LSST telescope, in what filter to expose in, how long to expose for, and and how frequently to return to a given field, are the knobs that will be adjusted during system operations. We need to understand, well in advance, how to best make these choices in order to maximize the dark energy science impact of LSST. Since the extracted constraints on the properties of the dark energy depend upon both the image archive and the data analysis algorithms, the DESC team will play a vital role in learning how to optimize operation of the LSST system. \n   - _Activities:_ The DESC will assess observing strategies that balance the tensions between the operating modes that are deemed optimal for different dark energy probes. The supernova technique favors repeated observations of a few fields, while the weak lensing approach favors wide field coverage. We will explore and understand these tradeoffs and strive to understand the operational approach that will best use LSST for the understanding of the dark energy. \n   - _Deliverables:_ A cadence toolkit for optimizing dark energy science that can be delivered to the Project for assessment against comparable toolkits delivered by collaborations working on other science topics. \n128 \n_Chapter 5: References_ \n## **References** \nAbdalla, F. B., Amara, A., Capak, P., Cypriano, E. S., Lahav, O., & Rhodes, J., 2008, Monthly Notices of the Royal Astronomical Society, 387, 969 Abell, P. A. et al., 2009, 0912.0201 Acquaviva, V., & Gawiser, E., 2010, _Phys. Rev. D_ , 82, 082001 Agertz, O. et al., 2007, _MNRAS_ , 380, 963 Albrecht, A., & Bernstein, G., 2007, _Phys. Rev. D_ , 75, 103003 Albrecht, A. et al., 2006, ArXiv Astrophysics e-prints, arXiv:astro-ph/0609591 Alimi, J.-M. et al., 2012, ArXiv e-prints, 1206.2838 Allen, S. W., Evrard, A. E., & Mantz, A. B., 2011, _ARAA_ , 49, 409 Anderson, L. et al., 2012, ArXiv e-prints, 1203.6594 Applegate, D. E. et al., 2012, ArXiv e-prints, 1208.0605 Astier, P. et al., 2006, _A&A_ , 447, 31 Bailey, S. et al., 2009, _A&A_ , 500, L17 Baldauf, T., Seljak, U., Senatore, L., & Zaldarriaga, M., 2011, _JCAP_ , 10, 31 Baldauf, T., Smith, R. E., Seljak, U., & Mandelbaum, R., 2010, Phys.Rev., D81, 063531 Barone-Nugent, R. L. et al., 2012, ArXiv e-prints, 1204.2308 Bartelmann, M., 2010, preprint (arXiv:1010.3829), 1010.3829 Bartelmann, M., & Schneider, P., 2001, _Phys. Rep._ , 340, 291 Baugh, C. M., 2006, Reports on Progress in Physics, 69, 3101 Becker, M. R., & Kravtsov, A. V., 2011, _ApJ_ , 740, 25 Behroozi, P. S., Wechsler, R. H., & Wu, H.-Y., 2011, ArXiv e-prints, 1110.4372 Ben´ıtez, N., 2000, _ApJ_ , 536, 571 Benson, A. J., 2012, New Astron., 17, 175 Bernstein, G., & Huterer, D., 2009, ArXiv e-prints, 0902.2782 Bernstein, G. M., 2010a, _MNRAS_ , 406, 2793 —, 2010b, _MNRAS_ , 406, 2793 Bernstein, G. M., & Jarvis, M., 2002, _AJ_ , 123, 583 Bertacca, D., Maartens, R., Raccanelli, A., & Clarkson, C., 2012, JCAP, 1210, 025 Blandford, R. D., & Narayan, R., 1992, _ARAA_ , 30, 311 Bleem, L. E. et al., 2012, Astrophysical Journal Letters Blondin, S. et al., 2012, _AJ_ , 143, 126 Bonamente, M., Joy, M. K., LaRoque, S. J., Carlstrom, J. E., Reese, E. D., & Dawson, K. S., 2006, _ApJ_ , 647, 25 Bonvin, C., & Durrer, R., 2011, Physical Review D Borgani, S., & Kravtsov, A., 2011, Advanced Science Letters, 4, 204 Bruni, M., Crittenden, R., Koyama, K., Maartens, R., Pitrou, C., & Wands, D., 2012a, Phys. Rev. D, 85, 041301 —, 2012b, Physical Review D Budav´ari, T., 2009, _ApJ_ , 695, 747 Burud, I. et al., 2002, _A&A_ , 391, 481 Busha, M., & Wechsler, R., 2008, Proceedings, Cosmology 2008 (43rd Rencontres de Moriond) Carlson, J., White, M., & Padmanabhan, N., 2009, Phys.Rev., D80, 043531 Catelan, P., Kamionkowski, M., & Blandford, R. D., 2001, _MNRAS_ , 320, L7 Challinor, A., & Lewis, A., 2011, Physical Review D Chen, X., & Wang, Y., 2010, _JCAP_ , 4, 27 Coe, D., & Moustakas, L., 2009, ArXiv e-prints, 0906.4108 Cole, S., Hatton, S., Weinberg, D. H., & Frenk, C. S., 1998, _MNRAS_ , 300, 945 Collett, T. E., Auger, M. W., Belokurov, V., Marshall, P. J., & Hall, A. C., 2012, _MNRAS_ , 3398 Connolly, A. J. et al., 2010, in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 7738, Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Conroy, C., Wechsler, R. H., & Kravtsov, A. V., 2006, _ApJ_ , 647, 201 Copeland, E. J., 2007, in American Institute of Physics Conference Series, Vol. 957, Particles, Strings, and CosmologyPASCOS 2007, A. Rajantie, C. Contaldi, P. Dauncey, & H. Stoica, eds., pp. 21–29 Corless, V. L., & King, L. J., 2009, _MNRAS_ , 396, 315 Courbin, F. et al., 2011, _A&A_ , 536, A53 Cunha, C. E., Huterer, D., Busha, M. T., & Wechsler, R. H., 2012a, _MNRAS_ , 423, 909 \n129 \n## _Chapter 5: References_ \n—, 2012b, _MNRAS_ , 423, 909 Cunha, C. E., Huterer, D., Lin, H., Busha, M. T., & Wechsler, R. H., 2012c, ArXiv e-prints, 1207.3347 Dalal, N., Dor´e, O., Huterer, D., & Shirokov, A., 2008, Phys. Rev. D, 77, 123514 Dalal, N., Hennawi, J. F., & Bode, P., 2005, _ApJ_ , 622, 99 D’Aloisio, A., & Natarajan, P., 2011, _MNRAS_ , 411, 1628 D’Andrea, C. B. et al., 2011, _ApJ_ , 743, 172 Das, S. et al., 2011, Phys.Rev.Lett., 107, 021301 De Lucia, G., Springel, V., White, S. D. M., Croton, D., & Kauffmann, G., 2006, _MNRAS_ , 366, 499 Di Matteo, T., Khandai, N., DeGraf, C., Feng, Y., Croft, R. A. C., Lopez, J., & Springel, V., 2012, _ApJL_ , 745, L29 Dolence, J., & Brunner, R. J., 2007, in The 9th LCI International Conference on High-Performance Clustered Computing Eisenstein, D. J. et al., 2005a, _ApJ_ , 633, 560 —, 2005b, _ApJ_ , 633, 560 Ellis, R. et al., 2012, ArXiv e-prints, 1206.0737 Fadely, R., Hogg, D. W., & Willman, B., 2012, ArXiv e-prints, 1206.4306 Fang, W., & Haiman, Z., 2007, _Phys. Rev. D_ , 75, 043010 Fassnacht, C. D., Koopmans, L. V. E., & Wong, K. C., 2011a, _MNRAS_ , 410, 2167 —, 2011b, _MNRAS_ , 410, 2167 Fedeli, C., Carbone, C., Moscardini, L., & Cimatti, A., 2011, _MNRAS_ , 414, 1545 Foley, R. J., Sanders, N. E., & Kirshner, R. P., 2011, _ApJ_ , 742, 89 Frieman, J. A., Turner, M. S., & Huterer, D., 2008, _ARAA_ , 46, 385 Fryxell, B. et al., 2000, _ApJS_ , 131, 273 Garilli, B. et al., 2008, _A&A_ , 486, 683 Gavazzi, R., Treu, T., Koopmans, L. V. E., Bolton, A. S., Moustakas, L. A., Burles, S., & Marshall, P. J., 2008, _ApJ_ , 677, 1046 Gilmore, J., & Natarajan, P., 2009, _MNRAS_ , 396, 354 Gupta, R. R. et al., 2011, _ApJ_ , 740, 92 Guy, J. et al., 2007, _A&A_ , 466, 11 Habib, S., Heitmann, K., Higdon, D., Nakhleh, C., & Williams, B., 2007, _Phys. Rev. D_ , 76, 083503 Habib, S. et al., 2009, Journal of Physics Conference Series, 180, 012019 Hamaus, N., Seljak, U., Desjacques, V., Smith, R. E., & Baldauf, T., 2010, Phys.Rev., D82, 043515 Hand, N. et al., 2012a, Phys. Rev. Lett., 109, 041101 —, 2012b, Phys.Rev.Lett., 109, 041101 Hayes, B., Brunner, R., & Ross, A., 2012, _MNRAS_ , 421, 2043 Hayes, B., & Brunner, R. J., 2012, ”submitted to MNRAS” Hearin, A. P., & Zentner, A. R., 2009, _JCAP_ , 4, 32 Hearin, A. P., Zentner, A. R., & Ma, Z., 2012a, _JCAP_ , 4, 34 —, 2012b, _JCAP_ , 4, 34 Hearin, A. P., Zentner, A. R., Ma, Z., & Huterer, D., 2010, _ApJ_ , 720, 1351 Heitmann, K., Higdon, D., Nakhleh, C., & Habib, S., 2006, _ApJL_ , 646, L1 Heitmann, K., Higdon, D., White, M., Habib, S., Williams, B. J., Lawrence, E., & Wagner, C., 2009, _ApJ_ , 705, 156 Heitmann, K. et al., 2008, Computational Science and Discovery, 1, 015003 Heitmann, K., Ricker, P. M., Warren, M. S., & Habib, S., 2005, _ApJS_ , 160, 28 Hikage, C., Takada, M., & Spergel, D. N., 2012, Mon.Not.Roy.Astron.Soc., 419, 3457 Hilbert, S., Hartlap, J., White, S. D. M., & Schneider, P., 2009, _A&A_ , 499, 31 Hillebrandt, W., & Niemeyer, J. C., 2000, _ARAA_ , 38, 191 Hirata, C., & Seljak, U., 2003, _MNRAS_ , 343, 459 Hirata, C. M., Ho, S., Padmanabhan, N., Seljak, U., & Bahcall, N. A., 2008, Physical Review D Hirata, C. M., & Seljak, U. c. v., 2004, Phys. Rev. D, 70, 063526 Hlozek, R. et al., 2012, _ApJ_ , 752, 79 Ho, S. et al., 2012, ArXiv e-prints, 1201.2137 Ho, S., Lin, Y. T., Spergel, D., & Hirata, C. M., 2009, Astrophysical Journal, 697, 1358 Huff, E. M., Hirata, C. M., Mandelbaum, R., Schlegel, D., Seljak, U., & Lupton, R. H., 2011, ArXiv e-prints, 1111.6958 Huterer, D., & Takada, M., 2005, Astroparticle Physics, 23, 369 Ilbert, O. et al., 2009, _ApJ_ , 690, 1236 \n130 \n_Chapter 5: References_ \nIvezi´c, Z., & the LSST Science Collaboration, 2011, Large Synoptic Survey Telescope (LSST) Science Requirements Document. `http://www.lsst.org/files/docs/SRD.pdf` , [Online; accessed 29-October-2012] Ivezic, Z., Tyson, J., Allsman, R., Andrew, J., & Angel, R., 2008, 0805.2366 Jain, B., 2002, _ApJL_ , 580, L3 Jain, B., Connolly, A., & Takada, M., 2007, _JCAP_ , 3, 13 Jeong, D., Schmidt, F., & Hirata, C. M., 2012, Physical Review D Jha, S., Riess, A. G., & Kirshner, R. P., 2007, _ApJ_ , 659, 122 Jing, Y. P., Zhang, P., Lin, W. P., Gao, L., & Springel, V., 2006, _ApJL_ , 640, L119 Jullo, E., Natarajan, P., Kneib, J.-P., D’Aloisio, A., Limousin, M., Richard, J., & Schimd, C., 2010, Science, 329, 924 Juri´c, M. et al., 2008, _ApJ_ , 673, 864 Kaiser, N., 2000, _ApJ_ , 537, 555 Kelly, P. L., Hicken, M., Burke, D. L., Mandel, K. S., & Kirshner, R. P., 2010, _ApJ_ , 715, 743 Kelly, P. L. et al., 2012, ArXiv e-prints, 1208.0602 Kessler, R. et al., 2009, _ApJS_ , 185, 32 Kiessling, A., Heavens, A. F., Taylor, A. N., & Joachimi, B., 2011, _MNRAS_ , 414, 2235 Kirk, D., Rassat, A., Host, O., & Bridle, S., 2012, _MNRAS_ , 1112.4752 Kitching, T. D., Miller, L., Heymans, C. E., van Waerbeke, L., & Heavens, A. F., 2008, _MNRAS_ , 390, 149 Kochanek, C. S., 2004, _ApJ_ , 605, 58 Kochanek, C. S., Dai, X., Morgan, C., Morgan, N., & Poindexter, G., S. C., 2007, in Astronomical Society of the Pacific Conference Series, Vol. 371, Statistical Challenges in Modern Astronomy IV, G. J. Babu & E. D. Feigelson, eds., p. 43 Kochanek, C. S., Mochejska, B., Morgan, N. D., & Stanek, K. Z., 2006, _ApJL_ , 637, L73 Komatsu, E., 2010, Classical and Quantum Gravity, 27, 124010 Koo, D. C., 1999, in Astronomical Society of the Pacific Conference Series, Vol. 191, Photometric Redshifts and the Detection of High Redshift Galaxies, R. Weymann, L. Storrie-Lombardi, M. Sawicki, & R. Brunner, eds., pp. 3–+ Kratochvil, J. M., Haiman, Z., & May, M., 2010, _Phys. Rev. D_ , 81, 043519 Krisciunas, K., Phillips, M. M., & Suntzeff, N. B., 2004, _ApJL_ , 602, L81 Krisciunas, K., & Schaefer, B. E., 1991, _PASP_ , 103, 1033 Kundi´c, T. et al., 1997, _ApJ_ , 482, 75 Lampeitl, H. et al., 2010, _ApJ_ , 722, 566 Landy, S. D., & Szalay, A. S., 1993, 412, 64 Laszlo, I., Bean, R., Kirk, D., & Bridle, S., 2011, 1109.4535 Laszlo, I., Bean, R., Kirk, D., & Bridle, S., 2012, _MNRAS_ , 423, 1750 Lawrence, E., Heitmann, K., White, M., Higdon, D., Wagner, C., Habib, S., & Williams, B., 2010a, _ApJ_ , 713, 1322 —, 2010b, _ApJ_ , 713, 1322 Le F`evre, O. et al., 2005, _A&A_ , 439, 845 Lesgourgues, J., & Pastor, S., 2006, _Phys. Rep._ , 429, 307 Lewis, A., & Bridle, S., 2002, _Phys. Rev. D_ , 66, 103511 Lewis, A., Challinor, A., & Lasenby, A., 2000, _ApJ_ , 538, 473 Lilly, S. J. et al., 2009, _ApJS_ , 184, 218 Linder, E. V., 2011, _Phys. Rev. D_ , 84, 123529 Lombriser, L., 2011, _Phys. Rev. D_ , 83, 063519 LSST Science Collaborations et al., 2009a, ArXiv e-prints, 0912.0201 —, 2009b, ArXiv e-prints, 0912.0201 Ma, Z., Hu, W., & Huterer, D., 2006, _ApJ_ , 636, 21 Maartens, R., Zhao, G.-B., Bacon, D., Koyama, K., & Raccanelli, A., 2012, 1206.0732 Mantz, A., Allen, S. W., & Rapetti, D., 2010a, _MNRAS_ , 406, 1805 Mantz, A., Allen, S. W., Rapetti, D., & Ebeling, H., 2010b, _MNRAS_ , 406, 1759 Marshall, P. J., Hogg, D. W., Moustakas, L. A., Fassnacht, C. D., Bradaˇc, M., Schrabback, T., & Blandford, R. D., 2009, _ApJ_ , 694, 924 Matsubara, T., 1999, Astrophys.J., astro-ph/9908056 Melchior, P., & Viola, M., 2012, _MNRAS_ , 3383 Melchior, P., B¨ohnert, A., Lombardi, M., & Bartelmann, M., 2010, A&A, 510, A75 Miller, L., Kitching, T. D., Heymans, C., Heavens, A. F., & van Waerbeke, L., 2007, _MNRAS_ , 382, 315 Moore, A. W. et al., 2001, in Mining the Sky, A. J. Banday, S. Zaroubi, & M. Bartelmann, ed., pp. 71–+ \n131 \n## _Chapter 5: References_ \nMorgan, C. W., Kochanek, C. S., Morgan, N. D., & Falco, E. E., 2010, _ApJ_ , 712, 1129 Newman, J. A., 2008a, _ApJ_ , 684, 88 —, 2008b, _ApJ_ , 684, 88 Newman, J. A. et al., 2012, ArXiv e-prints, 1203.3192 Nishizawa, A. J., Takada, M., Hamana, T., & Furusawa, H., 2010, _ApJ_ , 718, 1252 Noh, Y., & Cohn, J. D., 2012, ArXiv e-prints, 1204.1577 Oguri, M., 2007, _ApJ_ , 660, 1 Oguri, M. et al., 2006, _AJ_ , 132, 999 Oguri, M., & Marshall, P. J., 2010, _MNRAS_ , 405, 2579 Oguri, M., & Takada, M., 2011, Phys.Rev., D83, 023008 Overzier, R., Lemson, G., Angulo, R. E., Bertin, E., Blaizot, J., Henriques, B. M. B., Marleau, G.-D., & White, S. D. M., 2012a, ArXiv e-prints, 1206.6923 —, 2012b, ArXiv e-prints, 1206.6923 Padmanabhan, N. et al., 2007, _MNRAS_ , 378, 852 Papai, P., & Szapudi, I., 2008, Monthly Notices of the Royal Astronomical Society, 389, 292 Paulin-Henriksson, S., Refregier, A., & Amara, A., 2009, _A&A_ , 500, 647 Peacock, J. A., & Smith, R. E., 2000, _MNRAS_ , 318, 1144 Peebles, P. J. E., 1980, The large-scale structure of the universe. Princeton University Press, 1980. 435p. Perlmutter, S. et al., 1999, _ApJ_ , 517, 565 Press, W. H., Rybicki, G. B., & Hewitt, J. N., 1992, _ApJ_ , 385, 416 Raccanelli, A., Samushia, L., & Percival, W. J., 2010, Monthly Notices of the Royal Astronomical Society, 409, 1525 Rapetti, D., Allen, S. W., & Mantz, A., 2008, _MNRAS_ , 388, 1265 Rapetti, D., Blake, C., Allen, S. W., Mantz, A., Parkinson, D., & Beutler, F., 2012, ArXiv e-prints, 1205.4679 Refregier, A., 2003, _ARAA_ , 41, 645 Refsdal, S., 1964, _MNRAS_ , 128, 307 Reid, B. A., Verde, L., Jimenez, R., & Mena, O., 2010, _JCAP_ , 1, 3 Reyes, R. et al., 2010, Nature, 464, 256 Richards, G. T. et al., 2006, _ApJS_ , 166, 470 Riess, A. G. et al., 1998, _AJ_ , 116, 1009 Ross, A. J., Brunner, R. J., & Myers, A. D., 2007, _ApJ_ , 665, 67 Ross, A. J., Brunner, R. J., Myers, A. D., & et al., 2006, 649, 48 Rudd, D. H., Zentner, A. R., & Kravtsov, A. V., 2008, _ApJ_ , 672, 19 Rykoff, E. S. et al., 2008, _MNRAS_ , 387, L28 —, 2012, _ApJ_ , 746, 178 Sako, M. et al., 2011, _ApJ_ , 738, 162 Samushia, L., Percival, W. J., & Raccanelli, A., 2012, Monthly Notices of the Royal Astronomical Society, 420, 2102 Samushia, L. et al., 2012, ArXiv e-prints, 1206.5309 Sanchez, A. G., & Cole, S., 2008, Monthly Notices of the Royal Astronomical Society, 385, 830 Sanchez, A. G., Crocce, M., Cabre, A., Baugh, C., & Gaztanaga, E., 2009, 0901.2570 Sartoris, B., Borgani, S., Rosati, P., & Weller, J., 2012, _MNRAS_ , 423, 2503 Scannapieco, C. et al., 2012, _MNRAS_ , 423, 1726 Schneider, M., Knox, L., Zhan, H., & Connolly, A., 2006a, _ApJ_ , 651, 14 Schneider, M. D., Cole, S., Frenk, C. S., & Szapudi, I., 2011, _ApJ_ , 737, 11 Schneider, P., 2006, in Gravitational Lensing: Strong, Weak and Micro, Saas-Fee Advanced Courses, Schneider, P., Kochanek, C. S., & Wambsganss, J., ed., Vol. 33, Springer-Verlag Berlin Heidelberg, 2006, pp. 1–+ Schneider, P., Kochanek, C. S., & Wambsganss, J., 2006b Scranton, R., Johnston, D., Dodelson, S., Frieman, J. A., Connolly, A., Eisenstein, D. J., Gunn, J. E., & et al., 2002, 579, 48 Seljak, U., 1994, _ApJ_ , 436, 509 —, 2000, _MNRAS_ , 318, 203 Semboloni, E., Hoekstra, H., Schaye, J., van Daalen, M. P., & McCarthy, I. G., 2011, _MNRAS_ , 417, 2020 Seo, H.-J., & Eisenstein, D. J., 2003, _ApJ_ , 598, 720 Seo, H.-J. et al., 2012, ArXiv e-prints, 1201.2172 Sherwin, B. D. et al., 2012, 1207.4543 Silverman, J. M., Ganeshalingam, M., Li, W., & Filippenko, A. V., 2012, ArXiv e-prints, 1202.2130 Skibba, R. A., & Sheth, R. K., 2009, _MNRAS_ , 392, 1080 \n132 \n_Chapter 5: References_ \nSmith, K. M., Zahn, O., & Dore, O., 2007, Physical Review D Smith, R. et al., 2003, Mon.Not.Roy.Astron.Soc., 341, 1311 Song, J., Mohr, J. J., Barkhouse, W. A., Warren, M. S., Dolag, K., & Rude, C., 2012, _ApJ_ , 747, 58 Song, Y. S., & Knox, L., 2004, Physical Review D Spillar, E. J., 1985, PhD thesis, AA(Princeton Univ., NJ.) Springel, V., 2005, _MNRAS_ , 364, 1105 —, 2012, Astronomische Nachrichten, 333, 515 Springel, V. et al., 2005, _Nature_ , 435, 629 Stanek, R., Rudd, D., & Evrard, A. E., 2009, _MNRAS_ , 394, L11 Sullivan, M. et al., 2010, _MNRAS_ , 406, 782 Suyu, S. H., Marshall, P. J., Auger, M. W., Hilbert, S., Blandford, R. D., Koopmans, L. V. E., Fassnacht, C. D., & Treu, T., 2010, _ApJ_ , 711, 201 Suyu, S. H. et al., 2012, ArXiv e-prints, 1202.4459 Suzuki, N. et al., 2012, _ApJ_ , 746, 85 Szalay, A. S., Matsubara, T., & Landy, S. D., 1998, Astrophysical Journal, 498, L1 Szapudi, I., 2004, Astrophysical Journal, 614, 51 Tegmark, M. et al., 2002, _ApJ_ , 571, 191 Tewes, M. et al., 2012, ArXiv e-prints, 1208.6009 Trac, H., & Cen, R., 2007, _ApJ_ , 671, 1 Trac, H., & Pen, U.-L., 2006, New Astron., 11, 273 Treu, T., 2010, _ARAA_ , 48, 87 Turk, M. J., Smith, B. D., Oishi, J. S., Skory, S., Skillman, S. W., Abel, T., & Norman, M. L., 2011, _ApJS_ , 192, 9 Tyson, J. A., 2006, ArXiv Astrophysics e-prints, astro-ph/0609516 van Daalen, M. P., Schaye, J., Booth, C. M., & Dalla Vecchia, C., 2011, _MNRAS_ , 415, 3649 van Engelen, A. et al., 2012, Astrophys.J., 756, 142 Vikhlinin, A. et al., 2009, _ApJ_ , 692, 1060 Voigt, L. M., & Bridle, S. L., 2010a, _MNRAS_ , 404, 458 —, 2010b, _MNRAS_ , 404, 458 von der Linden, A. et al., 2012, ArXiv e-prints, 1208.0597 Wang, Y., & Brunner, R. J., 2012, ”submitted to MNRAS” Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J., Hirata, C., Riess, A. G., & Rozo, E., 2012, ArXiv e-prints, 1201.2434 Whitaker, K. E. et al., 2011, _ApJ_ , 735, 86 White, M., 2004, Astroparticle Physics, 22, 211 Wood-Vasey, W. M. et al., 2008, _ApJ_ , 689, 377 Woodring, J., Heitmann, K., Ahrens, J., Fasel, P., Hsu, C.-H., Habib, S., & Pope, A., 2011, _ApJS_ , 195, 11 Wu, H.-Y., Rozo, E., & Wechsler, R. H., 2010, _ApJ_ , 713, 1207 Xu, X., Cuesta, A. J., Padmanabhan, N., Eisenstein, D. J., & McBride, C. K., 2012, ArXiv e-prints, 1206.6732 Yang, X., Kratochvil, J. M., Wang, S., Lim, E. A., Haiman, Z., & May, M., 2011, _Phys. Rev. D_ , 84, 043529 Yang, X., Mo, H. J., & van den Bosch, F. C., 2003, _MNRAS_ , 339, 1057 Yoo, J., 2010, Physical Review D Yoo, J., Fitzpatrick, A. L., & Zaldarriaga, M., 2009, Physical Review D Yoo, J., Hamaus, N., Seljak, U., & Zaldarriaga, M., 2012, Phys.Rev., D86, 063514 Zentner, A. R., Rudd, D. H., & Hu, W., 2008, _Phys. Rev. D_ , 77, 043507 Zhan, H., 2006a, _JCAP_ , 8, 8 —, 2006b, Journal of Cosmology and Astro-Particle Physics, 8, 8 Zhan, H., & Knox, L., 2004, _ApJL_ , 616, L75 —, 2006, _ApJ_ , 644, 663 Zhan, H., Knox, L., & Tyson, J. A., 2009, _ApJ_ , 690, 923 Zhan, H., Wang, L., Pinto, P., & Tyson, J. A., 2008, _ApJL_ , 675, L1 Zhang, P., 2010, _ApJ_ , 720, 1090 Zhang, P., Liguori, M., Bean, R., & Dodelson, S., 2007, Phys. Rev. Lett., 99, 141302 \n133",
  "conclusion": "## **1.3 Planned survey strategy and delivered data products** \nThe LSST observing strategy is designed to maximize scientific throughput by minimizing slew and other downtime and by making appropriate choices of the filter bands given the real-time weather conditions. The fundamental basis of the LSST concept is to scan the sky deep, wide, and fast, and to obtain a dataset that simultaneously satisfies the majority of the science goals. This concept, the so-called “universal cadence”, will yield the main deep-wide-fast survey and use about 90% of the observing time. The observing strategy for the main survey will be optimized for homogeneity of depth and number of visits. In times of good seeing and at low airmass, preference will be given to _r_ -band and _i_ -band observations. As often as possible, each field will be observed twice, with visits separated by 15-60 minutes. The ranking criteria also ensure that the visits to each field are widely distributed in position on the sky and rotation angle of the camera in order to minimize systematic effects in galaxy shape determination. \n## **1.4 Overview of dark energy science** \n27 \n## **3 The Analysis Framework and Key Systematics for Investigation** \n## **3.1 Weak lensing** \n## **3.1.1 Overview** \nGravitational lensing (Schneider 2006; Bartelmann 2010), the deflection of light from distant “sources” due to the bending of space-time by masses (“lenses”) along the line of sight, is a useful cosmological probe because it is sensitive to all matter, whether baryonic or dark. Weak lensing (WL) is gravitational lensing in the limit that the deflections are very small, causing tiny distortions, or “shear”, in galaxy shapes (Bartelmann & Schneider 2001; Refregier 2003). In the absence of lensing, galaxy orientations are assumed to be random, so they should not exhibit statistically significant, coherent alignments. Lensing induces small (≲ 1%) but coherent shears ( _γ_ ), in background galaxy images, which have typical ellipticities that are far larger (RMS ellipticity _∼_ 0 _._ 36). Thus, WL is detected statistically by averaging over many lensed galaxies, for which shapes have been measured in a way that eliminates contributions from observational effects that also distort galaxy shapes (e.g., the point-spread function, or PSF). \n**Constraints from the data** LSST lensing data will be an essential component in constraining dark energy’s effects on the growth of large structure. The clustering of galaxies provides a measure of how nonrelativistic species move through the large scale structure gravitational potentials, assuming galaxy bias is well characterized. Lensing gives a different perspective, showing how photons move through the potentials. While in General Relativity the two probes will be consistent, evidence of inconsistency in these tracers could well be a signal of a modification to gravity on cosmic scales Zhang et al. (2007). To extract these powerful signatures of dark energy requires careful characterization and simultaneous fitting for degenerate astrophysical systematics such as intrinsic alignments, stochasticity in shear-position correlations, nonlinear correlations and relativistic effects. Quantifying the dark energy constraints in light of these systematics will be a primary focus of the Theory/Joint Probe Working Group, and is discussed in more detail in Section 3.6.3. \n## **3.2 Large-scale structure** \n## **3.2.1 Overview** \nMeasurements of galaxy clustering constrain the cosmic expansion history, the cosmological distance scale, the growth rate of structure, the mass of the neutrinos and the abundance of dark matter. Baryon acoustic oscillations (BAO) in the early Universe imprint a standard ruler in the power-spectrum (or the two-point correlation function), which measures the angular-diameter distance and the Hubble parameter. BAO is a special case of using the full shape of galaxy clustering data to constrain cosmological models. The BAO feature has now been observed in the SDSS, 2dF, and BOSS surveys, using both spectroscopic (e.g., Eisenstein et al. 2005a; Anderson et al. 2012; Xu et al. 2012) and photometrically selected galaxy samples (Padmanabhan et al. 2007; Ho et al. 2012; Seo et al. 2012), and has proven to be a robust probe of dark energy. A second probe of dark energy, and a potential discriminator between dark energy and modified-gravity models, is the growth of large-scale structure in the Universe. \nThe galaxy distributions measured by LSST will reveal information from both of these classes of probes (see LSST Science Collaborations et al. 2009a, Chapter 13 for details). LSST’s most sensitive measurements of the growth of structure will involve cross-correlations of the galaxy distribution with the shear field measured by lensing (so-called galaxy-galaxy lensing) or with the cosmic microwave background (the integrated Sachs-Wolfe effect). Cross-correlations with external spectroscopic surveys will also lead to powerful tests of modified-gravity models. \n38 \n_3.2 Large-scale structure_ \nBeyond dark energy, the large scale power spectrum is a probe of both neutrino mass and primordial non-gaussianity. For all these purposes, identifying and removing systematic effects in the maps of the galaxy density, especially on large scales, will be crucial. \n## **3.2.2 Analysis steps** \n## _**Basic data processing by LSST DM**_ \nIn studies of large-scale structure, galaxies are treated as point sources; hence, the relevant LSST Data Management output is a catalog of the angular position of the centroid of each galaxy, the galaxy flux, the background flux, the radial distance (via photometric redshift), and the galaxy type determined from the _ugrizy_ colors. For all of these quantities, the respective errors must also be accurately estimated. \n## _**Additional inputs needed**_ \nInterpreting angular power spectra and correlation functions requires that the radial (redshift) distribution of galaxies in each photo- _z_ bin be well-determined. Several cosmological parameters that will not be determined directly by LSST but rather by CMB and other astronomical probes are also needed as input, including the primordial power spectrum, the BAO scale, and the baryonto-photon ratio. \nA number of complementary external (Stage III and IV) surveys, some contemporary with LSST, will also overlap with the LSST survey area. Spectroscopic surveys, such as BOSS, HETDEX, eBOSS, BigBOSS, DESpec, Euclid, and WFIRST will observe the same galaxy population as LSST and will provide peculiar velocity measurements, to contrast with the relativistic tracer of the large scale gravitational potential given by the lensing shear. They will also facilitate improved \n39 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \ncalibration of LSST’s photo- _z_ algorithms. CMB lensing and the integrated Sachs-Wolfe effect will provide measurements of relativistic tracers of the gravitational potentials complementary to LSST’s shear survey, to constrain modified gravity models and to constrain shear systematics that contaminate the cosmological lensing signal. Additional joint analyses of dark energy and modified gravity will be enabled by combining LSST with Stage III-IV spectroscopic-redshift surveys. By probing redshift space distortions, these spectroscopic surveys enable a direct test of the logarithmic derivative of structure growth as a function of spatial scale (e.g., Acquaviva & Gawiser 2010; Samushia et al. 2012). \n## _**Constructing cosmological statistics**_ \nSource data must be properly masked to the survey window function, including deweighting or removing areas within the survey that are severely affected by significant seeing variations, excessive extinction from Galactic dust, obscuration by bright foreground stars or nearby galaxies, or general instrumental defects. Random catalog data must be masked in the exact same manner as the data in order to prevent false power from being introduced in the estimation process (Scranton et al. 2002; Wang & Brunner 2012). Higher order correlation functions (or their power spectra analogues) can be used to constrain any scale-dependent bias between the source population and the underlying dark matter distribution. \n## _**Cosmological parameter estimation**_ \nOnce the angular correlation function or angular power spectrum has been measured, the next step is to compute the theoretical expectation for the signals, in order to constrain cosmological models (see, e.g., Ho et al. 2012). The standard practice for this has been to start with a theoretical 3-d matter power spectrum, which can be calculated using the Code for Anisotropies in the Microwave Background (CAMB Lewis et al. 2000). This 3-d power spectrum must be projected \n40 \n_3.2 Large-scale structure_ \n## _**Cosmological simulation needs**_ \nCosmological simulations are necessary inputs on multiple fronts in LSS studies. On one hand, cosmological simulations provide us with predictions of the galaxy power spectrum. We will need to use dark-matter simulations to calibrate the theoretical power spectrum. For example, Heitmann et al. (2009) has simulated the 3-d galaxy power spectrum for a large range of cosmologies (see more discussions on this in Section 4.1.1). An expanded set of these simulations for different types of galaxies would be very useful to help calibrate the theoretical power spectra. On the other hand, cosmological simulations will act as inputs to the mock catalog productions on a much larger scale, which will require larger volumes with comparable mass resolution to that which is currently used in the mock generation. \n## _**Improved software/algorithmic needs**_ \n43 \n_Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \nfor details. Additional joint analyses of dark energy and modified gravity will be enabled by combination with (Stage III-IV) spectroscopic redshift surveys such as (Big)BOSS, HETDEX, Euclid, and WFIRST. By probing redshift space distortions, these spectroscopic surveys enable a direct test of the logarithmic derivative of structure growth as a function of spatial scale (e.g., Acquaviva & Gawiser 2010; Samushia et al. 2012). \n## **3.3 Supernovae** \n## **3.3.1 Overview** \nObservations of nearby and distant type Ia supernovae (SNe Ia) led to the discovery of the accelerating Universe driven by dark energy (Riess et al. 1998; Perlmutter et al. 1999). SNe Ia have played a starring role in the current renaissance of time-domain astrophysics that will reach a new apex with LSST. SNe Ia discovered by LSST will provide important new insight into dark energy, not only through improvements in the precision of constraints on dark energy parameters ( _w_ , _wa_ , etc.), but also through novel tests of dark energy models (its isotropy, for example). \nSNe Ia are exploding stars defined by the lack of hydrogen and the presence of silicon in their emission, and are the product of the thermonuclear explosion of a C/O white dwarf (Hillebrandt & Niemeyer 2000). The brightening and fading of a SNe Ia are observed through its photon flux at different dates and wavelengths, i.e., multi-band light curves. The luminosity at peak brightness can be predicted to _∼_ 12% accuracy from light-curve shapes and colors. The ratio between the observed flux and predicted luminosity at peak brightness is a measure of the object’s luminosity distance. An accurate redshift is typically obtained from the host galaxy, although it can be obtained from the supernova itself. \nAs “standardizable candles”, SNe Ia map the expansion history of the Universe through measurements of their redshifts and luminosity distances. The SN Ia Hubble diagram is compared to cosmological models to measure the parameters that determine the dynamical evolution of the Universe. The redshift range of SNe Ia discovered by LSST, 0 _._ 1 _< z <_ 1 _._ 2, spans eras of both cosmic acceleration and deceleration, and the transition between the two. SNe Ia are therefore excellent probes of the physical properties of the dark energy responsible for the accelerated expansion of the Universe. \n## **3.3.2 Analysis steps** \nThe LSST data come in the form of pixelized wide-field images observed in multiple bands and dates. Within these images lie the measured supernova flux where the bulk (but not all) of the information from which discoveries, typing, redshift, and distances are determined, which in turn are used to make cosmological inferences. The survey and analysis steps roughly proceed as follows. \n44 \n_3.3 Supernovae_ \n## _**Survey**_ \nLSST, like the most successful SN cosmology projects today, will perform a “rolling” SN search and followup (Astier et al. 2006). Cadenced reobservations of the survey region in multiple bands provide multi-color optical light curves for the many active supernovae within the imager field of view. These constitute the fundamental data used in the analysis. It is absolutely clear that the observing plan for the main survey and for the deep drilling fields will have a direct and major impact on LSST’s ability to do SN Ia cosmology. Exploration and optimization of the phase space of survey parameters is of fundamental importance. \n## _**Candidate discovery**_ \nThe starting point of supernova analysis is discovery. There are two distinct types of transient discoveries that are made: \n**Live discovery.** Near real-time discovery of supernovae is needed to trigger supplemental observations while the object is still active. Generally, this is done by aligning a set of “new” images with “reference” images, subtracting the two, and identifying new point sources in the subtraction. \n**A posteriori discovery.** The images contain supernovae that were not discovered in the real-time analysis but yet can be identified from the full time series. \nWell-defined discovery criteria are important to quantify selection bias in the sample. Supernova searches are susceptible to Malmquist bias (preferred discovery of intrinsically brighter objects) and a bias toward objects that are less contaminated by host-galaxy light, i.e., away from the core of the host. In practice, some amount of human intervention is needed to distinguish between real and spurious transients. As long as the discovery efficiency is well-quantified, this potential source of systematic error is controlled. \n## _**Triggered external observations**_ \nSupernova typing, redshift, and distance determinations improve with additional information beyond the light curves and galaxy photometry provided by LSST. Type Ia classification is traditionally done through low-resolution rest-frame optical spectroscopy. If the SN has a clearly identifiable host galaxy, the host spectroscopic redshift can be obtained at any subsequent time. The rest-frame near-infrared (Krisciunas et al. 2004; Wood-Vasey et al. 2008; Barone-Nugent et al. 2012) and spectral features (Bailey et al. 2009; Foley et al. 2011; Blondin et al. 2012; Silverman et al. 2012) have proven to be a promising route to make SNe Ia even more standard. \nAfter identification, a transient is classified as a likely SN Ia or non-Ia. An early classification is necessary to send pure target lists for external observations of active supernovae. Follow-up resources are scarce so getting supplemental data on all LSST SNe is not feasible. A pure and efficient supernova typer that works on both early data and complete light curves is required for efficient use of non-LSST resources. Just as with candidate discovery, the efficiency of the typer must be quantified to monitor potential bias in the SN Ia subsample that receives additional observations. \n45 \n## _Chapter 3: The Analysis Framework and Key Systematics for Investigation_ \n## _**Extracting multi-band light curves**_ \nThe “final” SN photometry (as opposed to real-time measurements for detection and quick followup) needs to be of the highest fidelity. This important step transforms pixelized data to light curves in physical units necessary for supernova analyses. Of extreme importance is photometric calibration, within and across fields over the whole survey region. Not just the zeropoints but also the system response as a function of wavelength, time, and position on the detector must be provided with sufficient accuracy. The SN photometry will typically be modeled as a time-varying point source on a static host-galaxy background surface brightness. All observations of a field will contribute to this model (as even observations without the SN provide information about the static background, i.e., the host galaxy). In addition to the response function, each image should also come with an accurate PSF model. \nThe flux calibration is identified as a major challenge. We must understand how the performance of the LSST Project calibration system propagates into the covariance matrix for all supernova photometry. \n## _**Typing, redshift, and distance modulus**_ \nConstruction of the supernova Hubble diagram depends on determination of the type, redshift, and distance modulus for each SN, which in turn depends on the available data. \n**Photometric data only.** A large fraction of LSST SNe will have no independent typing or redshift measurements; these ingredients and distances are determined simultaneously from the complete light curves and host-galaxy photometry. Work is underway (but more is needed) to determine to what extent the photometry of the SN itself can yield the classification, redshift, and distance simultaneously (Sako et al. 2011; Hlozek et al. 2012). If a sample of SNe Ia can be classified photometrically with sufficient purity and efficiency to yield competitive dark energy constraints, it is possible to imagine much of the analysis could be done with the LSST data already taken, without the need for real-time discovery and ancillary follow-up of objects. A wide array of different approaches to obtain classification and redshifts can be explored (SN photo-classification and host spectroscopic redshift, vs. SN photo-classification and SN photo- _z_ , SN classification based on host color/morphology and host photo- _z_ , etc.). \n## **4.1 Simulations** \nSimulating and modeling the LSST system as a whole (from the underlying cosmology to the performance of the LSST telescope and camera) is integral to our understanding of the capabilities of the LSST in characterizing dark energy. For example, as a Stage IV dark energy experiment, the LSST must achieve three orders of magnitude improvement in PSF interpolation (as a function of wavelength and position on the focal plane), and two orders of magnitude improvement in shear calibration over published Stage II dark energy surveys. Our ability to achieve these advances will be dependent on the level of systematics present within the LSST data and our ability to correct for these effects. Developing the strategies and algorithms necessary to attain these improvements requires high-fidelity simulations that capture the properties of the LSST and how they couple to the cosmological measurements (including uncertainties due to perturbations in the optical surfaces, modulation of the PSF by the atmosphere, variations in photometricity, and complex astrophysical foregrounds). \nFigure 4.1 illustrates the simulation tools and their dependencies that will be required as part the DESC software framework. Cosmological simulations, based on N-body codes (Habib et al. 2009; Springel 2005) and hydrodynamic codes (Fryxell et al. 2000; Trac & Pen 2006), feed into the generation of mock catalogs, which reproduce the properties of astrophysical sources including redshift, color, and magnitude distributions. Coupling these catalogs to sequences of simulated LSST observations (using the Operations Simulator) enables the generation of data with appropriate densities and uncertainties. These data can then be used to evaluate the performance of the LSST system (e.g., how does Galactic extinction or photometric calibration modulate the densities \n79 \n## _Chapter 4: Simulation Tools and Technical Infrastructure_ \nof galaxies and how can that imprint systematic errors in the measures of dark energy) or, for higher fidelity questions, be fed directly into a photon based image simulator (PhoSim). \n**==> picture [467 x 344] intentionally omitted <==**\nFigure 4.1: The flow of information through the DESC simulation framework. Cosmological n-body and hydrodynamic simulations feed the generation of mock catalogs (with appropriate source properties and densities, and measurement uncertainties). Coupled with the Operations Simulator, this parameterized view of the sky can be used either directly for DESC science analyses or input into the photon simulator to generate images that are representative of the LSST. The software and analysis framework will provide a unified interface to all aspects of the simulation, catalog and image generation, image processing and analyses. \nThe vertical arrow in Figure 4.1 demonstrates the connection between these tools with the flow of information from the cosmological models to the generation of catalogs with realistic uncertainties to the production of simulated LSST images. The horizontal arrow illustrates the flow of these simulated data sets into the DESC analysis framework. This incorporates the analysis of images using the LSST and DESC pipelines and the task-specific algorithms that will be generated by the individual working groups as discussed in Chapter 3 (e.g., see the weak lensing high priority tasks broken out in Figure 4.1). All of these tasks will need to be developed within a software framework that combines the simulations and data analyses, enabling common development tools and providing a scalable platform for working with the volume of LSST data. \nThroughout the following sections we describe the simulation tools available to the DESC, their \n80 \n_4.1 Simulations_ \ncapabilities and limitations, and how we expect to combine these tools within a coherent simulation and analysis framework that will enable our dark energy studies to be accomplished. \n## **4.1.1 Simulation tools** \n## _**Cosmological simulations**_ \n83 \n## _Chapter 4: Simulation Tools and Technical Infrastructure_ \nfor and modeling possible astrophysical systematics as described below, (iii) generating a large simulation suite that encompasses a wide class of dark energy models and theories of modified gravity, (iv) ensuring that the available analysis tools are adequate and develop new ones as required. \nDuring the LSST Development Phase, the simulation group will work closely together with the analysis groups to map out the required set of simulations and generate the first set of these simulations. The requirements will include determination of volumes, resolution, required accuracy, and the set of cosmological models to be covered. Some work has already been carried out in this direction. The longer-term goal during the LSST Construction Phase will be to generate precision prediction tools from a large suite of simulations for the various probes. Again, close collaboration with the analysis working groups will be required. While one might hope that some of these prediction tools will be developed as part of some of the Stage III missions, such as the Dark Energy Survey, the accuracy requirements for LSST DESC will be much more stringent (LSST Science Collaborations et al. 2009b). Although some of this work will build upon progress for Stage III experiments, it is unlikely that those tools will meet LSST DESC requirements. \n**Weak lensing star-galaxy separation.** This task will require flexibility: ImSim runs will be needed with varying star and galaxy brightness distributions to study contamination of the PSF star sample. Dense star field simulations with limited numbers of galaxies will also be performed. In addition to running the standard DM pipeline, custom runs will be needed for specific sets of objects to determine the PSF, with resulting PSF models as outputs. Small sets of simulations that can be run locally are expected to satisfy many of these needs, with community large scale \n92 \n_4.2 Computing infrastructure_ \nTable 4.1: Working Group Task Requirements “Std DM” refers to standard catalogs; “Custom DM” allows for modification or replacement of DM algorithms. “SimpleSky” refers to a limited set of stars or galaxies in the field; “Custom ImSim” involves special studies of seeing conditions and other telescope- or atmosphere-related parameters. \n|WG|#|tasks|FullSky|SimpleSky|Std|ImSim|Custom|ImSim|Std|DM|Custom|DM|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|WL||5|4|5||5|3|||5|4||\n|SL||4|2|0||2|1|||2|1||\n|LSS||3|2|0||2|0|||0|1||\n|SN||3|1|1||1|0|||1|0||\n|Clusters||6|5|0||5|0|||5|0||\n|Theory||4|0|0||0|0|||0|0||\n|Photo-_z_||3|2|0||2|2|||2|0||\nsimulations (e.g., latest Data Challenge runs or shared simulations with other Working Groups, like LSS or Photo- _z_ ) for data analysis validation. \n**Photo-** _z_ **.** The photo- _z_ group will characterize the impact of various systematics, observational characteristics, and different source properties on the accuracy of the photometric redshift, the photometric redshift error or probability distribution function, and associated galaxy spectral classification. Their analyses will be used to investigate and mitigate the following basic issues: host morphology or spectral energy distribution, internal host absorption, AGN contamination, intergalactic absorption, Galactic extinction, sky brightness, atmospheric and telescopic throughput, seeing, and stellar density. In addition, the photo- _z_ group will need simulations that allow testing of systematics induced by tiling or stacking of individual images, including photometric uniformity and the effect of the stacking algorithm. Likewise, they will need to understand the effects of the actual dithering algorithm employed for a single pointing, and will likely want to test the stacking and tiling at different Galactic latitudes to verify that the aforementioned systematics are under control at all survey pointings. \nMuch of this analysis could be acomplished by having simulated images and resultant catalogs at a grid of different Galactic coordinates (both longitude and latitude). Sampling at approximately ten to fifteen degrees in both coordinates is probably sufficient (this can likely be adjusted larger at higher Galactic latitudes), although the team might have to revisit the sampling rate after looking at the data. This leads to several hundred simulated fields in total, with some of these fields simulated under different observing conditions to quantify the effects of throughput or sky brightness \nIn terms of the measured catalog parameters from these different fields, interest focuses primarily on photometry and photometric errors. Ideally there will be at least two apertures (but more is generally better): a PSF-type aperture and a model aperture measured in all filters. Other parameters that might prove useful include basic shape parameters including a radius, and the object coordinates, as well as parameters for general observing conditions (e.g., airmass, sky brightness, seeing). \n93 \n_Chapter 4: Simulation Tools and Technical Infrastructure_ \n## _**ImSim resource usage**_ \n- _Activities:_ During the first three years, we will work closely with the analysis groups and the ImSim team to generate a first set of mock catalogs suitable for some of the tasks outlined in Chapter 5. We will explore different ways of generating these mock catalogs and start developing accurate validation procedures. We will develop a detailed simulation plan with analysis groups to map out their needs with respect to accuracy and coverage. \n- _Deliverables:_ First set of detailed mock catalogs that will be used as input to the ImSim pipeline and by the analysis teams; comprehensive plan for necessary simulation activities for the next phase of the project. These deliverables have a direct impact on tasks from all the working groups, in particular, high priority tasks for WL (Task 5.1.1:H-3), clusters (Task 5.4.1:H-1), strong lensing (Task 5.5.1:H-1 and lower priority Task 5.5.2:LT-1), and theory (Task 5.6.1:H-1 – Task 5.6.1:H-3). \n## **H-2. CoSim: Data analysis and prediction tools** \n- _Motivation:_ The LSST DESC is aiming to derive constraints on dark energy and modified gravity with unprecedented accuracy. As emphasized by the different analysis groups in Chapter 3, simulations will play an important role in achieving this ambitious goal. In particular, high precision predictions in the nonlinear regime of structure formation will be crucial. A few examples from Chapter 3 include: cosmological parameter estimation from weak lensing with two-point statistics (Section 3.1.2), the 3-d power spectrum from LSS measurements (Section 3.2.2), prediction of the distribution of massive halos as a function of mass and redshift (Section 3.4.2), and the integration of different cosmological probes (Section 3.6.3). As outlined in Section 4.1.1, we have recently developed a framework to build emulators for observables spanning a range of parameters from a relatively limited set of simulations. Such emulators and their improved descendants will be necessary LSST DESC prediction tools. The emulator strategy will be extended to provide estimates for covariance matrices. In particular, for combining different probes, large volume simulations and many realizations of these are needed. The brute force approach (simply generating thousands of large simulations) is very undesirable. More intelligent strategies for tackling this problem are therefore needed and will be investigated as part of this program. \n- _Activities:_ Much as for the mock catalog generation, the first step will be to develop a detailed plan together with the different analysis groups that covers all desired prediction capabilities, the requirements on their accuracy, and the cosmological model space (including dark energy/modified gravity models) to be covered. For some of the desired \n116 \n_5.9 Simulations improvements_ \n- _Activities:_ All validation requires a comparison source, either some measurements from real data, an alternative code that can calculate a similar quantity, or a design expectation based on a detailed study. Direct validation with data or other codes is a significant element of the simulator improvement work. In particular, we have experience with existing codes developed for use in the adaptive optics community and codes for active optics wavefront estimation and feedback loops. In many cases, a specific test simulation \n122 \n_5.10 Software framework_ \nwith as many alternative codes as possible will help validate the simulations. We also expect to simulate either existing telescopes (we have already been testing the simulator for the Subaru telescope) as well as future telescopes where joint analyses are expected, such as Euclid. \n- _Deliverables:_ We will run validation simulations and collect either alternative code simulations or real data. The outputs will be compared with metrics using a uniform analysis techniques and documented."
}