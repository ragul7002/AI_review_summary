{
  "abstract": "It is challenging to build a multi-singer high-fidelity singing voice\nsynthesis system with cross-lingual ability by only using mono-\nlingual singers in the training stage.\nIn this paper, we propose\nCrossSinger, which is a cross-lingual singing voice synthesizer\nbased on Xiaoicesing2. Specifically, we utilize International Pho-\nnetic Alphabet to unify the representation for all languages of the\ntraining data. Moreover, we leverage conditional layer normaliza-\ntion to incorporate the language information into the model for better\npronunciation when singers meet unseen languages. Additionally,\ngradient reversal layer (GRL) is utilized to remove singer biases\nincluded in lyrics since all singers are monolingual, which indicates\nsinger’s identity is implicitly associated with the text. The experi-\nment is conducted on a combination of three singing voice datasets\ncontaining Japanese Kiritan dataset, English NUS-48E dataset, and\none internal Chinese dataset.\nThe result shows CrossSinger can\nsynthesize high-fidelity songs for various singers with cross-lingual\nability, including code-switch cases.\nIndex Terms— Singing voice synthesis, Cross-lingual, Genera-\ntive adversarial network, Conditional layer normalization\n1.",
  "introduction": "With the advancements in deep learning and the continuous evolu-\ntion of computing resources, speech synthesis has experienced sig-\nnificant progress, especially in acoustic models [1, 2, 3, 4, 5] and\nvocoders [6, 7, 8]. These two key components play crucial roles in\nmodern speech synthesis systems. Given the similarity in workflow\nbetween speech synthesis and singing voice synthesis (SVS), the re-\ncent breakthroughs in the speech synthesis community [9, 10, 11]\nhave also greatly benefited the field of SVS. Consequently, SVS\nhas gained considerable attention from both academia and industry.\nNotably, the architectural framework of Fastspeech2 [5] has been\nadopted in singing voice synthesis models such as Xiaoicesing [11]\nand its improved version, Xiaoicesing2 [12]. These models have\ndemonstrated the capability to generate high-fidelity singing voices.\nAdditionally, SingGAN [13] effectively employed the sine excitation\nmethod originally proposed in [7] to synthesize high-quality singing\nvoices.\nThe cross-lingual scenario is a pivotal topic within the speech\nsynthesis community, given the prevalence of multilingual speakers\nin today’s world [14].",
  "related_work": "",
  "methodology": "of\nCrossSinger are presented in Section 3. In Section 4, we showcase\nthe experimental",
  "results": "demonstrate that our CrossSinger can generate singing\nvoices with high naturalness and intelligibility in the cross-lingual\nscenario, including code-switch cases.\nThe remainder of this paper is organized as follows. In Section\n2, we provide a brief review of Xiaoicesing2 [12], as our CrossSinger\nmodel is built upon it. The detailed architecture and methodology of\nCrossSinger are presented in Section 3. In Section 4, we showcase\nthe experimental results*. Finally, we conclude this paper in Sec-\ntion 5, summarizing the key findings and discussing potential future\ndirections.\n2.",
  "discussion": "",
  "conclusion": "In this paper, we present an enhanced version of the Xiaoicesing2\nmodel, called CrossSinger, which is designed for cross-lingual\n\nmulti-singer singing voice synthesis. To effectively incorporate lan-\nguage information into the original Xiaoicesing2 model, we first\nstandardize the speech representation using IPA annotation. Subse-\nquently, we explicitly integrate the language information by fusing\nit with the feature map in the Fastspeech2 encoder through condi-\ntional layer normalization. Additionally, to overcome the limitations\nof available data, we introduce a singer bias eliminator that im-\nplicitly removes the singer-specific information associated with the\nlyrics, resulting in a more natural and expressive generated singing\nvoice. The experimental results demonstrate that CrossSinger out-\nperforms Xiaoicesing2 by synthesizing high-quality singing voices\nwith improved pronunciation accuracy and enhanced naturalness.\nMoreover, the ablation study illustrates the individual contributions\nof each proposed component.\n6.",
  "references": "[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,\nNavdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,\nYuxuan Wang, Rj Skerrv-Ryan, et al., “Natural tts synthesis\nby conditioning wavenet on mel spectrogram predictions,” in\n2018 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP). IEEE, 2018, pp. 4779–4783.\n[2] Takuhiro Kaneko, Hirokazu Kameoka, Nobukatsu Hojo,\nYusuke Ijima, Kaoru Hiramatsu, and Kunio Kashino, “Genera-\ntive adversarial network-based postfilter for statistical paramet-\nric speech synthesis,” in 2017 IEEE international conference\non acoustics, speech and signal processing (ICASSP). IEEE,\n2017, pp. 4910–4914.\n[3] Yi Zhao, Shinji Takaki, Hieu-Thi Luong, Junichi Yamagishi,\nDaisuke Saito, and Nobuaki Minematsu, “Wasserstein GAN\nand waveform loss-based acoustic model training for multi-\nspeaker text-to-speech synthesis systems using a WaveNet\nvocoder,” IEEE access, vol."
}