{
  "abstract": "In this paper, we investigate the degrees of freedom (dof) of penalized _ℓ_ 1 minimization (also\nknown as the Lasso) for linear regression models. We give a closed-form expression of the dof\nof the Lasso response. Namely, we show that for any given Lasso regularization parameter _λ_\nand any observed data _y_ belonging to a set of full (Lebesgue) measure, the cardinality of the\nsupport of a particular solution of the Lasso problem is an unbiased estimator of the degrees\nof freedom. This is achieved without the need of uniqueness of the Lasso solution. Thus,",
  "results": "latter was originally studied in [32]. We also show, by providing a simple counterexample, that\nalthough the dof theorem of [32] is correct, their proof contains a flaw since their divergence\nformula holds on a different set of a full measure than the one that they claim. An effective\nestimator of the number of degrees of freedom may have several applications including an\nobjectively guided choice of the regularization parameter in the Lasso through the SURE\nframework. Our theoretical findings are illustrated through several numerical simulations.\n**Keywords:** Lasso, model selection criteria, degrees of freedom, SURE.\n**AMS classification code:** Primary 62 _M_ 10, secondary 62 _M_ 20.\n1\nhave a unique solution. It obviously holds when the Lasso problem (P1( _y, λ_ )) has a unique solution,\nand in particular in the overdetermined case studied in [32]. Using the estimator at hand, we also\nestablish the reliability of the SURE as an unbiased estimator of the Lasso prediction risk.\nWhile this paper was submitted, we became aware of the independent work of Tibshirani and\nTaylor [25], who studied the dof for general _A_ both for the Lasso and the general (analysis) Lasso.\n[12, 25, 28] for the general case.\n**1.3** **Overview of the paper**\nThis paper is organized as follows. Section 2 is the core contribution of this work where we state our\nthe reliability of the SURE estimate of the Lasso prediction risk. Then, we discuss relation of\nour work with concurrent one in the literature in Section 3. Numerical illustrations are given in\nof this work are provided in Section 6.\n**2.1** **An unbiased estimator of the** dof\nFirst, some notations and definitions are necessary. For any vector _x_, _xi_ denotes its _i_ th component.\nThe support or the active set of _x_ is defined by\n_I_ = supp( _x_ ) = _i_ : _xi_ = 0 _,_\n_{_ _̸_ _}_\nand we denote its cardinality as supp( _x_ ) = _I_ . We denote by _xI_ R _[|][I][|]_ the vector built by\n_|_ _|_ _|_ _|_ _∈_\nrestricting _x_ to the entries indexed by _I_ . The active matrix _AI_ = ( _ai_ ) _i∈I_ associated to a vector _x_\nis obtained by selecting the columns of _A_ indexed by the support _I_ of _x_ . Let _·_ [T] be the transpose\nsymbol. Suppose that _AI_ is full column rank, then we denote the Moore-Penrose pseudo-inverse\nof _AI_, _A_ [+] _I_ [= (] _[A]_ _I_ [T] _[A][I]_ [)] _[−]_ [1] _[A]_ _I_ [T][.][ sign(] _[·]_ [)][ represents the sign function:][ sign(] _[a]_ [) = 1][ if] _[ a >]_ [ 0][;][ sign(] _[a]_ [) = 0]\nif _a_ = 0; sign( _a_ ) = _−_ 1 if _a <_ 0.\nFor any _I_ 1 _,_ 2 _,_ _, p_, let _VI_ = span( _AI_ ), _PVI_ the orthogonal projector onto _VI_ and _PVI ⊥_ that\n_⊆{_ _· · ·_ _}_\nonto the orthogonal complement _VI_ _[⊥]_ [.]\nLet _S ∈{−_ 1 _,_ 1 _}_ _[|][I][|]_ be a sign vector, and _j ∈{_ 1 _,_ 2 _, · · ·, p}_ . Fix _λ >_ 0. We define the following\nset of hyperplanes\n_HI,j,S_ = _{u ∈_ R _[n]_ : _⟨PVI ⊥_ [(] _[a][j]_ [)] _[, u][⟩]_ [=] _[ ±][λ]_ [(1] _[ −⟨][a][j][,]_ [ (] _[A]_ _I_ [+][)][T] _[S][⟩]_ [)] _[}][.]_ (4)\nNote that, if _aj_ does not belong to _VI_, then _HI,j,S_ becomes a finite union of two hyperplanes. Now,\nwe define the following finite set of indices\nΩ= ( _I, j, S_ ) : _aj_ _VI_ (5)\n_{_ _̸∈_ _}_\n3\nand let _Gλ_ be the subset of R _[n]_ which excludes the finite union of hyperplanes associate to Ω, that\nis\n          _Gλ_ = R _[n]_ _\\_ _HI,j,S._ (6)\n( _I,j,S_ ) _∈_ Ω\nTo cut a long story short, [�] ( _I,j,S_ ) _∈_ Ω _[H][I,j,S]_ [ is a set of (Lebesgue) measure zero (Hausdorff dimen-]\nsion _n −_ 1), and therefore _Gλ_ is a set of full measure.\nWe are now ready to introduce our main theorem.\n**Theorem 1.** _Fix λ >_ 0 _. For any y ∈_ _Gλ, consider My,λ the set of solutions of_ (P1( _y, λ_ )) _. Let_\n_x_ _[∗]_ _λ_ [(] _[y]_ [)] _[ ∈M][y,λ][ with support][ I]_ _[∗]_ _[such that][ A][I]_ _[∗]_ _[is full rank. Then,]_\n_I_ _[∗]_ = min (7)\n_|_ _|_ _x_                           - _λ_ ( _y_ ) _∈My,λ_\n_[|]_ [ supp(] _[x]_ [�] _[λ]_ [(] _[y]_ [))] _[|][.]_\n_Furthermore, there exists ε >_ 0 _such that for all z ∈_ Ball( _y, ε_ ) _, the n-dimensional ball with center_\n_y and radius ε, the Lasso response mapping z_ _µ_ - _λ_ ( _z_ ) _satisfies_\n_�→_\n_µ_             - _λ_ ( _z_ ) = � _µλ_ ( _y_ ) + _PVI∗_ ( _z_ _y_ ) _._ (8)\n_−_\nAs stated, this theorem assumes the existence of a solution whose active matrix _AI_ _∗_ is full rank.\nThis can be shown to be true; see e.g. [5, Proof of Theorem 1] or [20, Theorem 3, Section B.1] [1] . It\nis worth noting that this proof is constructive, in that it yields a solution _x_ _[∗]_ _λ_ [(] _[y]_ [)][ of (P][1][(] _[y, λ]_ [)][) such]\nthat _AI_ _[∗]_ is full column rank from any solution � _xλ_ ( _y_ ) whose active matrix has a nontrivial kernel.\nThis will be exploited in Section 4 to derive an algorithm to get _x_ _[∗]_ _λ_ [(] _[y]_ [)][, and hence] _[ I]_ _[∗]_ [.]\nA direct consequence of our main theorem is that outside _Gλ_, the mapping � _µλ_ ( _y_ ) is _C_ _[∞]_ and\nthe sign and support are locally constant. Applying Stein’s lemma yields Corollary 1 below. The\nlatter states that the number of nonzero coefficients of _x_ _[∗]_ _λ_ [(] _[y]_ [)][ is an unbiased estimator of the][ dof]\nof the Lasso.\n**Corollary 1.** _Under the assumptions and with the same notations as in Theorem 1, we have the_\n_following divergence formula_\n_df_                   - _λ_ ( _y_ ) := div( _µ_                   - _λ_ ( _y_ )) = _I_ _[∗]_ _._ (9)\n_|_ _|_\n_Therefore,_\n_df_ = E( _df_ [�] _λ_ ( _y_ )) = E( _I_ _[∗]_ ) _._ (10)\n_|_ _|_\nholds true.\n**2.2** **Reliability of the** SURE **estimate of the Lasso prediction risk**\nIn this work, we focus on the SURE as a model selection criterion. The SURE applied to the Lasso\nreads\nSURE( _µ_         - _λ_ ( _y_ )) = _nσ_ [2] + _µ_         - _λ_ ( _y_ ) _y_ 2 [+ 2] _[σ]_ [2][ �] _[df]_ _λ_ [(] _[y]_ [)] _[,]_ (11)\n_−_ _∥_ _−_ _∥_ [2]\nwhere _df_ ( _y_ ) is an unbiased estimator of the dof as given in Corollary 1. It follows that the\n[�]\nSURE( _µ_ - _λ_ ( _y_ )) is an unbiased estimate of the prediction risk, i.e.\nRisk( _µ_ ) = E            - _µ_            - _λ_ ( _y_ ) _µ_ 2� = E (SURE( _µ_            - _λ_ ( _y_ ))) _._\n_∥_ _−_ _∥_ [2]\n1This proof is alluded to in the note at the top of [21, Page 363].\n4\nWe now evaluate its reliability by computing the expected squared-error between SURE( _µ_ - _λ_ ( _y_ ))\nand SE( _µ_ - _λ_ ( _y_ )), the true squared-error, that is\nSE( _µ_               - _λ_ ( _y_ )) = _µ_               - _λ_ ( _y_ ) _µ_ 2 _[.]_ (12)\n_∥_ _−_ _∥_ [2]\n**Theorem 2.** _Under the assumptions of Theorem 1, we have_\nE �(SURE( _µ_   - _λ_ ( _y_ )) SE( _µ_   - _λ_ ( _y_ ))) [2][�] = 2 _σ_ [4] _n_ + 4 _σ_ [2] E   - _µ_   - _λ_ ( _y_ ) _y_ 2� + 4 _σ_ [4] E ( _I_ _[∗]_ ) _._ (13)\n_−_ _−_ _∥_ _−_ _∥_ [2] _|_ _|_\n_Moreover,_\n�2 [�]\n  - 1\n= _O_\n_n_\n_._ (14)\nE\n��\nSURE( _µ_  - _λ_ ( _y_ )) SE( _µ_  - _λ_ ( _y_ ))\n_−_\n_nσ_ [2]\n### **3 Relation to prior work**\n**Overdetermined case [32]**\nThe authors in [32] studied the dof of the Lasso in the overdetermined case. Precisely, when _n ≥_ _p_\nand all the columns of the design matrix _A_ are linearly independent, i.e. rank( _A_ ) = _p_ . In fact, in\nthis case the Lasso problem has a unique minimizer � _xλ_ ( _y_ ) = _x_ _[∗]_ _λ_ [(] _[y]_ [)][ (see Theorem 1).]\n]0 _,_ + _∞_ [:\nFor _λ_ _A_ [T] _y_ _∞_, the optimum is attained at � _xλ_ ( _y_ ) = 0.\n_•_ _≥∥_ _∥_\nThe interval �0 _,_ _A_ [T] _y_ _∞_   - is divided into a finite number of subintervals characterized by the\n_•_ _∥_ _∥_\nfact that within each such subinterval, the support and the sign vector of � _xλ_ ( _y_ ) are constant.\nExplicitly, let ( _λm_ )0 _≤m≤K_ be the finite sequence of _λ_ ’s values corresponding to a variation\nof the support and the sign of � _xλ_ ( _y_ ), defined by\n_∥A_ [T] _y∥∞_ = _λ_ 0 _> λ_ 1 _> λ_ 2 _> · · · > λK_ = 0 _._\nThus, in ] _λm_ +1 _, λm_ [, the support and the sign of � _xλ_ ( _y_ ) are constant, see [7, 17, 18]. Hence,\nwe call ( _λm_ )0 _≤m≤K_ the _transition points_ .\nNow, let _λ_ ] _λm_ +1 _, λm_ [. Thus, from Lemma 1 (see Section 5), we have the following implicit form\n_∈_\nof � _xλ_ ( _y_ ),\n( _x_                 - _λ_ ( _y_ )) _Im_ = _A_ [+] _Im_ _[y][ −]_ _[λ]_ [(] _[A]_ _I_ [T] _m_ _[A][I]_ _m_ [)] _[−]_ [1] _[Sm,]_ (15)\nwhere _Im_ and _S_ _[m]_ are respectively the (constant) support and sign vector of � _xλ_ ( _y_ ) for _λ_ ] _λm_ +1 _, λm_ [.\n_∈_\nHence, based on (15), [32] showed that for all _λ >_ 0, there exists a set of measure zero _λ_, which\n_N_\nis a finite collection of hyperplanes in R _[n]_, and they defined\n_Kλ_ = R _[n]_ _\\ Nλ,_ (16)\nso that _y_ _λ_, _λ_ is not any of the transition points.\n_∀_ _∈K_\nThen, for the overdetermined case, [32] stated that for all _y_ _λ_, the number of nonzero coefficients\n_∈K_\nof the unique solution of (P1( _y, λ_ )) is an unbiased estimator of the dof. In fact, their main argument\nis that, by eliminating the vectors associated to the transition points, the support and the sign of\nthe Lasso solution are locally constant with respect to _y_, see [32, Lemma 5].\n5\nbut valid on a different set _y ∈_ _Gλ_ = R _[n]_ _\\_ [�] ( _I,j,S_ ) _∈_ Ω _[H][I,j,S]_ [. A natural question arises: can we]\ncompare our assumption to that of [32] ? In other words, is there a link between _Kλ_ and _Gλ_ ?\nThe answer is that, depending on the matrix _A_, these two sets may be different. More importantly, it turns out that although the dof formula [32, Theorem 1] is correct, unfortunately, their\nproof contains a flaw since their divergence formula [32, Lemma 5] is not true on the set _λ_ . We\n_K_\nprove this by providing a simple counterexample.\n**Example of vectors in** _Gλ_ **but not in** _Kλ_ Let _{e_ 1 _, e_ 2 _}_ be an orthonormal basis of R [2] and let’s\ndefine _a_ 1 = _e_ 1 and _a_ 2 = _e_ 1 + _e_ 2, and _A_ the matrix whose columns are _a_ 1 and _a_ 2.\nLet’s define _I_ = _{_ 1 _}_, _j_ = 2 and _S_ = 1. It turns out that _A_ [+] _I_ [=] _[ a]_ [1][ and] _[ ⟨]_ [(] _[A]_ _I_ [+][)][T] _[S, a][j][⟩]_ [= 1][ which]\nimplies that for all _λ >_ 0,\n_HI,j,S_ = _{u ∈_ R _[n]_ : _⟨PVI ⊥_ [(] _[a][j]_ [)] _[, u][⟩]_ [= 0] _[}]_ [ = span(] _[a]_ [1][)] _[ .]_\nLet _y_ = _αa_ 1 with _α >_ 0, for any _λ >_ 0, _y ∈_ _HI,j,S_ (or equivalently here _y /∈_ _Gλ_ ). Using Lemma 1\n(see Section 5), one gets that for any _λ_ ]0 _, α_ [, the solution of (P1( _y, λ_ )) is � _xλ_ ( _y_ ) = ( _α_ _λ,_ 0) and\n_∈_ _−_\nthat for any _λ_ _α_, � _xλ_ ( _y_ ) = (0 _,_ 0). Hence the only transition point is _λ_ 0 = _α_ . It follows that for\n_≥_\n_λ < α_, _y_ belongs to _Kλ_ defined in [32], but _y /∈_ _Gλ_ .\nWe prove then that in any ball centered at _y_, there exists a vector _z_ 1 such that the support of\nthe solution of (P1( _z_ 1 _, λ_ )) is different from the support of (P1( _y, λ_ )).\nLet’s choose _λ < α_ and _ε ∈_ ]0 _, α −_ _λ_ [ and let’s define _z_ 1 = _y_ + _εe_ 2. From Lemma 1 (see Section 5),\none deduces that the solution of (P1( _z_ 1 _, λ_ )) is equal to � _xλ_ ( _z_ 1) = ( _α_ _λ_ _ε, ε_ ) whose support is\n_−_ _−_\ndifferent from that of � _xλ_ ( _y_ ) = ( _α_ _λ,_ 0).\n_−_\nMore generally, when there are sets _{I, j, S}_ such that _⟨_ ( _A_ [+] _I_ [)][T] _[S, a][j][⟩]_ [= 1][, a difference between]\nthe two sets _Gλ_ and _Kλ_ may arise. Clearly, _Gλ_ is not only the set of transition points associated\nto _λ_ .\nAccording to the previous example, in this specific situation, for any _λ >_ 0 there may exist\nsome vectors _y_ that are not transition points associated to _λ_ where the support of the solution\nof (P1( _y, λ_ )) is not stable to infinitesimal perturbations of _y_ . This situation may occur for under\npanel, each containing three plots. Hence, for each case, from left to right, the first plot represents\nthe SURE for one realization of the noise as a function of _λ_ . In the second graph, we plot the computed prediction risk curve and the empirical mean of the SURE as a function of the regularization\nparameter _λ_ . Namely, the dashed curve represents the calculated prediction risk, the solid curve\nrepresents the empirical mean of the SURE, and the shaded area represent the empirical mean\nof the sure _±_ the empirical standard deviation of the SURE. The latter shows that the SURE\nis an unbiased estimator of the prediction risk with a controlled variance. This suggests that the\nSURE is consistent, and then so is our estimator of the degrees of freedom. In the third graph,\nwe plot the theoretical and empirical normalized reliability, defined respectively by (17) and (18),\nas a function of the regularization parameter _λ_ . More precisely, the solid and dashed blue curves\nrepresent respectively _RT_ and _R_ [�] _T_ . This confirms numerically that both sides ( _RT_ and _R_ [�] _T_ ) of (13)\nindeed coincide.\n(top panel) displays the normalized empirical mean of the SURE (solid line) and its 5% quantiles\n(dotted) as well as the computed normalized prediction risk (dashed). Unbiasedness is again clear\nwhatever the value of _λ_ . The trend on the prediction risk (and average SURE) is in agreement\nwith rates known for the Lasso, see e.g. [2]. The second plot confirms that the SURE is an\nasymptotically reliable estimate of the prediction risk with the rate established in Theorem 2.\nMoreover, as expected, the actual reliability gets closer to the upper-bound (48) as the number of\nsamples _n_ increases.\n### **5 Proofs**\nFirst of all, we recall some classical properties of any solution of the Lasso (see, e.g., [17, 7, 11, 27]).\nTo lighten the notation in the two following lemmas, we will drop the dependency of the minimizers\nof (P1( _y, λ_ )) on either _λ_ or _y_ .\n**Lemma 1.** - _x is a (global) minimizer of the Lasso problem_ (P1( _y, λ_ )) _if and only of:_\n_1. A_ [T] _I_ [(] _[y][ −]_ _[A][x]_ [�][) =] _[ λ]_ [ sign(] _[x]_ [�] _[I]_ [)] _[, where][ I]_ [ =] _[ {][i]_ [ :][ �] _[x][i][ ̸]_ [= 0] _[}][, and]_\n_2._ _aj, y_ _Ax_  - _λ,_ _j_ _I_ _[c]_ _,_\n_|⟨_ _−_ _⟩| ≤_ _∀_ _∈_\n10\n_where I_ _[c]_ = 1 _, . . ., p_ _I. Moreover, if AI is full column rank, then_ - _x satisfies the following implicit_\n_{_ _}\\_\n_relationship:_\n_x_             - _I_ = _A_ [+] _I_ _[y][ −]_ _[λ]_ [(] _[A]_ _I_ [T] _[A][I]_ [)] _[−]_ [1][ sign(] _[x]_ [�] _[I]_ [)] _[ .]_ (20)\nNote that if the inequality in condition 2 above is strict, then _x_ is the unique minimizer of the\n                               Lasso problem (P1( _y, λ_ )) [11].\nLemma 2 below shows that all solutions of (P1( _y, λ_ )) have the same image by _A_ . In other\nwords, the Lasso response � _µλ_ ( _y_ ), is unique, see [5].\n**Lemma 2.** _If_ - _x_ [1] _and_ - _x_ [2] _are two solutions of_ (P1( _y, λ_ )) _, then_\n_Ax_              - [1] = _Ax_              - [2] = � _µλ_ ( _y_ ) _._\nBefore delving into the technical details, we recall the following trace formula of the divergence.\nLet _Jµ_ �( _y_ ) be the Jacobian matrix of a mapping _y_ _µ_ �( _y_ ), defined as follows\n_�→_\n- _Jµ_ �( _y_ )�\n_[∂][µ]_ [�][(] _[y]_ [)] _[i]_ _,_ _i, j_ = 1 _,_ _, n._ (21)\n_i,j_ [:=] _∂yj_ _· · ·_\nThen we can write\ndiv ( _µ_ �( _y_ )) = tr                      - _Jµ_ �( _y_ )� _._ (22)\n_Proof of Theorem 1._ Let _x_ _[∗]_ _λ_ [(] _[y]_ [)][ be a solution of the Lasso problem (P][1][(] _[y, λ]_ [)][) and] _[ I]_ _[∗]_ [its support]\nsuch that _AI_ _∗_ is full column rank. Let ( _x_ _[∗]_ _λ_ [(] _[y]_ [))] _[I]_ _[∗]_ [be the restriction of] _[ x][∗]_ _λ_ [(] _[y]_ [)][ to its support and]\n_S_ _[∗]_ = sign (( _x_ _[∗]_ _λ_ [(] _[y]_ [))] _[I]_ _[∗]_ [)][. From Lemma 2 we have,]\n_µ_             - _λ_ ( _y_ ) = _Ax_ _[∗]_ _λ_ [(] _[y]_ [) =] _[ A][I]_ _[∗]_ [(] _[x]_ _λ_ _[∗]_ [(] _[y]_ [))] _[I]_ _[∗]_ _[.]_\nAccording to Lemma 1, we know that\n_A_ [T] _I_ _[∗]_ [(] _[y][ −]_ _[µ]_ [�] _[λ]_ [(] _[y]_ [)) =] _[ λS][∗]_ [;]\n_ak, y_ _µ_                     - _λ_ ( _y_ ) _λ,_ _k_ ( _I_ _[∗]_ ) _[c]_ _._\n_|⟨_ _−_ _⟩| ≤_ _∀_ _∈_\nFurthermore, from (20), we get the following implicit form of _x_ _[∗]_ _λ_ [(] _[y]_ [)]\n( _x_ _[∗]_ _λ_ [(] _[y]_ [))] _[I]_ _[∗]_ [=] _[ A]_ [+] _I_ _[∗]_ _[y][ −]_ _[λ]_ [(] _[A]_ _I_ [T] _[∗]_ _[A][I]_ _[∗]_ [)] _[−]_ [1] _[S][∗][.]_ (23)\nIt follows that\n_µ_              - _λ_ ( _y_ ) = _PVI∗_ ( _y_ ) _λdI_ _∗,S∗_ _,_ (24)\n_−_\nand\n_r_               - _λ_ ( _y_ ) = _y −_ _µ_               - _λ_ ( _y_ ) = _PVI ⊥_ _[∗]_ [(] _[y]_ [) +] _[ λd][I]_ _[∗][,S][∗]_ _[,]_ (25)\nwhere _dI_ _∗,S∗_ = ( _A_ [+] _I_ _[∗]_ [)][T] _[S][∗]_ [. We define the following set of indices]\n_J_ = _j_ : _aj,_               - _rλ_ ( _y_ ) = _λ_ _._ (26)\n_{_ _|⟨_ _⟩|_ _}_\nFrom Lemma 1 we deduce that\n_I_ _[∗]_ _⊂_ _J._\nSince the orthogonal projection is a self-adjoint operator and from (25), for all _j ∈_ _J_, we have\n_|⟨PVI ⊥_ _[∗]_ [(] _[a][j]_ [)] _[, y][⟩]_ [+] _[ λ][⟨][a][j][, d][I]_ _[∗][,S][∗]_ _[⟩|]_ [ =] _[ λ.]_ (27)\n11\nAs _y_ _Gλ_, we deduce that if _j_ _J_ ( _I_ _[∗]_ ) _[c]_ then inevitably we have\n_∈_ _∈_ _∩_\n_aj_ _VI_ _∗_ _,_ and therefore _aj, dI_ _∗,S∗_ = 1 _._ (28)\n_∈_ _|⟨_ _⟩|_\nIn fact, if _aj_ _VI_ _∗_ then ( _I_ _[∗]_ _, j, S_ _[∗]_ ) Ω and from (27) we have that _y_ _HI_ _∗,j,S∗_, which is a\n_̸∈_ _∈_ _∈_\ncontradiction with _y_ _Gλ_ .\n_∈_\nTherefore, the collection of vectors ( _ai_ ) _i∈I_ _∗_ forms a basis of _VJ_ = span( _aj_ ) _j∈J_ . Now, suppose that\n_x_ - _λ_ ( _y_ ) is another solution of (P1( _y, λ_ )), such that its support _I_ is different from _I_ _[∗]_ . If _AI_ is full\ncolumn rank, then by using the same above arguments we can deduce that ( _ai_ ) _i∈I_ forms also a\nbasis of _VJ_ . Therefore, we have\n_I_ = _I_ _[∗]_ = dim( _VJ_ ) _._\n_|_ _|_ _|_ _|_\nOn the other hand, if _AI_ is not full rank, then there exists a subset _I_ 0 ⊊ _I_ such that _AI_ 0 is full\nthrough a divergence formula, valid almost everywhere except on a set of measure zero. We gave\na precise characterization of this set, and the latter turns out to be larger than the set of all the\nvectors associated to the transition points considered in [32] in the overdetermined case. We also\nhighlight the fact that even in the overdetermined case, the set of transition points is not sufficient\nfor the divergence formula to hold.\nWe think that some techniques developed in this article can be applied to derive the degrees of\nfreedom of other nonlinear estimating procedures. Typically, a natural extension of this work is to\nconsider other penalties such as those promoting structured sparsity, e.g. the group Lasso.\n**Acknowledgement** This work was partly funded by the ANR grant NatImages, ANR-08-EMER\n009.\n### **References**\n[1] Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle.\nSecond International Symposium on Information Theory 267-281.\n[2] Bickel, P. J., Ritov, Y., and Tsybakov, A., (2009). Simultaneous analysis of Lasso and Dantzig\nselector. Annals of Statistics. 37 1705?1732.\n[3] Craven, P. and Wahba, G. (1979). Smoothing Noisy Data with Spline Functions: estimating",
  "introduction": "**1.1** **Problem statement**\nWe consider the following linear regression model\n_y_ = _Ax_ [0] + _ε,_ _µ_ = _Ax_ [0] _,_ (1)\nwhere _y_ R _[n]_ is the observed data or the response vector, _A_ = ( _a_ 1 _,_ _, ap_ ) is an _n_ _p_ design\n_∈_    - �T _· · ·_ _×_\nmatrix, _x_ [0] = _x_ [0] 1 _[,][ · · ·][, x]_ [0] _p_ is the vector of unknown regression coefficients and _ε_ is a vector of i.i.d.\ncentered Gaussian random variables with variance _σ_ [2] _>_ 0. In this paper, the number of observations\n_n_ can be greater than the ambient dimension _p_ of the regression vector to be estimated. Recall\nthat when _n < p_, (1) is an underdetermined linear regression model, whereas when _n ≥_ _p_ and all\nthe columns of _A_ are linearly independent, it is overdetermined.\nLet _x_ ( _y_ ) be an estimator of _x_ [0], and _µ_ ( _y_ ) = _Ax_ ( _y_ ) be the associated response or predictor. The\n    -    -    concept of degrees of freedom plays a pivotal role in quantifying the complexity of a statistical\nmodeling procedure. More precisely, since _y ∼N_ ( _µ_ = _Ax_ [0] _, σ_ [2] Id _n×n_ ) (Id _n×n_ is the identity on\nR _[n]_ ), according to [8], the degrees of freedom (dof) of the response _µ_ ( _y_ ) is defined by\n_df_ =\n_n_\n_i_ =1\ncov( _µ_ - _i_ ( _y_ ) _, yi_ ) _._ (2)\n_σ_ [2]\nMany model selection criteria involve _df_, e.g. _Cp_ (Mallows [14]), AIC (Akaike Information Criterion,\n[1]), BIC (Bayesian Information Citerion, [22]), GCV (Generalized Cross Validation, [3]) and SURE\n(Stein’s unbiased risk estimation [23], see Section 2.2). Thus, the dof is a quantity of interest in\nmodel validation and selection and it can be used to get the optimal hyperparameters of the\nestimator. Note that the optimality here is intended in the sense of the prediction _µ_ ( _y_ ) and not\n                                     the coefficients _x_ ( _y_ ).\n       The well-known Stein’s lemma [23] states that if _y_ _µ_ ( _y_ ) is weakly differentiable then its\n                        _�→_\ndivergence is an unbiased estimator of its degrees of freedom, i.e.\n_df_ - ( _y_ ) = div( _µ_ �( _y_ )) =\n_n_\n_i_ =1\n_∂µ_ - _i_ ( _y_ ) _,_ and E( _df_ ( _y_ )) = _df ._ (3)\n_∂yi_ [�]\nHere, in order to estimate _x_ [0], we consider solutions to the Lasso problem, proposed originally\nin [26]. The Lasso amounts to solving the following convex optimization problem\n1\n_x_ min _∈_ R _[p]_ 2 _[∥][y][ −]_ _[Ax][∥]_ 2 [2] [+] _[ λ][∥][x][∥]_ [1] _[,]_ (P1( _y, λ_ ))\nwhere _λ >_ 0 is called the Lasso regularization parameter and _∥· ∥_ 2 (resp. _∥· ∥_ 1) denotes the _ℓ_ 2\n(resp. _ℓ_ 1) norm. An important feature of the Lasso is that it promotes sparse solutions. In the\nlast years, there has been a huge amount of work where efforts have focused on investigating the\ntheoretical guarantees of the Lasso as a sparse recovery procedure from noisy measurements. See,\ne.g., [9, 10, 30, 31, 19, 16, 17, 7, 11, 27], to name just a few.\n**1.2** **Contributions and related work**\nLet � _µλ_ ( _y_ ) = _Ax_ - _λ_ ( _y_ ) be the Lasso response vector, where � _xλ_ ( _y_ ) is a solution of the Lasso problem\n(P1( _y, λ_ )). Note that all minimizers of the Lasso share the same image under _A_, i.e. � _µλ_ ( _y_ ) is\n2\nuniquely defined; see Lemma 2 in Section 5 for details. The main contribution of this paper is first\nto provide an unbiased estimator of the degrees of freedom of the Lasso response for any design\nmatrix. The estimator is valid everywhere except on a set of (Lebesgue) measure zero. We reach\nour goal without any additional assumption to ensure uniqueness of the Lasso solution. Thus, our\ndegrees of freedom of the Lasso is to provide a data-driven objective way for selecting the optimal\nLasso regularization parameter _λ_ . For this, one can compute the optimal _λ_ that minimizes the\nSURE, i.e.\n_λ_ optimal = argmin SURE( _µ_             - _λ_ ( _y_ )) _._ (19)\n_λ>_ 0\nIn practice, this optimal value can be found either by a exhaustive search over a fine grid, or alternatively by any dicothomic search algorithm (e.g. golden section) if _λ_ SURE( _µ_ - _λ_ ( _y_ )) is unimodal.\n_�→_\nNow, for our second simulation study, we consider a partial Fourier design matrix, with _n < p_\nand a constant underdeterminacy factor _p/n_ = 4. _x_ [0] was again simulated according to a mixed\nGaussian-Bernoulli distribution with _⌈_ 0 _._ 1 _p⌉_ non-zero entries. For each of three values of _λ/σ ∈_\n_{_ 0 _._ 1 _,_ 1 _,_ 10 _}_ (small, medium and large), we compute the prediction risk curve, the empirical mean\nof the SURE, as well as the values of the normalized reliability _RT_ and _R_ [�] _T_, as a function of",
  "conclusion": "transition points is not sufficient to guarantee stability of the support and sign of the Lasso solution.\n**General case [12, 25, 28]**\nIn [12], the author studies the degrees of freedom of a generalization of the Lasso where the\nregression coefficients are constrained to a closed convex set. When the latter is a _ℓ_ 1 ball and\n_p > n_, he proposes the cardinality of the support as an estimate of _df_ but under a restrictive\nassumption on _A_ under which the Lasso problem has a unique solution.\nIn [25, Theorem 2], the authors proved that\n_df_ = E(rank( _AI_ ))\nwhere _I_ = _I_ ( _y_ ) is the active set of any solution � _xλ_ ( _y_ ) to (P1( _y, λ_ )). This coincides with Corollary 1\nwhen _AI_ is full rank with rank( _AI_ ) = rank( _AI_ _∗_ ). Note that in general, there exist vectors _y ∈_ R _[n]_\nwhere the smallest cardinality among all supports of Lasso solutions is different from the rank of\nthe active matrix associated to the largest support. But these vectors are precisely those excluded\nin _Gλ_ . In the case of the generalized Lasso (a.k.a. analysis sparsity prior in the signal processing\n6\nFigure 1: A counterexample for _n_ = _p_ = 2 of vectors in _Gλ_ but not in _Kλ_ . See text for a detailed\ncommunity), Vaiter et al. [28, Corollary 1] and Tibshirani and Taylor [25, Theorem 3] provide\na formula of an unbiased estimator of _df_ . This formula reduces to that of Corollary 1 when the\nanalysis operator is the identity.\n### **4 Numerical experiments**\n**Experiments description** In this section, we support the validity of our main theoretical findings with some numerical simulations, by checking the unbiasedness and the reliability of the SURE\nfor the Lasso. Here is the outline of these experiments.\nFor our first study, we consider two kinds of design matrices _A_, a random Gaussian matrix\nwith _n_ = 256 and _p_ = 1024 whose entries are iid (0 _,_ 1 _/n_ ), and a deterministic convolution\n_∼_ _N_\ndesign matrix _A_ with _n_ = _p_ = 256 and a Gaussian blurring function. The original sparse vector\n_x_ [0] was drawn randomly according to a mixed Gaussian-Bernoulli distribution, such that _x_ [0] is\n15-sparse (i.e. _|_ supp( _x_ [0] ) = 15 _|_ ). For each design matrix _A_ and vector _x_ [0], we generate _K_ = 100\nindependent replications _y_ _[k]_ _∈_ R _[n]_ of the observation vector according to the linear regression model\n(1). Then, for each _y_ _[k]_ and a given _λ_, we compute the Lasso response � _µλ_ ( _y_ _[k]_ ) using the now popular\niterative soft-thresholding algorithm [4] [2], and we compute SURE( _µ_ - _λ_ ( _y_ _[k]_ )) and SE( _µ_ - _λ_ ( _y_ _[k]_ )). We then\ncompute the empirical mean and the standard deviation of �SURE( _µ_ - _λ_ ( _y_ _[k]_ ))�1 _≤k≤K_ [, the empirical]\ncompute the empirical mean and the standard deviation of SURE( _µ_ - _λ_ ( _y_ )) 1 _≤k≤K_ [, the empirical]\nmean of �SE( _µ_ - _λ_ ( _y_ _[k]_ ))�1 _≤k≤K_ [, which corresponds to the computed prediction risk, and we compute]\nmean of SE( _µ_ - _λ_ ( _y_ )) 1 _≤k≤K_ [, which corresponds to the computed prediction risk, and we compute]\n_RT_ the empirical normalized reliability on the left-hand side of (13),\n2\n_._ (17)\n_RT_ = [1]\n_K_\n_K_\n_k_ =1\n- SURE( _µ_ - _λ_ ( _yk_ )) SE( _µ_ - _λ_ ( _yk_ ))\n_−_\n_nσ_ [2]\nMoreover, based on the right-hand side of (13), we compute _R_ [�] _T_ as\n_,_ (18)\n_K_\n_k_ =1\n- _µ_ - _λ_ ( _y_ _[k]_ ) _y_ _[k]_ 2�\n_∥_ _−_ _∥_ [2]\n+ [4]\n_n_ [2]\n_K_\n- ( _I_ _[∗]_ _k_ )\n_|_ _|_\n_k_ =1\n4\n_R_ - _T_ =\n_−_ _n_ [2] [+] _n_ [2] _σ_ [2]\n1\n_K_\n1\n_K_\nwhere at the _k_ th replication, _|I_ _[∗]_ _|k_ is the cardinality of the support of a Lasso solution whose active\nmatrix is full column rank as stated in Theorem 1. Finally, we repeat all these computations for\n2Iterative soft-thresholding through block-coordinate relaxation was proposed in [21] for matrices _A_ structured\nas the union of a finite number of orthonormal matrices.\n7\n(b) Convolution\n0.025\n0.02\n0.015\n0.01\n100.1\n100\n10−0.1\n10−0.2\n10−0.4\n10−0.3\nSURE 1 realization\nSURE 1 realization\n10\n0\n−0.6\n0.005\n(a) Gaussian\n0.08\n0.06\n0.04\n0.02\n10−1\n10−1\nFigure 2: The SURE and its reliability as a function of _λ_ for two types of design matrices. (a)\nGaussian; (b) Convolution. For each kind of design matrix, we associate three plots.\nvarious values of _λ_, for the two kinds of design matrices considered above.\nin situations where the Lasso problem has non-unique solutions, and the minimization algorithm\nreturns a solution whose active matrix is rank deficient, one can construct an alternative optimal\nsolution whose active matrix is full column rank, and then get the estimator of the degrees of\nfreedom.\nMore precisely, let � _xλ_ ( _y_ ) be a solution of the Lasso problem with support _I_ such that its active\nmatrix _AI_ has a non-trivial kernel. The construction is as follows:\n1. Take _h ∈_ ker _AI_ such that supp _h ⊂_ _I_ .\n2. For _t_ R, _Ax_  - _λ_ ( _y_ ) = _A_ ( _x_  - _λ_ ( _y_ ) + _th_ ) and the mapping _t_ _x_  - _λ_ ( _y_ ) + _th_ 1 is locally affine\n_∈_ _�→∥_ _∥_\nin a neighborhood of 0, i.e. for _t_ _<_ min _j∈I_ ( _x_    - _λ_ ( _y_ )) _j_ _/_ _h_ _∞_ . � _xλ_ ( _y_ ) being a minimizer of\n_|_ _|_ _|_ _|_ _∥_ _∥_\n(P1( _y, λ_ )), this mapping is constant in a neighborhood of 0. We have then constructed a\nwhole collection of solutions to (P1( _y, λ_ )) having the same image and the same _ℓ_ 1 norm,\nwhich lives on a segment.\n3. Move along _h_ with the largest step _t_ 0 _>_ 0 until an entry of � _x_ [1] _λ_ [(] _[y]_ [) =][ �] _[x][λ]_ [(] _[y]_ [) +] _[ t]_ [0] _[h]_ [ vanishes, i.e.]\nsupp( _x_   - [1] _λ_ [(] _[y]_ [) +] _[ t]_ [0] _[h]_ [)][ ⊊] _[I]_ [.]\n4. Repeat this process until getting a vector _x_ _[∗]_ _λ_ [(] _[y]_ [)][ with a full column rank active matrix] _[ A][I]_ _[∗]_ [.]\nNote that this construction bears similarities with the one in [20].\n8\n1.5\n1\n1 10\n10\n_n_\n2 103\n10\n4\n3\n2\n1 10\n3\n10\n_n_\n2 103\n(a) _λ/σ_ = 0 _._ 1\n1 10\n10\n_n_\n2 103\n101\n20\n15\n10\n1 10\n3\n10\n2 103\n_n_\n(b) _λ/σ_ = 1\n1 10\n10\n_n_\n2 103\n101\n1 10\n10\n_n_\n2 103\n3\n(c) _λ/σ_ = 10\n9\nFigure 3: The SURE and its reliability as a function of the number of observations _n_ .\nthat\n_I_ _>_ _I_ 0 = dim( _VJ_ ) = _I_ _[∗]_ _._\n_|_ _|_ _|_ _|_ _|_ _|_\nWe conclude that for any solution � _xλ_ ( _y_ ) of (P1( _y, λ_ )), we have\nsupp( _x_                    - _λ_ ( _y_ )) _I_ _[∗]_ _,_\n_|_ _| ≥|_ _|_\nand then _I_ _[∗]_ is equal to the minimum of the cardinalities of the supports of solutions of (P1( _y, λ_ )).\n_|_ _|_\nThis proves the first part of the theorem.\nLet’s turn to the second statement. Note that _Gλ_ is an open set and all components of ( _x_ _[∗]_ _λ_ [(] _[y]_ [))] _[I]_ _[∗]_\nare nonzero, so we can choose a small enough _ε_ such that Ball( _y, ε_ ) ⊊ _Gλ_, that is, for all _z_\n_∈_\nBall( _y, ε_ ), _z ∈_ _Gλ_ . Now, let _x_ [1] _λ_ [(] _[z]_ [)][ be the vector supported in] _[ I]_ _[∗]_ [and defined by]\n( _x_ [1] _λ_ [(] _[z]_ [))] _[I]_ _[∗]_ [=] _[ A]_ [+] _I_ _[∗]_ _[z][ −]_ _[λ]_ [(] _[A]_ _I_ [T] _[∗]_ _[A][I]_ _[∗]_ [)] _[−]_ [1] _[S][∗]_ [= (] _[x]_ _λ_ _[∗]_ [(] _[y]_ [))] _[I]_ _[∗]_ [+] _[ A]_ [+] _I_ _[∗]_ [(] _[z][ −]_ _[y]_ [)] _[.]_ (29)\nIf _ε_ is small enough, then for all _z ∈_ Ball( _y, ε_ ), we have\nsign( _x_ [1] _λ_ [(] _[z]_ [))] _[I]_ _[∗]_ [= sign(] _[x]_ _λ_ _[∗]_ [(] _[y]_ [))] _[I]_ _[∗]_ [=] _[ S][∗][.]_ (30)\nIn the rest of the proof, we invoke Lemma 1 to show that, for _ε_ small enough, _x_ [1] _λ_ [(] _[z]_ [)][ is actually]\na solution of (P1( _z, λ_ )). First we notice that _z −_ _Ax_ [1] _λ_ [(] _[z]_ [) =] _[ P]_ _VI_ _[⊥]_ [(] _[z]_ [) +] _[ λd][I]_ _[∗][,S][∗]_ [. It follows that]\n_A_ [T] _I_ _[∗]_ [(] _[z][ −]_ _[Ax]_ _λ_ [1] [(] _[z]_ [)) =] _[ λA]_ [T] _I_ _[∗]_ _[d][I]_ _[∗][,S][∗]_ [=] _[ λS][∗]_ [=] _[ λ]_ [ sign (] _[x]_ _λ_ [1] [(] _[z]_ [))] _[I]_ _[∗]_ _[.]_ (31)\nMoreover for all _j ∈_ _J ∩_ _I_ _[∗]_, from (28), we have that\n_|⟨aj, z −_ _Ax_ [1] _λ_ [(] _[z]_ [)] _[⟩|]_ = _|⟨aj, PVI ⊥_ _[∗]_ [(] _[z]_ [) +] _[ λd][I]_ _[∗][,S][∗]_ _[⟩|]_\n= _|⟨PVI ⊥_ _[∗]_ [(] _[a][j]_ [)] _[, z][⟩]_ [+] _[ λ][⟨][a][j][, d][I]_ _[∗][,S][∗]_ _[⟩|]_\n= _λ_ _aj, dI_ _[∗]_ _,S_ _[∗]_ = _λ._\n_|⟨_ _⟩|_\nand for all _j /∈_ _J_\n_|⟨aj, z −_ _Ax_ [1] _λ_ [(] _[z]_ [)] _[⟩| ≤|⟨][a][j][, y][ −]_ _[Ax][∗]_ _λ_ [(] _[y]_ [)] _[⟩|]_ [ +] _[ |⟨][P]_ _VI_ _[⊥][∗]_ [(] _[a][j]_ [)] _[, z][ −]_ _[y][⟩|]_\nSince for all _j /∈_ _J_, _|⟨aj, y −_ _Ax_ _[∗]_ _λ_ _[⟩|][ < λ]_ [, there exists] _[ ε]_ [ such that for all] _[ z][ ∈]_ [Ball(] _[y, ε]_ [)][ and] _[ ∀]_ _[j /][∈]_ _[J]_ [,]\nwe have\n_|⟨aj, z −_ _Ax_ [1] _λ_ [(] _[z]_ [)] _[⟩|][ < λ.]_\n12\nTherefore, we obtain\n_|⟨aj, z −_ _Ax_ [1] _λ_ [(] _[z]_ [)] _[⟩| ≤]_ _[λ,][ ∀]_ _[j][ ∈]_ [(] _[I]_ _[∗]_ [)] _[c][.]_\nWhich, by Lemma 1, means that _x_ [1] _λ_ [(] _[z]_ [)][ is a solution of][ (][P][1][(] _[z, λ]_ [))][, and the unique Lasso response]\nassociated to (P1( _z, λ_ )), denoted by � _µλ_ ( _z_ ), is defined by\n_µ_              - _λ_ ( _z_ ) = _PVI∗_ ( _z_ ) _λdI_ _∗,S∗_ _._ (32)\n_−_\nTherefore, from (24) and (32), we can deduce that for all _z ∈_ Ball( _y, ε_ ) we have\n_µ_             - _λ_ ( _z_ ) = � _µλ_ ( _y_ ) + _PVI∗_ ( _z_ _y_ ) _._\n_−_\n_Proof of Corollary 1._ We showed that there exists _ε_ sufficiently small such that\n_z_ _y_ 2 _ε_ _µ_              - _λ_ ( _z_ ) = � _µλ_ ( _y_ ) + _PVI∗_ ( _z_ _y_ ) _._ (33)\n_∥_ _−_ _∥_ _≤_ _⇒_ _−_\nLet _h_ _VI_ _∗_ such that _h_ 2 _ε_ and _z_ = _y_ + _h_ . Thus, we have that _z_ _y_ 2 _ε_ and then\n_∈_ _∥_ _∥_ _≤_ _∥_ _−_ _∥_ _≤_\n_µ_           - _λ_ ( _z_ ) _µ_           - _λ_ ( _y_ ) 2 = _PVI∗_ ( _h_ ) 2 = _h_ 2 _ε._ (34)\n_∥_ _−_ _∥_ _∥_ _∥_ _∥_ _∥_ _≤_\nTherefore, the Lasso response � _µλ_ ( _y_ ) is uniformly Lipschitz on _Gλ_ . Moreover, � _µλ_ ( _y_ ) is a continuous\nfunction of _y_, and thus � _µλ_ ( _y_ ) is uniformly Lipschitz on R _[n]_ . Hence, � _µλ_ ( _y_ ) is almost differentiable;\nsee [15] and [7].\nOn the other hand, we proved that there exists a neighborhood of _y_, such that for all _z_ in this\nneighborhood, there exists a solution of the Lasso problem (P1( _z, λ_ )), which has the same support\nand the same sign as _x_ _[∗]_ _λ_ [(] _[y]_ [)][, and thus][ �] _[µ][λ]_ [(] _[z]_ [)][ belongs to the vector space] _[ V][I]_ _[∗]_ [, whose dimension]\nequals to _I_ _[∗]_, see (24) and (32). Therefore, � _µλ_ ( _y_ ) is a locally affine function of _y_, and then\n_|_ _|_\n_Jµ_                  - _λ_ ( _y_ ) = _PVI∗_ _._ (35)\nThen the trace formula (22) implies that\ndiv ( _µ_                    - _λ_ ( _y_ )) = tr ( _PVI∗_ ) = _I_ _[∗]_ _._ (36)\n_|_ _|_\nThis holds almost everywhere since _Gλ_ is of full measure, and (10) is obtained by invoking Stein’s\nlemma.\n_Proof of Theorem 2._ First, consider the following random variable\n_Q_ 1( _µ_      - _λ_ ( _y_ )) = _µ_      - _λ_ ( _y_ ) 2 [+] _[ ∥][µ][∥]_ 2 [2]\n_∥_ _∥_ [2] _[−]_ [2] _[⟨][y,]_ [ �] _[µ][λ]_ [(] _[y]_ [)] _[⟩]_ [+ 2] _[σ]_ [2][ div(] _[µ]_ [�] _[λ]_ [(] _[y]_ [))] _[.]_\nFrom Stein’s lemma, we have\nE _ε,_            - _µλ_ ( _y_ ) = _σ_ [2] E (div( _µ_            - _λ_ ( _y_ ))) _._\n_⟨_ _⟩_\nThus, we can deduce that _Q_ 1( _µ_ - _λ_ ( _y_ )) and SURE( _µ_ - _λ_ ( _y_ )) are unbiased estimator of the prediction\nrisk, i.e.\nE (SURE( _µ_        - _λ_ ( _y_ ))) = E ( _Q_ 1( _µ_        - _λ_ ( _y_ ))) = E (SE( _µ_        - _λ_ ( _y_ ))) = Risk( _µ_ ) _._\nMoreover, note that SURE( _µ_ - _λ_ ( _y_ )) _Q_ 1( _µ_ - _λ_ ( _y_ )) = _y_ 2 - _y_ 2�, where\n_−_ _∥_ _∥_ [2] _[−]_ [E] _∥_ _∥_ [2]\n              E - _∥y∥_ 2 [2] - = _nσ_ [2] + _∥µ∥_ 2 [2] _[,]_ [ and][ V] - _∥y∥_ 2 [2] - = 2 _σ_ [4] _n_ + 2 _[∥]_ _σ_ _[µ][∥]_ [2] 2 [2]\n13\n_._ (37)\nNow, we remark also that\n_Q_ 1( _µ_      - _λ_ ( _y_ )) SE( _µ_      - _λ_ ( _y_ )) = 2      - _σ_ [2] div( _µ_      - _λ_ ( _y_ )) _ε,_      - _µλ_ ( _y_ )      - _._ (38)\n_−_ _−⟨_ _⟩_\nAfter an elementary calculation, we obtain\nE(SURE( _µ_    - _λ_ ( _y_ )) SE( _µ_    - _λ_ ( _y_ ))) [2] = E( _Q_ 1( _µ_    - _λ_ ( _y_ )) SE( _µ_    - _λ_ ( _y_ ))) [2] + V    - _y_ 2� + 4 _T,_ (39)\n_−_ _−_ _∥_ _∥_ [2]\nwhere\n_T_ = _σ_ [2] E �div( _µ_        - _λ_ ( _y_ )) _y_ 2� E        - _ε,_        - _µλ_ ( _y_ ) _y_ 2� = _T_ 1 + _T_ 2 _,_ (40)\n_∥_ _∥_ [2] _−_ _⟨_ _⟩∥_ _∥_ [2]\nwith\n_T_ 1 = 2         - _σ_ [2] E (div( _µ_         - _λ_ ( _y_ )) _ε, µ_ ) E ( _ε,_         - _µλ_ ( _y_ ) _ε, µ_ )� (41)\n_⟨_ _⟩_ _−_ _⟨_ _⟩⟨_ _⟩_\nand\n_T_ 2 = _σ_ [2] E �div( _µ_          - _λ_ ( _y_ )) _ε_ 2� E          - _ε,_          - _µλ_ ( _y_ ) _ε_ 2� _._ (42)\n_∥_ _∥_ [2] _−_ _⟨_ _⟩∥_ _∥_ [2]\nHence, by using the fact that a Gaussian probability density _ϕ_ ( _εi_ ) satisfies _εiϕ_ ( _εi_ ) = _σ_ [2] _ϕ_ _[′]_ ( _εi_ )\n_−_\nand integrations by parts, we find that\n_T_ 1 = 2 _σ_ [2] E ( _µ_               - _λ, µ_ )\n_−_ _⟨_ _⟩_\nand\n_T_ 2 = 2 _σ_ [4] E (div( _µ_              - _λ_ ( _y_ ))) _._\n_−_\nIt follows that\n_T_ = 2 _σ_ [2][�] E ( _µ_           - _λ, µ_ ) + _σ_ [2] E (div( _µ_           - _λ_ ( _y_ )))           - _._ (43)\n_−_ _⟨_ _⟩_\nMoreover, from [13, Property 1], we know that\nE( _Q_ 1( _µ_     - _λ_ ( _y_ )) SE( _µ_     - _λ_ ( _y_ ))) [2] = 4 _σ_ [2] �E     - _µ_     - _λ_ ( _y_ ) 2� + _σ_ [2] E �tr �� _Jµ_     - _λ_ ( _y_ )�2 [���] _,_ (44)\n_−_ _∥_ _∥_ [2]\nThus, since _Jµ_ - _λ_ ( _y_ ) = _PVI∗_ which is an orthogonal projector (hence self-adjoint and idempotent),\nwe have tr �� _Jµ_ - _λ_ ( _y_ )�2 [�] = div( _µ_ - _λ_ ( _y_ )) = _I_ _[∗]_ . Therefore, we get\n_|_ _|_\nE( _Q_ 1( _µ_        - _λ_ ( _y_ )) SE( _µ_        - _λ_ ( _y_ ))) [2] = 4 _σ_ [2][ �] E        - _µ_        - _λ_ ( _y_ ) 2� + _σ_ [2] E ( _I_ _[∗]_ )� _._ (45)\n_−_ _∥_ _∥_ [2] _|_ _|_\nFurthermore, observe that\nE (SURE( _µ_        - _λ_ ( _y_ ))) = _nσ_ [2] + E        - _µ_        - _λ_ ( _y_ ) _y_ 2� + 2 _σ_ [2] E ( _I_ _[∗]_ ) _._ (46)\n_−_ _∥_ _−_ _∥_ [2] _|_ _|_\nTherefore, by combining (37), (39), (43) and (45), we obtain\nE(SURE( _µ_ - _λ_ ( _y_ )) SE( _µ_ - _λ_ ( _y_ ))) [2] = 2 _nσ_ [4] + 4 _σ_ [2] E (SE( _µ_ - _λ_ ( _y_ ))) 4 _σ_ [4] E ( _I_ _[∗]_ )\n_−_ _−_ _|_ _|_\n= 2 _nσ_ [4] + 4 _σ_ [2] E (SURE( _µ_ - _λ_ ( _y_ ))) 4 _σ_ [4] E ( _I_ _[∗]_ )\n_−_ _|_ _|_\n(by using (46)) = 2 _nσ_ [4] + 4 _σ_ [2] E - _µ_ - _λ_ ( _y_ ) _y_ 2� + 4 _σ_ [4] E ( _I_ _[∗]_ ) _._\n_−_ _∥_ _−_ _∥_ [2] _|_ _|_\nOn the other hand, since _x_ _[∗]_ _λ_ [(] _[y]_ [)][ is a minimizer of the Lasso problem (P][1][(] _[y, λ]_ [)][), we observe that]\n1\n2 _[∥][µ]_ [�] _[λ]_ [(] _[y]_ [)] _[ −]_ _[y][∥]_ 2 [2] _[≤]_ 2 [1]\n2 [+] _[ λ][∥][x]_ _λ_ _[∗]_ [(] _[y]_ [)] _[∥]_ [1]\n2 [1] _[∥][µ]_ [�] _[λ]_ [(] _[y]_ [)] _[ −]_ _[y][∥]_ [2] _[≤]_ 2 [1]\n2 [1] _[∥][A.]_ [0] _[ −]_ _[y][∥]_ 2 [2] [+] _[ λ][∥]_ [0] _[∥]_ [1] [= ][1] 2\n2 _[.]_\n2 _[∥][y][∥]_ [2]\nTherefore, we have\nE          - _µ_          - _λ_ ( _y_ ) _y_ 2� E          - _y_ 2� = _nσ_ [2] + _µ_ 2 _[.]_ (47)\n_∥_ _−_ _∥_ [2] _≤_ _∥_ _∥_ [2] _∥_ _∥_ [2]\n14\nThen, since _|I_ _[∗]_ _|_ = _O_ ( _n_ ) and from (47), we have\n�2 [�]\n_≤_ _n_ [6] [+ ][4] _n_ _[∥]_ [2] _[µ]_ _σ_ _[∥]_ [2] 2 [2] _[.]_ (48)\nE\n��\nSURE( _µ_  - _λ_ ( _y_ )) SE( _µ_  - _λ_ ( _y_ ))\n_−_\n_nσ_ [2]\nFinally, since _∥µ∥_ 2 _<_ + _∞_, we can deduce that\n  - 1\n= _O_\n_n_\n�2 [�]\n_._\nE\n��\nSURE( _µ_  - _λ_ ( _y_ )) SE( _µ_  - _λ_ ( _y_ ))\n_−_\n_nσ_ [2]\nIn this paper we proved that the number of nonzero coefficients of a particular solution of the\nLasso problem is an unbiased estimate of the degrees of freedom of the Lasso response for linear\n15\n[7] Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression (with\n[8] Efron, B. (1981). How biased is the apparent error rate of a prediction rule. J. Amer. Statist.\nAssoc. vol. 81 pp. 461-470.\n[9] Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle\nproperties. J. Amer. Statist. Assoc. 96 1348-1360.\n[10] Fan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a diverging number of\nparameters. The Annals of Statistics, 32(3), 928-961.\n[11] Fuchs, J. J. (2004). On sparse representations in arbitrary redundant bases. IEEE Trans.\nInform. Theory, vol. 50, no. 6, pp. 1341-1344.\n[12] Kato, K. (2009). On the degrees of freedom in shrinkage estimation. Journal of Multivariate\nAnalysis 100(7), 1338-1352.\n[13] Luisier, F. (2009). The SURE-LET approach to image denoising. Ph.D. dissertation, EPFL,\n[Lausanne. Available: http://library.epfl.ch/theses/?nr=4566.](http://library.epfl.ch/theses/?nr=4566)\n[14] Mallows, C. (1973). Some comments on _Cp_ . Technometrics 15, 661-675.\n[15] Meyer, M. and Woodroofe, M. (2000). On the degrees of freedom in shape restricted regression.\nAnn. Statist. 28 1083-1104\n[16] Nardi, Y. and Rinaldo, A (2008). On the asymptotic properties of the group Lasso estimator\nfor linear models. Electronic Journal of Statistics, 2 605-633.\n[17] Osborne, M., Presnell, B. and Turlach, B. (2000a). A new approach to variable selection in\nleast squares problems. IMA J. Numer. Anal. 20 389-403.\n[18] Osborne, M. R., Presnell, B. and Turlach, B. (2000b). On the LASSO and its dual. J. Comput.\nGraph. Statist. 9 319-337.\n[19] Ravikumar, P., Liu, H., Lafferty, J., and Wasserman, L (2008). Spam: Sparse additive models.\nIn Advances in Neural Information Processing Systems (NIPS), volume 22.\n[20] Rosset, S., Zhu, J., Hastie, T. (2004). Boosting as a Regularized Path to a Maximum Margin\nClassifier. J. Mach. Learn. Res. 5 941-973.",
  "methods": "Mathematik 31, 377-403.\n[4] Daubechies, I., Defrise, M., and Mol, C. D. (2004). An iterative thresholding algorithm for\nlinear inverse problems with a sparsity constraint, Communications on Pure and Applied\nMathematics 57, 1413-1541.\n[5] Dossal, C (2007). A necessary and sufficient condition for exact recovery by l1 minimization.\nTechnical report, HAL-00164738:1.\n[6] Efron, B. (2004). The estimation of prediction error: Covariance penalties and cross-validation\nmetric wavelet denoising. J. of Comp. Graph. Stat. 9(2) 361?379.\n[22] Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6 461-464.\n[23] Stein, C. (1981). Estimation of the mean of a multivariate normal distribution. Ann. Statist.\n9 1135-1151.\n[24] Tibshirani, R. and Taylor, J. (2011). The Solution Path of the Generalized Lasso. Annals of\nStatistics. In Press.\n[25] Tibshirani, R. and Taylor, J. (2012). Degrees of Freedom in Lasso Problems. Technical report,\n[arXiv:1111.0653.](http://arxiv.org/abs/1111.0653)\n16\n[26] Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. J. Roy. Statist. Soc.\nSer. B 58(1) 267-288.\nnoise, IEEE Trans. Info. Theory 52 (3), 1030-1051.\n[28] Vaiter, S., Peyré, G., Dossal, C. and Fadili, M.J. (2011), Robust sparse analysis regularization.\n[arXiv:1109.6222.](http://arxiv.org/abs/1109.6222)\n[29] Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped\nvariables. J. Roy. Statist. Soc. Ser. B 68 49-67.\n[30] Zhao, P. and Bin, Y. (2006). On model selection consistency of Lasso. Journal of Machine\nLearning Research, 7, 2541-2563.\n[31] Zou, H. (2006). The adaptive Lasso and its oracle properties. Journal of the American Statis\ntical Association, 101(476), 1418-1429\n[32] Zou, H., Hastie, T. and Tibshirani, R. (2007). On the \"degrees of freedom\" of the Lasso. Ann.\nStatist. Vol. 35, No. 5. 2173-2192.\n17"
}