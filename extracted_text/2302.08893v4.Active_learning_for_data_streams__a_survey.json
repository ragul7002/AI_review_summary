{
  "abstract": "Online active learning is a paradigm in machine learning that aims to select the most informative data\npoints to label from a data stream. The problem of minimizing the cost associated with collecting labeled\nobservations has gained a lot of attention in recent years, particularly in real-world applications where data is\nonly available in an unlabeled form. Annotating each observation can be time-consuming and costly, making\nit difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies\nhave been proposed in the last decades, aiming to select the most informative observations for labeling in\norder to improve the performance of machine learning models. These approaches can be broadly divided\ninto two categories: static pool-based and stream-based active learning.",
  "introduction": "The deployment of machine learning models in real-world applications is often reliant on the availability of\nsignificant amounts of annotated data. While recent advancements in sensor technology have facilitated the\ncollection of larger amounts of data, this data is not always labeled and ready for use in training models. Indeed,\nthe process of obtaining labeled observations for supervised learning models can be cost-prohibitive and time-\nconsuming, as it often requires quality inspections or manual annotation. In such cases, active learning proves\nto be a valuable strategy to identify the most informative data points for use in training, thereby reducing the\noverall cost of labeling and improving the performance of the model. Over the years, a plethora of active learning\napproaches have been proposed in the literature, each with its own benefits and limitations.",
  "related_work": "s. However, the growing availability of data streams has led to an increase in\nthe number of approaches that focus on online active learning, which involves continuously selecting and\nlabeling observations as they arrive in a stream. This work aims to provide an overview of the most recently\nproposed approaches for selecting the most informative observations from data streams in real time. We\nreview the various techniques that have been proposed and discuss their strengths and limitations, as well as\nthe challenges and opportunities that exist in this area of research.\nKeywords: stream-based active learning; online active learning; data streams; online learning; unlabeled data; query\nstrategies; selective sampling; concept drift; experimental design; bandits.\n1 Introduction\nThe deployment of machine learning models in real-world applications is often reliant on the availability of\nsignificant amounts of annotated data. While recent advancements in sensor technology have facilitated the\ncollection of larger amounts of data, this data is not always labeled and ready for use in training models.",
  "methodology": "that assume a\ncomplete availability of labels, which is not the case in many real-world applications. The aim of this review is to\nfill this gap by providing a comprehensive overview 1 of the most recently developed query strategies for online\nactive learning. It is worth noting that in certain cases, stream-based active learning is narrowly defined as the\nact of selecting the most informative observations from a data stream to fit a predictive model. Instead, the act\nof determining which observations to query while making predictions is referred to as online selective sampling\n(Hanneke and Yang, 2021). In this work, we cover and examine all the methods that address the crucial problem\nof selecting the most informative data points to label from a data stream in an online fashion.",
  "results": "in various applications. However, it is important to note that single models have their limitations and can\nsometimes struggle to capture complex patterns and diverse representations present in the data. To address these\nlimitations, researchers have proposed the use of ensembles or committees as an alternative (Krawczyk et al,\n2017). An ensemble or committee refers to a group of multiple models that collaborate to produce a more robust\nand accurate prediction by combining their individual predictions. The models in an ensemble or committee can\nbe trained on different subsets of the data or with varying hyperparameters, and the final prediction is typically\nmade through either voting or weighted averaging.",
  "discussion": ". Additionally, we included related papers that were necessary to understand the bigger picture from the",
  "conclusion": "s and summarizes the key contributions of the review.\n2 Preliminaries on active learning\nIn supervised learning, we seek to learn a function that can predict the output variable, also known as response,\ngiven a set of input variables, also known as covariates. This function is often learned by training a model on\na labeled dataset that consists of a large number of input-output pairs. However, obtaining labeled examples is\nnot always straightforward, and it may not be possible or practical to label all the available data. In these cases,\nactive learning can be used to select a subset of the data for labeling in order to improve the performance of the\nmodel, when there is a budget constraint on the number of unlabeled observations that can be queried. Indeed,\nthere are many examples of how a classification or regression model can achieve a performance that is similar to\nwhat can be achieved when all the labels are available, using only a small fraction of the available observations.\n1We conducted a search on SCOPUS and Google Scholar using the following keywords: ”on-line active learning”, ”online active learning”,\n”stream-based active learning”, ”single pass active learning”, ”online selective sampling”, ”sequential selective sampling”, and ”active\nlearning” combined with ”data stream”.",
  "references": "of the reviewed strategies.\n\n2.1 Instance selection criteria\nThe main challenge in active learning is deciding which data points to label. There are many strategies for\nselecting data points in active learning, and most of them can be associated with one of these groups:\n• Uncertainty-based query strategies. These approaches focus on selecting data points that the model is least\nconfident about, in order to reduce its uncertainty (Lu et al, 2016; Tong and Koller, 2002). When using\nclassification models, the most widely used is the margin-based query strategy, where data points close to the\ndecision boundary are selected (Roth and Small, 2006; Balcan et al, 2007).\n• Expected error or variance minimization. These strategies estimate the future error or variance, when a newly\nlabeled example is made available, and try to minimize it directly (Cohn et al, 1996; Roy and Mccallum, 2001).\n• Expected model change maximization."
}