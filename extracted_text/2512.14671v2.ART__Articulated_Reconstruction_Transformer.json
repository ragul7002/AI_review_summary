{
  "abstract": "We\nintroduce\nART,\nArticulated\nReconstruction\nTransformer—a category-agnostic,\nfeed-forward model\nthat reconstructs complete 3D articulated objects from\nonly sparse, multi-state RGB images. Previous",
  "introduction": "Articulated objects are ubiquitous in daily lives and central\nto human–scene interactions [38].\nAccurately construct-\ning their digital replicas is important for VR/AR, robotics,\nand embodied AI [9, 10, 25, 27, 28, 53, 68]. While re-\ncent 3D generation and reconstruction",
  "related_work": "2.1. Articulation structure understanding\nA large body of work studies articulation understanding\nacross diverse input modalities, including RGB [59] and\nRGB-D [1, 18, 39] images, point clouds [12, 40, 60],\nvideos [41, 50] and 3D meshes [51]. Within this space,\none line of work identifies movable parts to reveal the po-\ntential degrees of freedom of the object [20, 50, 56] from\nthe given input. Another complementary line directly es-\ntimates articulation parameters—from high-level kinematic\ngraphs to low-level joint directions, pivots, and motion an-\ngles [21, 31, 64]—with recent advances adopting generative\nor diffusion-based formulations to improve robustness and\naccuracy [26, 37]. However, most prior",
  "methodology": "for\narticulated object reconstruction either rely on slow op-\ntimization with fragile cross-state correspondences or use\nfeed-forward models limited to specific object categories.\nIn contrast, ART treats articulated objects as assemblies\nof rigid parts, formulating reconstruction as part-based\nprediction. Our newly designed transformer architecture\nmaps sparse image inputs to a set of learnable part slots,\nfrom which ART jointly decodes unified representations for\nindividual parts, including their 3D geometry, texture, and\nexplicit articulation parameters. The resulting reconstruc-\ntions are physically interpretable and readily exportable\nfor simulation. Trained on a large-scale, diverse dataset\n*Work done during internship at Meta\nwith per-part supervision, and evaluated across diverse\nbenchmarks, ART achieves significant improvements over\nexisting baselines and establishes a new state of the art for\narticulated object reconstruction from image inputs.\n1. Introduction\nArticulated objects are ubiquitous in daily lives and central\nto human–scene interactions [38].\nAccurately construct-\ning their digital replicas is important for VR/AR, robotics,\nand embodied AI [9, 10, 25, 27, 28, 53, 68].",
  "results": "on diverse categories, with each image\npair displaying the part-based decomposition (left, each movable part in a unique color) and the final textured mesh (right).\nAbstract\nWe\nintroduce\nART,\nArticulated\nReconstruction\nTransformer—a category-agnostic,\nfeed-forward model\nthat reconstructs complete 3D articulated objects from\nonly sparse, multi-state RGB images. Previous methods for\narticulated object reconstruction either rely on slow op-\ntimization with fragile cross-state correspondences or use\nfeed-forward models limited to specific object categories.\nIn contrast, ART treats articulated objects as assemblies\nof rigid parts, formulating reconstruction as part-based\nprediction. Our newly designed transformer architecture\nmaps sparse image inputs to a set of learnable part slots,\nfrom which ART jointly decodes unified representations for\nindividual parts, including their 3D geometry, texture, and\nexplicit articulation parameters. The resulting reconstruc-\ntions are physically interpretable and readily exportable\nfor simulation. Trained on a large-scale, diverse dataset\n*Work done during internship at Meta\nwith per-part supervision, and evaluated across diverse\nbenchmarks, ART achieves significant improvements over\nexisting baselines and establishes a new state of the art for\narticulated object reconstruction from image inputs.\n1.",
  "discussion": "s and",
  "conclusion": "Limitations. Our method assumes a known part count for\nthe target object and relies on pre-calibrated camera poses.\nFuture work should include learning a pose-free variant\n(self-calibrated cameras) with larger datasets and integrat-\ning part-count estimation directly into the model.\n\nConclusion. In this work, we propose ART, a feed-forward\nmodel to reconstruct complete 3D articulated objects from\nsparse, multi-state images.\nBy casting reconstruction as\na part-based prediction problem, ART jointly decodes ge-\nometry, texture, and articulation structure for each part.\nExperiments across a broad range of articulated objects\ndemonstrate that ART consistently outperforms strong feed-\nforward and optimization-based baselines.\nAcknowledgments. We thank Yawar Siddiqui, Ruocheng\nWang and Qiao Gu for the comments and fruitful discus-\nsions, and Ka Chen, David Clabaugh and Samir Aroudj for\nthe help on dataset collecting and constructions.",
  "references": "[1] Ben Abbatematteo, Stefanie Tellex, and George Konidaris.\nLearning to generalize kinematic models to novel objects. In\nProceedings of the 3rd Conference on Robot Learning, 2019.\n[2] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130–141, 2023. 4\n[3] Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom\nMonnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan,\nIro Laina, and Andrea Vedaldi.\nAutopartgen: Autogres-\nsive 3d part generation and discovery.\narXiv preprint\n 2025."
}